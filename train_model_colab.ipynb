{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain: Llama 3 Fine-Tuning ðŸ§ \n",
        "\n",
        "This notebook fine-tunes the `meta-llama/Meta-Llama-3-8B-Instruct` model on the high-quality architectural dataset generated by the `data_generation_colab.ipynb` notebook.\n",
        "\n",
        "### Workflow:\n",
        "1.  **Setup:** Mounts Google Drive, clones the repository, and installs all necessary training libraries.\n",
        "2.  **Authentication:** Logs into Hugging Face to download the Llama 3 model.\n",
        "3.  **Data Loading:** Loads the `gold_standard` JSON files from the assembly line's output directory in your Google Drive.\n",
        "4.  **Data Preparation:** Formats the JSON data into the specific instruction format required for fine-tuning.\n",
        "5.  **Model Loading:** Loads the Llama 3 model and its tokenizer in 4-bit precision for memory efficiency.\n",
        "6.  **Training:** Runs the fine-tuning process using the `SFTTrainer` and LoRA.\n",
        "7.  **Save Adapter:** Saves the resulting trained LoRA adapter to your Google Drive for future use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive to persist our dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Securely provide your GitHub token to clone the private repository\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# Prompt for the GitHub token\n",
        "github_token = getpass('Enter your GitHub Personal Access Token (PAT): ')\n",
        "os.environ['GITHUB_TOKEN'] = github_token\n",
        "\n",
        "# Clean up any previous clones\n",
        "!rm -rf HouseBrainLLM\n",
        "\n",
        "# Clone the repository using the token\n",
        "# Replace 'Vinay-O/HouseBrainLLM' with your own GitHub username and repository if it's different.\n",
        "!git clone https://{os.environ.get('GITHUB_TOKEN')}@github.com/Vinay-O/HouseBrainLLM.git\n",
        "%cd HouseBrainLLM\n",
        "\n",
        "print(\"\\nâœ… Repository cloned successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install necessary Python packages for training\n",
        "!pip install -q -U transformers peft accelerate bitsandbytes trl datasets\n",
        "\n",
        "print(\"âœ… Training dependencies installed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Authentication & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Log in to Hugging Face to download the Llama 3 model\n",
        "# You'll need a Hugging Face account and a User Access Token with 'read' permissions.\n",
        "# Get a token here: https://huggingface.co/settings/tokens\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "\n",
        "# The model we want to fine-tune\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# The path to the dataset generated by the assembly line notebook\n",
        "# MAKE SURE THIS PATH IS CORRECT\n",
        "dataset_path = \"/content/drive/MyDrive/housebrain_final_dataset/gold_standard\"\n",
        "\n",
        "# Where to save the final trained model adapter\n",
        "new_adapter_path = \"/content/drive/MyDrive/housebrain_llama3_adapter\"\n",
        "\n",
        "print(\"Configuration is set.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load and Prepare the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "import glob\n",
        "\n",
        "# Find all the generated JSON files\n",
        "json_files = glob.glob(f\"{dataset_path}/**/*.json\", recursive=True)\n",
        "\n",
        "def format_data_for_training(file_path):\n",
        "    \"\"\"Reads a JSON file and formats it into the required Llama 3 instruction format.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        \n",
        "        # Extract the original prompt from the 'input' block\n",
        "        prompt = data.get(\"input\", {}).get(\"basicDetails\", {}).get(\"prompt\", \"\")\n",
        "        if not prompt:\n",
        "            return None\n",
        "        \n",
        "        # The full JSON data becomes the 'answer'\n",
        "        answer = json.dumps(data, indent=2)\n",
        "        \n",
        "        # This is the specific format Llama 3 Instruct was trained on.\n",
        "        # We must match it precisely.\n",
        "        # The <|begin_of_text|> and <|end_of_text|> tokens are added automatically by the tokenizer.\n",
        "        formatted_text = f\"<|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|>\"\n",
        "        formatted_text += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{answer}<|eot_id|>\"\n",
        "        \n",
        "        return {\"text\": formatted_text}\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process all files and create a dataset\n",
        "data_list = [format_data_for_training(f) for f in json_files if f is not None]\n",
        "data_list = [item for item in data_list if item is not None] # Filter out any errors\n",
        "\n",
        "if not data_list:\n",
        "    raise ValueError(\"No valid data found! Please ensure the dataset_path is correct and contains valid JSON files.\")\n",
        "\n",
        "dataset = Dataset.from_list(data_list)\n",
        "\n",
        "print(f\"âœ… Successfully loaded and formatted {len(dataset)} examples.\")\n",
        "print(\"\\n--- Example ---\\n\")\n",
        "print(dataset[0]['text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Load Model and Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Configure quantization to load the model in 4-bit precision\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token # Set padding token\n",
        "tokenizer.padding_side = 'right' # Avoid issues with fp16 training\n",
        "\n",
        "print(\"âœ… Model and tokenizer loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Configure LoRA and Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16, # Rank\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Add LoRA adapters to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"LoRA configured.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=new_adapter_path,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=3, # Adjust as needed\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True, # Use mixed precision\n",
        ")\n",
        "\n",
        "# Create the trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=4096, # Adjust based on your VRAM\n",
        "    args=training_args,\n",
        "    packing=True,\n",
        ")\n",
        "\n",
        "print(\"Trainer is ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Start Fine-Tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting training...\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"âœ… Training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Save the Final Adapter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Saving final model adapter to {new_adapter_path}\")\n",
        "\n",
        "trainer.save_model(new_adapter_path)\n",
        "\n",
        "print(\"ðŸŽ‰ All done! Your fine-tuned adapter is saved in your Google Drive.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Save the Final Adapter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
