{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. Install and Start Ollama ---\n",
        "# This is the most critical step. We must install the Ollama service in the Colab environment.\n",
        "# The '%%bash' magic runs the entire cell as a shell script.\n",
        "%%bash\n",
        "# 1. Install Ollama\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# 2. Start the server in the background and log its output for debugging\n",
        "ollama serve > ollama_server.log 2>&1 &\n",
        "\n",
        "# 3. Wait 10 seconds to ensure the server has time to fully initialize\n",
        "sleep 10\n",
        "\n",
        "# 4. Verify that the server process is running by checking for the 'ollama' process\n",
        "if pgrep -x \"ollama\" > /dev/null\n",
        "then\n",
        "    echo \"‚úÖ Ollama server is running successfully.\"\n",
        "else\n",
        "    echo \"‚ùå Ollama server failed to start. Please check the log below:\"\n",
        "    cat ollama_server.log\n",
        "fi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2. Install Python Client ---\n",
        "# With the server running, now we install the Python library that lets our code talk to it.\n",
        "!pip install -q ollama ipywidgets pandas tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3. Mount Drive & Load Prompts ---\n",
        "from google.colab import drive\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Path to your prompt file ---\n",
        "# IMPORTANT: Verify this path is correct for your Google Drive setup.\n",
        "PROMPT_FILE_PATH = \"/content/drive/MyDrive/housebrain_prompts/platinum_prompts.txt\" \n",
        "\n",
        "# --- Load Prompts ---\n",
        "def load_prompts_from_file(filepath):\n",
        "    \"\"\"Loads prompts from a text file, one prompt per line.\"\"\"\n",
        "    print(f\"\\\\nLoading prompts from {filepath}...\")\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"‚ùå ERROR: Prompt file not found. Please check the path.\")\n",
        "        return []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        prompts = [line.strip() for line in f if line.strip()]\n",
        "    print(f\"‚úÖ Successfully loaded {len(prompts)} prompts.\")\n",
        "    return prompts\n",
        "\n",
        "ALL_PROMPTS = load_prompts_from_file(PROMPT_FILE_PATH)\n",
        "\n",
        "# --- SHUFFLE PROMPTS ---\n",
        "# This is a critical step to ensure that multiple parallel runs generate unique data.\n",
        "if ALL_PROMPTS:\n",
        "    random.shuffle(ALL_PROMPTS)\n",
        "    print(\"‚úÖ Prompts have been successfully shuffled for this run.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 4. Configure Run & Pull Model ---\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "# --- A. Select Model ---\n",
        "model_options = [\n",
        "    \"phi4-reasoning:latest\", \"phi4-reasoning:plus\", \"phi4-reasoning:14b\",\n",
        "    \"phi4-reasoning:14b-plus-q4_K_M\", \"phi3:instruct\", \"llama3:instruct\"\n",
        "]\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=model_options, value='phi4-reasoning:latest', description='Select Model:',\n",
        "    disabled=False, style={'description_width': 'initial'}\n",
        ")\n",
        "display(model_dropdown)\n",
        "\n",
        "# --- B. Tier & Number of Samples ---\n",
        "DATASET_TIER = \"gold_tier\"\n",
        "NUM_PLANS_TO_GENERATE = 15000 \n",
        "BASE_OUTPUT_DIR = \"raw_generated_data\"\n",
        "TIER_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, DATASET_TIER)\n",
        "os.makedirs(TIER_OUTPUT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 5. Pull & Verify Model ---\n",
        "# This cell takes the model you selected above and downloads it.\n",
        "\n",
        "selected_model = model_dropdown.value\n",
        "print(f\"--- Preparing Model: {selected_model} ---\")\n",
        "print(f\"Attempting to download via Ollama... (This may take several minutes)\")\n",
        "\n",
        "# Pull the model. The ! allows us to run the shell command.\n",
        "!ollama pull {selected_model}\n",
        "\n",
        "print(f\"\\\\n--- Verifying {selected_model} Installation ---\")\n",
        "!ollama list\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Setup Complete!\")\n",
        "print(\"You are ready to run the data generation in the next cell.\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 6. Run Data Generation ---\n",
        "import ollama\n",
        "import json\n",
        "import hashlib\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "\n",
        "def generate_and_save_raw_plan(prompt, model_name, tier_dir):\n",
        "    \"\"\"Generates a plan and saves the raw output.\"\"\"\n",
        "    prompt_hash = hashlib.md5(prompt.encode()).hexdigest()[:10]\n",
        "    timestamp = int(time.time())\n",
        "    unique_id = f\"prompt_{prompt_hash}_{timestamp}\"\n",
        "    run_output_dir = os.path.join(tier_dir, model_name, unique_id)\n",
        "    os.makedirs(run_output_dir, exist_ok=True)\n",
        "    output_filename = os.path.join(run_output_dir, \"raw_output.json\")\n",
        "    \n",
        "    try:\n",
        "        structured_prompt = f\"\"\"\n",
        "        Please act as an expert architect specializing in Indian residential and commercial design. Your task is to generate a detailed JSON representation of a floor plan based on the following request, keeping local building norms and Vastu principles in mind where appropriate.\n",
        "        **Architectural Request:** \"{prompt}\"\n",
        "        **Instructions:** Provide ONLY the JSON output.\n",
        "        **JSON Schema:** {{ \"levels\": [ {{ \"level_id\": \"ground_floor\", \"rooms\": [], \"openings\": [] }} ] }}\n",
        "        Now, begin.\n",
        "        \"\"\"\n",
        "        response = ollama.chat(\n",
        "            model=model_name,\n",
        "            messages=[{'role': 'user', 'content': structured_prompt}],\n",
        "            format='json'\n",
        "        )\n",
        "        raw_output = response['message']['content']\n",
        "        with open(output_filename, 'w') as f:\n",
        "            f.write(raw_output)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred: {str(e)}\"\n",
        "        with open(os.path.join(run_output_dir, \"error.log\"), 'w') as f:\n",
        "            f.write(error_message)\n",
        "        return False\n",
        "\n",
        "# --- Main Generation Loop ---\n",
        "if 'ALL_PROMPTS' in locals() and ALL_PROMPTS:\n",
        "    print(f\"\\\\nüöÄ Starting data generation for {NUM_PLANS_TO_GENERATE} samples...\")\n",
        "    successful_generations = 0\n",
        "    for i in tqdm(range(NUM_PLANS_TO_GENERATE), desc=f\"Generating with {selected_model}\"):\n",
        "        # Cycle through the shuffled list of prompts.\n",
        "        # This ensures variety and that each run uses the prompts in a unique order.\n",
        "        current_prompt = ALL_PROMPTS[i % len(ALL_PROMPTS)]\n",
        "        if generate_and_save_raw_plan(current_prompt, selected_model, TIER_OUTPUT_DIR):\n",
        "            successful_generations += 1\n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ Gold Tier Data Generation Complete!\")\n",
        "    print(f\"Successfully generated {successful_generations} / {NUM_PLANS_TO_GENERATE} raw plan files.\")\n",
        "    print(f\"All outputs are saved in: '{TIER_OUTPUT_DIR}/{selected_model}'\")\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(\"\\\\nüõë HALTED. Please run the prompt loading cell (Cell 3) successfully first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 7. Package and Download Results ---\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "# This cell packages the output from the last generation run into a single zip file.\n",
        "output_directory_path = os.path.join(BASE_OUTPUT_DIR, DATASET_TIER, selected_model)\n",
        "zip_filename = f\"{DATASET_TIER}_{selected_model}_raw_data_{int(time.time())}\"\n",
        "zip_filepath = f\"/content/{zip_filename}\"\n",
        "\n",
        "print(f\"Locating generated data in: {output_directory_path}...\")\n",
        "\n",
        "if os.path.isdir(output_directory_path):\n",
        "    print(f\"Compressing into '{zip_filename}.zip'...\")\n",
        "    shutil.make_archive(zip_filepath, 'zip', output_directory_path)\n",
        "    print(f\"‚úÖ Success! Your data is compressed and ready at {zip_filepath}.zip\")\n",
        "else:\n",
        "    print(f\"‚ùå Error: Could not find the output directory '{output_directory_path}'.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
