{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. Verify TPU and Install JAX/Flax Dependencies ---\n",
        "import jax\n",
        "import os\n",
        "\n",
        "# This is the most important step. We must confirm a TPU is available.\n",
        "try:\n",
        "    # Get the number of TPU devices available. Should be > 0.\n",
        "    tpu_device_count = jax.device_count()\n",
        "    if tpu_device_count > 0:\n",
        "        print(f\"✅ Success! Found {tpu_device_count} JAX devices (TPUs).\")\n",
        "        # Set a flag to prevent a known JAX warning\n",
        "        os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
        "    else:\n",
        "         raise Exception(\"No TPU devices found.\")\n",
        "except Exception as e:\n",
        "    print(\"❌ ERROR: Could not initialize JAX with a TPU backend.\")\n",
        "    print(\"Please go to 'Runtime' -> 'Change runtime type' and select a 'TPU' hardware accelerator.\")\n",
        "\n",
        "# --- Install Libraries ---\n",
        "# We need specific versions of these libraries that are compatible with TPUs.\n",
        "print(\"\\\\nInstalling JAX, Flax, and Hugging Face Transformers...\")\n",
        "!pip install -q \"jax[tpu]>=0.4.16\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -q flax transformers sentencepiece\n",
        "\n",
        "print(\"\\\\n✅ Dependencies installed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2. Load TPU-Compatible Model and Tokenizer ---\n",
        "from transformers import AutoTokenizer, FlaxAutoModelForCausalLM\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# --- Model Selection ---\n",
        "# We must use a model that has a JAX/Flax version available on the Hugging Face Hub.\n",
        "# \"google/gemma-2b-it-flax\" is a powerful and TPU-compatible choice.\n",
        "MODEL_NAME = \"google/gemma-2b-it-flax\"\n",
        "\n",
        "print(f\"--- Loading Model: {MODEL_NAME} ---\")\n",
        "print(\"This can take a few minutes as the model is downloaded to the Colab instance...\")\n",
        "\n",
        "# Load the tokenizer and the Flax model\n",
        "# The 'bf16' dtype is highly optimized for modern TPUs.\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = FlaxAutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        dtype=jnp.bfloat16,\n",
        "    )\n",
        "    print(f\"✅ Model '{MODEL_NAME}' and tokenizer loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: Failed to load the model. This could be due to a model name typo or a Hugging Face Hub issue.\")\n",
        "    print(e)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
