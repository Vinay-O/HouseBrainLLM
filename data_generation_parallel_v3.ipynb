{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# --- 1. Install and Start Ollama (High-Concurrency Mode) ---\n",
        "# This is the most critical step. We must install and configure the Ollama service.\n",
        "# The '%%bash' magic runs the entire cell as a shell script.\n",
        "%%bash\n",
        "# 1. Install Ollama\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# 2. CONFIGURE FOR PARALLELISM: Set the number of parallel requests Ollama will handle.\n",
        "# This is the key to maximizing A100 GPU utilization.\n",
        "export OLLAMA_NUM_PARALLEL=20\n",
        "\n",
        "# 3. Start the server in the background and log its output for debugging\n",
        "ollama serve > ollama_server.log 2>&1 &\n",
        "\n",
        "# 4. Wait 10 seconds to ensure the server has time to fully initialize\n",
        "sleep 10\n",
        "\n",
        "# 5. Verify that the server process is running by checking for the 'ollama' process\n",
        "if pgrep -x \"ollama\" > /dev/null\n",
        "then\n",
        "    echo \"‚úÖ Ollama server is running successfully in high-concurrency mode.\"\n",
        "else\n",
        "    echo \"‚ùå Ollama server failed to start. Please check the log below:\"\n",
        "    cat ollama_server.log\n",
        "fi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# --- 2. Install Python Client ---\n",
        "!pip install -q ollama ipywidgets pandas tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# --- 3. Mount Drive & Load Prompts ---\n",
        "from google.colab import drive\n",
        "import os\n",
        "import random\n",
        "\n",
        "print(\"‚ñ∂Ô∏è Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted successfully.\")\n",
        "\n",
        "PROMPT_FILE_PATH = \"/content/drive/MyDrive/housebrain_prompts/platinum_prompts.txt\" \n",
        "\n",
        "def load_prompts_from_file(filepath):\n",
        "    print(f\"\\nLoading prompts from {filepath}...\")\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"‚ùå ERROR: Prompt file not found. Please check the path.\")\n",
        "        return []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        prompts = [line.strip() for line in f if line.strip()]\n",
        "    print(f\"‚úÖ Successfully loaded {len(prompts)} prompts.\")\n",
        "    return prompts\n",
        "\n",
        "ALL_PROMPTS = load_prompts_from_file(PROMPT_FILE_PATH)\n",
        "\n",
        "if ALL_PROMPTS:\n",
        "    random.shuffle(ALL_PROMPTS)\n",
        "    print(\"‚úÖ Prompts have been successfully shuffled for this run.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# --- 4. Configure Run ---\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "# --- A. Select Model ---\n",
        "# The powerful gpt-oss:120b is now the default for high-quality generation.\n",
        "model_options = [\n",
        "    \"gpt-oss:120b\", \"gpt-oss:20b\",\n",
        "    \"magistral:24b\", \"magistral:12b\", \"magistral:8b\",\n",
        "    \"phi4-reasoning:latest\", \"phi4-reasoning:plus\", \"phi4-reasoning:14b\",\n",
        "    \"phi4-reasoning:14b-plus-q4_K_M\", \"phi3:instruct\", \"llama3:instruct\",\n",
        "    \"llama3.1:8b\", \"gemma3:27b\",\"qwen3:14b\",\"qwen2.5:72b\",\"qwen3:32b\",\"qwen2.5:32b\"\n",
        "]\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=model_options, value='gpt-oss:120b', description='Select Model:',\n",
        "    disabled=False, style={'description_width': 'initial'}\n",
        ")\n",
        "display(model_dropdown)\n",
        "\n",
        "# --- B. Configure Paths and Counts ---\n",
        "# All output will be saved directly into your Google Drive to prevent data loss.\n",
        "BASE_OUTPUT_DIR = \"/content/drive/MyDrive/HouseBrain/generated_data\" # SAVING TO DRIVE!\n",
        "DATASET_TIER = \"platinum_tier\" # We are aiming for a higher quality tier now\n",
        "NUM_PLANS_TO_GENERATE = 10000 # The script will generate *up to* this many new plans.\n",
        "\n",
        "# Create the base directory on Google Drive if it doesn't exist\n",
        "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"‚úîÔ∏è Base output directory is set to: {BASE_OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# --- 5. Pre-pull the LLM Model (Recommended) ---\n",
        "# This step is highly recommended to avoid a long delay on the first generation.\n",
        "# It explicitly downloads the selected model so you can see the progress.\n",
        "# If the model is already local, this will finish instantly.\n",
        "selected_model = model_dropdown.value\n",
        "print(\"---  ‡§Æ‡•â‡§°‡§≤ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à (Model is downloading) ---\")\n",
        "print(f\"Selected model: {selected_model}\")\n",
        "print(\"This may take several minutes for large models like gpt-oss:120b...\")\n",
        "\n",
        "# Using a system call to ollama pull to get nice, clean progress bars.\n",
        "get_ipython().system(f'ollama pull {selected_model}')\n",
        "\n",
        "print(\"\\n‚úÖ Model is ready locally.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!ollama list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# --- 6. Run Data Generation (V3.1 - Big Guns Prompting) ---\n",
        "import ollama\n",
        "import json\n",
        "import hashlib\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import concurrent.futures\n",
        "import os\n",
        "\n",
        "# --- A. Set Parallelism Level ---\n",
        "MAX_WORKERS = 20\n",
        "\n",
        "# --- B. V3.1 Master Prompt Template ---\n",
        "# Upgraded for more powerful models. Demands higher architectural realism.\n",
        "V3_MASTER_PROMPT_TEMPLATE = \"\"\"\n",
        "You are a world-class AI architect with a deep understanding of spatial design, residential building codes, and architectural aesthetics. Your task is to generate a single, complete, and meticulously detailed JSON object representing a house plan based on the user's request. You must adhere strictly to the provided schema and instructions.\n",
        "\n",
        "### USER REQUEST:\n",
        "**{user_prompt}**\n",
        "\n",
        "### CRITICAL INSTRUCTIONS:\n",
        "1.  **Output ONLY JSON:** Your entire response must be a single JSON object. Do not include any introductory text, conversation, or markdown (```json) formatting.\n",
        "2.  **Strict Schema Adherence:** The generated JSON MUST perfectly conform to the structure shown in the `GOLDEN_EXAMPLE`. Use the exact key names and data types.\n",
        "3.  **Geometric Consistency & Realism:** Rooms must have realistic dimensions and be placed logically. The `bounds` (`x`, `y`, `width`, `height`) for each room must NOT overlap with other rooms on the same level. Avoid simple, unrealistic linear layouts; create plans with believable adjacencies and flow.\n",
        "4.  **Connectivity is Paramount:** `doors` MUST be placed precisely on the shared wall between the two rooms they connect. `windows` MUST be placed on exterior walls (walls that do not touch another room).\n",
        "5.  **Valid Room Types:** The `type` for each room must be one of the following: `living_room`, `dining_room`, `kitchen`, `master_bedroom`, `bedroom`, `bathroom`, `half_bath`, `family_room`, `study`, `garage`, `utility`, `storage`, `stairwell`, `corridor`, `entrance`, `balcony`.\n",
        "6.  **Populate All Required Fields:** Ensure top-level fields like `input`, `total_area`, and `construction_cost` are present and filled with reasonable, calculated values based on the generated plan.\n",
        "\n",
        "### GOLDEN_EXAMPLE (A realistic two-room layout):\n",
        "```json\n",
        "{{\n",
        "  \"input\": {{ \"basicDetails\": {{\"bedrooms\": 2, \"floors\": 1, \"totalArea\": 840, \"style\": \"Modern\"}}, \"plot\": {{\"shape\": \"rectangular\", \"length\": 40, \"width\": 30}}, \"roomBreakdown\": [] }},\n",
        "  \"total_area\": 840.0,\n",
        "  \"construction_cost\": 150000.0,\n",
        "  \"levels\": [\n",
        "    {{\n",
        "      \"level_number\": 0,\n",
        "      \"rooms\": [\n",
        "        {{\n",
        "          \"id\": \"living_room\",\n",
        "          \"type\": \"living_room\",\n",
        "          \"bounds\": {{\"x\": 0, \"y\": 0, \"width\": 15, \"height\": 20}},\n",
        "          \"doors\": [\n",
        "            {{\"position\": {{\"x\": 15, \"y\": 10}}, \"width\": 3.0, \"type\": \"interior\", \"room1\": \"living_room\", \"room2\": \"kitchen\"}}\n",
        "          ],\n",
        "          \"windows\": [\n",
        "            {{\"position\": {{\"x\": 7.5, \"y\": 0}}, \"width\": 8.0, \"room_id\": \"living_room\"}}\n",
        "          ]\n",
        "        }},\n",
        "        {{\n",
        "          \"id\": \"kitchen\",\n",
        "          \"type\": \"kitchen\",\n",
        "          \"bounds\": {{\"x\": 15, \"y\": 0, \"width\": 10, \"height\": 12}},\n",
        "          \"doors\": [],\n",
        "          \"windows\": [\n",
        "            {{\"position\": {{\"x\": 25, \"y\": 6}}, \"width\": 4.0, \"room_id\": \"kitchen\"}}\n",
        "          ]\n",
        "        }}\n",
        "      ]\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "```\n",
        "\n",
        "Now, generate the complete, valid, and architecturally sound JSON for the user request.\n",
        "\"\"\"\n",
        "\n",
        "# --- C. Generation Function (Updated for V3) ---\n",
        "def generate_and_save_raw_plan(args):\n",
        "    task_id, prompt_content, model_name, base_dir, tier = args\n",
        "    prompt_hash = hashlib.sha1(prompt_content.encode()).hexdigest()[:10]\n",
        "    prompt_id = f\"prompt_{task_id:05d}_{prompt_hash}\"\n",
        "\n",
        "    output_dir_for_prompt = os.path.join(base_dir, tier, model_name, prompt_id)\n",
        "    final_output_path = os.path.join(output_dir_for_prompt, \"raw_output.json\")\n",
        "\n",
        "    if os.path.exists(final_output_path):\n",
        "        tqdm.write(f\"üü¢ [Task {task_id:05d}] SKIPPING (Already exists): {prompt_id}\")\n",
        "        return (prompt_id, True, \"Skipped\")\n",
        "\n",
        "    tqdm.write(f\"‚ö™ [Task {task_id:05d}] STARTING: {prompt_id}\")\n",
        "    os.makedirs(output_dir_for_prompt, exist_ok=True, mode=0o777)\n",
        "\n",
        "    try:\n",
        "        # Use the new V3 prompt template\n",
        "        structured_prompt = V3_MASTER_PROMPT_TEMPLATE.format(user_prompt=prompt_content)\n",
        "        \n",
        "        response = ollama.chat(\n",
        "            model=model_name,\n",
        "            messages=[{'role': 'user', 'content': structured_prompt}],\n",
        "            format='json'\n",
        "        )\n",
        "        raw_output = response['message']['content']\n",
        "\n",
        "        with open(final_output_path, 'w') as f:\n",
        "            f.write(raw_output)\n",
        "\n",
        "        tqdm.write(f\"‚úÖ [Task {task_id:05d}] COMPLETED & SAVED: {prompt_id}\")\n",
        "        return (prompt_id, True, final_output_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred in task {task_id}: {str(e)}\"\n",
        "        error_log_path = os.path.join(output_dir_for_prompt, \"error.log\")\n",
        "        with open(error_log_path, 'w') as f:\n",
        "            f.write(error_message)\n",
        "        tqdm.write(f\"‚ùå [Task {task_id:05d}] FAILED. Log saved to {error_log_path}\")\n",
        "        return (prompt_id, False, error_message)\n",
        "\n",
        "# --- D. Main Generation Loop (Unchanged) ---\n",
        "if 'ALL_PROMPTS' in locals() and ALL_PROMPTS:\n",
        "    print(\"\\n--- Checking for existing data in Google Drive to prevent re-work ---\")\n",
        "    \n",
        "    tasks_to_run = []\n",
        "    total_prompts_to_consider = min(NUM_PLANS_TO_GENERATE, len(ALL_PROMPTS))\n",
        "\n",
        "    for i in range(total_prompts_to_consider):\n",
        "        prompt_content = ALL_PROMPTS[i]\n",
        "        prompt_hash = hashlib.sha1(prompt_content.encode()).hexdigest()[:10]\n",
        "        prompt_id = f\"prompt_{i:05d}_{prompt_hash}\"\n",
        "        output_path = os.path.join(BASE_OUTPUT_DIR, DATASET_TIER, selected_model, prompt_id, \"raw_output.json\")\n",
        "        \n",
        "        if not os.path.exists(output_path):\n",
        "            tasks_to_run.append((i, prompt_content, selected_model, BASE_OUTPUT_DIR, DATASET_TIER))\n",
        "\n",
        "    total_already_generated = total_prompts_to_consider - len(tasks_to_run)\n",
        "    print(f\"Targeting {NUM_PLANS_TO_GENERATE} total plans.\")\n",
        "    print(f\"Found {total_already_generated} plans already completed in previous runs.\")\n",
        "    \n",
        "    if not tasks_to_run:\n",
        "        print(\"\\nüéâ All requested plans have already been generated. Nothing to do!\")\n",
        "    else:\n",
        "        print(f\"‚ñ∂Ô∏è Preparing to generate {len(tasks_to_run)} new plans...\")\n",
        "        print(f\"üöÄ Starting PARALLEL data generation using {MAX_WORKERS} workers...\")\n",
        "\n",
        "        successful_generations = 0\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "            results = list(tqdm(executor.map(generate_and_save_raw_plan, tasks_to_run), total=len(tasks_to_run), desc=f\"Generating with {selected_model}\"))\n",
        "        \n",
        "        successful_generations = sum(1 for res in results if res[1] is True and res[2] != \"Skipped\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"‚úÖ Data Generation Run Complete!\")\n",
        "        print(f\"Successfully generated {successful_generations} NEW raw plan files in this run.\")\n",
        "        final_output_dir = os.path.join(BASE_OUTPUT_DIR, DATASET_TIER, selected_model)\n",
        "        print(f\"All outputs are saved in your Google Drive at: '{final_output_dir}'\")\n",
        "        print(\"=\"*50)\n",
        "else:\n",
        "    print(\"\\nüõë HALTED. Please run the prompt loading cell (Cell 3) successfully first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# --- 7. (Optional) Package and Download Results ---\n",
        "import shutil\n",
        "import time\n",
        "import os\n",
        "\n",
        "# The data is already safe in Google Drive. This cell is for creating a convenient zip archive.\n",
        "output_directory_path = os.path.join(BASE_OUTPUT_DIR, DATASET_TIER, selected_model)\n",
        "zip_filename = f\"{DATASET_TIER}_{selected_model.replace(':', '_')}_raw_data_{int(time.time())}\"\n",
        "zip_filepath_in_colab = f\"/content/{zip_filename}\" # We'll create the zip in the local runtime for speed\n",
        "\n",
        "print(f\"Locating generated data in your Google Drive: {output_directory_path}...\")\n",
        "\n",
        "if os.path.isdir(output_directory_path):\n",
        "    print(f\"Compressing into '{zip_filename}.zip' in the Colab runtime...\")\n",
        "    # This might take a while if the dataset is very large\n",
        "    try:\n",
        "        shutil.make_archive(\n",
        "            base_name=zip_filepath_in_colab,\n",
        "            format='zip',\n",
        "            root_dir=output_directory_path\n",
        "        )\n",
        "        print(f\"‚úÖ Success! Your data is compressed and ready for download at {zip_filepath_in_colab}.zip\")\n",
        "        print(\"You can find it in the 'Files' panel on the left.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred during zipping: {e}\")\n",
        "else:\n",
        "    print(f\"‚ùå Error: Could not find the output directory '{output_directory_path}'. Please ensure the generation step ran correctly.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
