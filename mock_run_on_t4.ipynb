{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mock Run on T4 GPU: End-to-End Test\n",
        "\n",
        "This notebook is designed for a fast, low-cost mock run of the entire \"Human-in-the-Loop\" workflow on a T4 GPU.\n",
        "\n",
        "**Workflow:**\n",
        "1.  **Generate a small batch of raw drafts.**\n",
        "2.  **Manually refine the drafts locally** using `scripts/refine_drafts.py`.\n",
        "3.  **Run a brief fine-tuning job** on the perfected data to ensure the pipeline works from start to finish.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 1: Set Up the Environment\n",
        "import os\n",
        "# IMPORTANT: PASTE YOUR GITHUB PERSONAL ACCESS TOKEN HERE\n",
        "GITHUB_TOKEN = \"\"\n",
        "os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n",
        "\n",
        "# Clone the repository using your token\n",
        "!git clone https://$GITHUB_TOKEN@github.com/Vinay-O/HouseBrainLLM.git housebrain_v1_1\n",
        "%cd housebrain_v1_1\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install --upgrade transformers peft trl accelerate datasets bitsandbytes sentencepiece jsonschema pydantic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 2: Authenticate with Hugging Face\n",
        "from huggingface_hub import login\n",
        "# You will be prompted to enter your Hugging Face token.\n",
        "login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 3: Generate a Small Batch of Raw Drafts (T4 Optimized)\n",
        "\n",
        "# Install Ollama if not present\n",
        "!if ! command -v ollama &> /dev/null; then curl -fsSL https://ollama.com/install.sh | sh; fi\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import glob\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "\n",
        "# Start Ollama server in the background\n",
        "with open(\"ollama_server.log\", \"w\") as log_file:\n",
        "    ollama_process = subprocess.Popen([\"ollama\", \"serve\"], stdout=log_file, stderr=subprocess.STDOUT)\n",
        "\n",
        "print(\"ðŸš€ Starting Ollama server...\")\n",
        "time.sleep(5)\n",
        "\n",
        "# Health check loop\n",
        "print(\"... Waiting for Ollama server to become available...\")\n",
        "server_ready = False\n",
        "for _ in range(36):\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434\")\n",
        "        if response.status_code == 200:\n",
        "            print(\"âœ… Ollama server is up and running!\")\n",
        "            server_ready = True\n",
        "            break\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        time.sleep(5)\n",
        "\n",
        "if server_ready:\n",
        "    print(\"\\\\nâ³ Downloading deepseek-coder model...\")\n",
        "    !ollama pull deepseek-coder:6.7b-instruct\n",
        "\n",
        "    print(\"\\\\nâ³ Starting Raw Draft generation (4 parallel workers)...\")\n",
        "    processes = []\n",
        "    num_workers = 4  # Reduced for T4\n",
        "    num_examples = 20 # Small batch for a quick test\n",
        "    output_dir = \"data/training/silver_standard_raw\"\n",
        "\n",
        "    if os.path.exists(output_dir):\n",
        "        get_ipython().system(f'rm -rf {output_dir}')\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "    for i in range(num_workers):\n",
        "        command = f\"python scripts/generate_raw_drafts.py --num-examples {num_examples} --num-workers {num_workers} --worker-id {i}\"\n",
        "        log_file = open(f\"worker_{i}.log\", \"w\")\n",
        "        proc = subprocess.Popen(command, shell=True, stdout=log_file, stderr=subprocess.STDOUT)\n",
        "        processes.append((proc, log_file))\n",
        "\n",
        "    total_examples_to_generate = num_examples\n",
        "    \n",
        "    while any(p.poll() is None for p, _ in processes):\n",
        "        clear_output(wait=True)\n",
        "        generated_files = glob.glob(f\"{output_dir}/*.json\")\n",
        "        progress_percentage = (len(generated_files) / total_examples_to_generate) * 100\n",
        "        progress_bar = f\"[{'#' * int(progress_percentage / 4)}{'.' * (25 - int(progress_percentage / 4))}]\"\n",
        "        \n",
        "        print(\"--- Generating Raw Drafts (Mock Run) ---\")\n",
        "        print(f\"Progress: {progress_bar} {len(generated_files)}/{total_examples_to_generate} raw drafts generated ({progress_percentage:.2f}%)\\\\n\")\n",
        "        get_ipython().system('tail -n 3 worker_*.log')\n",
        "        \n",
        "        time.sleep(15)\n",
        "    \n",
        "    clear_output(wait=True)\n",
        "    generated_files = glob.glob(f\"{output_dir}/*.json\")\n",
        "    print(f\"--- Final Count ---\")\n",
        "    print(f\"âœ… Generated a total of {len(generated_files)} raw drafts.\")\n",
        "    \n",
        "    for proc, log_file in processes:\n",
        "        proc.wait()\n",
        "        log_file.close()\n",
        "\n",
        "    print(\"\\\\n\\\\nâœ… Raw draft generation complete.\")\n",
        "    print(\"NEXT STEP: Download the 'data/training/silver_standard_raw' directory and use 'scripts/refine_drafts.py' locally.\")\n",
        "\n",
        "else:\n",
        "    print(\"ðŸ”´ Ollama server failed to start.\")\n",
        "    get_ipython().system('cat ollama_server.log')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### **â›” STOP: Manual Refinement Required â›”**\n",
        "\n",
        "1.  **Download the `data/training/silver_standard_raw` directory** from this Colab instance to your local machine.\n",
        "2.  On your local machine, run the command: `python scripts/refine_drafts.py`\n",
        "3.  Follow the interactive prompts to review, edit, and validate each draft, creating the final `data/training/silver_standard` dataset.\n",
        "4.  **Upload the perfected `data/training/silver_standard` directory** to your GitHub repository.\n",
        "5.  Once the perfected data is on GitHub, you may proceed with the cells below.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 4: Prepare Refined Datasets for Fine-Tuning\n",
        "# This step assumes you have completed the manual refinement process and the\n",
        "# perfected data is now in the `data/training/silver_standard` directory on GitHub.\n",
        "\n",
        "# We need to pull the latest changes from the repo to get the refined data\n",
        "!git pull\n",
        "\n",
        "!python scripts/prepare_data_for_finetuning.py \\\n",
        "    --input-dir data/training/gold_standard \\\n",
        "    --output-dir data/training/gold_standard_finetune_ready\n",
        "\n",
        "!python scripts/prepare_data_for_finetuning.py \\\n",
        "    --input-dir data/training/silver_standard \\\n",
        "    --output-dir data/training/silver_standard_finetune_ready\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 5: Run Fine-Tuning (T4 Optimized Mock Run)\n",
        "!python scripts/run_finetuning.py \\\n",
        "    --model_id \"deepseek-ai/deepseek-coder-6.7b-instruct\" \\\n",
        "    --dataset_path \"data/training/gold_standard_finetune_ready\" \"data/training/silver_standard_finetune_ready\" \\\n",
        "    --output_dir \"models/housebrain-v1.0-mock-t4\" \\\n",
        "    --epochs 3 \\\n",
        "    --batch_size 1 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --use_4bit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 6: (Optional) Download the Trained Model Adapter\n",
        "!zip -r housebrain-v1.0-mock-t4-adapter.zip models/housebrain-v1.0-mock-t4\n",
        "\n",
        "from google.colab import files\n",
        "files.download('housebrain-v1.0-mock-t4-adapter.zip')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain Mock Run on T4 GPU\n",
        "\n",
        "This notebook is designed for a quick, low-cost \"mock run\" to verify the entire data generation and fine-tuning pipeline on a standard T4 GPU.\n",
        "\n",
        "**Changes from the main A100 notebook:**\n",
        "- Generates only **5** \"Silver Standard\" examples.\n",
        "- Uses a **batch size of 1** to fit in T4 memory.\n",
        "- Trains for only **3 epochs** for a faster test run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 1: Set Up the Environment\n",
        "# -----------------\n",
        "# IMPORTANT: PASTE YOUR GITHUB TOKEN HERE\n",
        "# -----------------\n",
        "import os\n",
        "GITHUB_TOKEN = \"\" # PASTE YOUR GITHUB TOKEN HERE\n",
        "os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n",
        "\n",
        "# Clone the repository using your token for private access\n",
        "!git clone https://$GITHUB_TOKEN@github.com/Vinay-O/HouseBrainLLM.git housebrain_v1_1\n",
        "%cd housebrain_v1_1\n",
        "\n",
        "# Install the necessary libraries\n",
        "!pip install --upgrade transformers peft trl accelerate datasets bitsandbytes sentencepiece jsonschema pydantic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 2: Authenticate with Hugging Face\n",
        "from huggingface_hub import login\n",
        "login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 3: Generate \"Silver Standard\" Dataset in Parallel\n",
        "\n",
        "# Install Ollama in the Colab environment if it's not already present\n",
        "!if ! command -v ollama &> /dev/null; then curl -fsSL https://ollama.com/install.sh | sh; fi\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Start the Ollama server as a background process\n",
        "with open(\"ollama_server.log\", \"w\") as log_file:\n",
        "    ollama_process = subprocess.Popen([\"ollama\", \"serve\"], stdout=log_file, stderr=subprocess.STDOUT)\n",
        "\n",
        "print(\"ðŸš€ Starting Ollama server in the background...\")\n",
        "time.sleep(5)\n",
        "\n",
        "# Health Check Loop\n",
        "max_wait_time = 180\n",
        "start_time = time.time()\n",
        "server_ready = False\n",
        "print(\"... Waiting for Ollama server to become available...\")\n",
        "while time.time() - start_time < max_wait_time:\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434\")\n",
        "        if response.status_code == 200:\n",
        "            server_ready = True\n",
        "            print(\"âœ… Ollama server is up and running!\")\n",
        "            break\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        time.sleep(5)\n",
        "else:\n",
        "    print(\"âŒ Timed out waiting for Ollama server to start.\")\n",
        "\n",
        "# Model Download, Verification, and Parallel Data Generation\n",
        "if server_ready:\n",
        "    print(\"\\\\nâ³ Downloading the deepseek-coder model...\")\n",
        "    !ollama pull deepseek-coder:6.7b-instruct\n",
        "    print(\"âœ… Model download complete.\")\n",
        "\n",
        "    print(\"\\\\nðŸ“‹ Verifying installed models...\")\n",
        "    !ollama list\n",
        "    print(\"------------------------------------\\\\n\")\n",
        "\n",
        "    print(\"â³ Starting the Silver Standard data generation process in parallel (4 workers)...\")\n",
        "    # Launch 4 worker processes in the background\n",
        "    processes = []\n",
        "    for i in range(4):\n",
        "        command = f\"python scripts/generate_silver_standard_data.py --num-examples 100 --num-workers 4 --worker-id {i}\"\n",
        "        # Redirect output to worker-specific log files\n",
        "        log_file = open(f\"worker_{i}.log\", \"w\")\n",
        "        proc = subprocess.Popen(command, shell=True, stdout=log_file, stderr=subprocess.STDOUT)\n",
        "        processes.append((proc, log_file))\n",
        "\n",
        "    # Wait for all processes to complete\n",
        "    for proc, log_file in processes:\n",
        "        proc.wait()\n",
        "        log_file.close()\n",
        "\n",
        "    print(\"\\\\nâœ… All data generation workers have finished.\")\n",
        "    # Optional: print the logs from each worker\n",
        "    # for i in range(4):\n",
        "    #   print(f\"--- Worker {i} Log ---\")\n",
        "    #   !cat worker_{i}.log\n",
        "else:\n",
        "    print(\"ðŸ”´ Ollama server failed to start. Cannot proceed.\")\n",
        "    print(\"ðŸ“œ Server logs:\")\n",
        "    !cat ollama_server.log\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 4: Prepare All Datasets for Fine-Tuning\n",
        "\n",
        "# Prepare the Gold Standard dataset\n",
        "!python scripts/prepare_data_for_finetuning.py \\\n",
        "    --input-dir data/training/gold_standard \\\n",
        "    --output-dir data/training/gold_standard_finetune_ready\n",
        "\n",
        "# Prepare the newly generated Silver Standard dataset\n",
        "!python scripts/prepare_data_for_finetuning.py \\\n",
        "    --input-dir data/training/silver_standard \\\n",
        "    --output-dir data/training/silver_standard_finetune_ready\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 5: Run the Fine-Tuning Script (T4 Optimized)\n",
        "\n",
        "!python scripts/run_finetuning.py \\\n",
        "    --model_id deepseek-ai/deepseek-coder:6.7b-instruct \\\n",
        "    --dataset_path data/training/gold_standard_finetune_ready data/training/silver_standard_finetune_ready \\\n",
        "    --output_dir models/housebrain-mock-t4 \\\n",
        "    --epochs 3 \\\n",
        "    --batch_size 1 \\\n",
        "    --learning_rate 0.0002 \\\n",
        "    --use_4bit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 6: (Optional) Download the Trained Mock Model\n",
        "\n",
        "!zip -r housebrain-mock-t4-adapter.zip models/housebrain-mock-t4\n",
        "\n",
        "from google.colab import files\n",
        "files.download('housebrain-mock-t4-adapter.zip')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
