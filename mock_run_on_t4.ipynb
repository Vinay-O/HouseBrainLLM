{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain Mock Run on T4 GPU\n",
        "\n",
        "This notebook is designed for a quick, low-cost \"mock run\" to verify the entire data generation and fine-tuning pipeline on a standard T4 GPU.\n",
        "\n",
        "**Changes from the main A100 notebook:**\n",
        "- Generates only **5** \"Silver Standard\" examples.\n",
        "- Uses a **batch size of 1** to fit in T4 memory.\n",
        "- Trains for only **3 epochs** for a faster test run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 1: Set Up the Environment\n",
        "# -----------------\n",
        "# IMPORTANT: PASTE YOUR GITHUB TOKEN HERE\n",
        "# -----------------\n",
        "import os\n",
        "GITHUB_TOKEN = \"\" # PASTE YOUR GITHUB TOKEN HERE\n",
        "os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n",
        "\n",
        "# Clone the repository using your token for private access\n",
        "!git clone https://$GITHUB_TOKEN@github.com/Vinay-O/HouseBrainLLM.git housebrain_v1_1\n",
        "%cd housebrain_v1_1\n",
        "\n",
        "# Install the necessary libraries\n",
        "!pip install --upgrade transformers peft trl accelerate datasets bitsandbytes sentencepiece jsonschema pydantic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 2: Authenticate with Hugging Face\n",
        "from huggingface_hub import login\n",
        "login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 3: Generate \"Silver Standard\" Dataset in Parallel\n",
        "\n",
        "# Install Ollama in the Colab environment if it's not already present\n",
        "!if ! command -v ollama &> /dev/null; then curl -fsSL https://ollama.com/install.sh | sh; fi\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Start the Ollama server as a background process\n",
        "with open(\"ollama_server.log\", \"w\") as log_file:\n",
        "    ollama_process = subprocess.Popen([\"ollama\", \"serve\"], stdout=log_file, stderr=subprocess.STDOUT)\n",
        "\n",
        "print(\"ðŸš€ Starting Ollama server in the background...\")\n",
        "time.sleep(5)\n",
        "\n",
        "# Health Check Loop\n",
        "max_wait_time = 180\n",
        "start_time = time.time()\n",
        "server_ready = False\n",
        "print(\"... Waiting for Ollama server to become available...\")\n",
        "while time.time() - start_time < max_wait_time:\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434\")\n",
        "        if response.status_code == 200:\n",
        "            server_ready = True\n",
        "            print(\"âœ… Ollama server is up and running!\")\n",
        "            break\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        time.sleep(5)\n",
        "else:\n",
        "    print(\"âŒ Timed out waiting for Ollama server to start.\")\n",
        "\n",
        "# Model Download, Verification, and Parallel Data Generation\n",
        "if server_ready:\n",
        "    print(\"\\\\nâ³ Downloading the deepseek-coder model...\")\n",
        "    !ollama pull deepseek-coder:6.7b-instruct\n",
        "    print(\"âœ… Model download complete.\")\n",
        "\n",
        "    print(\"\\\\nðŸ“‹ Verifying installed models...\")\n",
        "    !ollama list\n",
        "    print(\"------------------------------------\\\\n\")\n",
        "\n",
        "    print(\"â³ Starting the Silver Standard data generation process in parallel (4 workers)...\")\n",
        "    # Launch 4 worker processes in the background\n",
        "    processes = []\n",
        "    for i in range(4):\n",
        "        command = f\"python scripts/generate_silver_standard_data.py --num-examples 100 --num-workers 4 --worker-id {i}\"\n",
        "        # Redirect output to worker-specific log files\n",
        "        log_file = open(f\"worker_{i}.log\", \"w\")\n",
        "        proc = subprocess.Popen(command, shell=True, stdout=log_file, stderr=subprocess.STDOUT)\n",
        "        processes.append((proc, log_file))\n",
        "\n",
        "    # Wait for all processes to complete\n",
        "    for proc, log_file in processes:\n",
        "        proc.wait()\n",
        "        log_file.close()\n",
        "\n",
        "    print(\"\\\\nâœ… All data generation workers have finished.\")\n",
        "    # Optional: print the logs from each worker\n",
        "    # for i in range(4):\n",
        "    #   print(f\"--- Worker {i} Log ---\")\n",
        "    #   !cat worker_{i}.log\n",
        "else:\n",
        "    print(\"ðŸ”´ Ollama server failed to start. Cannot proceed.\")\n",
        "    print(\"ðŸ“œ Server logs:\")\n",
        "    !cat ollama_server.log\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 4: Prepare All Datasets for Fine-Tuning\n",
        "\n",
        "# Prepare the Gold Standard dataset\n",
        "!python scripts/prepare_data_for_finetuning.py \\\n",
        "    --input-dir data/training/gold_standard \\\n",
        "    --output-dir data/training/gold_standard_finetune_ready\n",
        "\n",
        "# Prepare the newly generated Silver Standard dataset\n",
        "!python scripts/prepare_data_for_finetuning.py \\\n",
        "    --input-dir data/training/silver_standard \\\n",
        "    --output-dir data/training/silver_standard_finetune_ready\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 5: Run the Fine-Tuning Script (T4 Optimized)\n",
        "\n",
        "!python scripts/run_finetuning.py \\\n",
        "    --model_id deepseek-ai/deepseek-coder:6.7b-instruct \\\n",
        "    --dataset_path data/training/gold_standard_finetune_ready data/training/silver_standard_finetune_ready \\\n",
        "    --output_dir models/housebrain-mock-t4 \\\n",
        "    --epochs 3 \\\n",
        "    --batch_size 1 \\\n",
        "    --learning_rate 0.0002 \\\n",
        "    --use_4bit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 6: (Optional) Download the Trained Mock Model\n",
        "\n",
        "!zip -r housebrain-mock-t4-adapter.zip models/housebrain-mock-t4\n",
        "\n",
        "from google.colab import files\n",
        "files.download('housebrain-mock-t4-adapter.zip')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
