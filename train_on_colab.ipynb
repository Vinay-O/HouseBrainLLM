{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bc0b23",
   "metadata": {},
   "source": [
    "# HouseBrain: Production Fine-Tuning on Google Colab\n",
    "\n",
    "This notebook provides the complete, end-to-end workflow for fine-tuning a production-grade Large Language Model for architectural design. It uses our full **20-example \"Gold Standard\" dataset** to teach a powerful base model (`meta-llama/Llama-2-7b-chat-hf`) the specific schema and nuances of Indian residential architecture.\n",
    "\n",
    "**GPU Requirement:** This notebook is designed for a T4 GPU, which is available in the free tier of Google Colab. For faster results, you can use an A100 on Colab Pro+.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873d9dc",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "This step clones the project repository from GitHub and installs all the necessary Python packages for fine-tuning, including `transformers`, `peft`, `trl`, and `bitsandbytes` for memory-efficient 4-bit training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/your-username/housebrain_v1_1.git\n",
    "%cd housebrain_v1_1\n",
    "\n",
    "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca665d",
   "metadata": {},
   "source": [
    "## Step 3: Prepare the Gold Standard Data\n",
    "\n",
    "This step runs our preparation script. It will process the 20 raw Gold Standard JSON files and create a new `gold_standard_finetune_ready` directory containing the data in the simple `{\"prompt\": \"...\", \"output\": \"...\"}` format required by the training script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/prepare_gold_standard_data.py\n",
    "!echo \"\\nâœ… Data preparation complete. Verifying the new directory:\"\n",
    "!ls -l data/training/gold_standard_finetune_ready | wc -l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbc72b",
   "metadata": {},
   "source": [
    "## Step 4: Run the Fine-Tuning Script\n",
    "\n",
    "This is the core of the process. We execute the `run_finetuning.py` script, which will:\n",
    "\n",
    "1.  **Load** our 20 prepared Gold Standard examples.\n",
    "2.  **Download** the base `Llama-2-7b-chat-hf` model from Hugging Face.\n",
    "3.  **Configure** 4-bit quantization and LoRA for efficient training.\n",
    "4.  **Fine-tune** the model on our data.\n",
    "5.  **Save** the final, specialized `housebrain-llama2-7b-v0.1` model to the `models/` directory.\n",
    "\n",
    "We will use a high number of epochs (e.g., 200) because our dataset is very high-quality but small. This is necessary to ensure the model learns the schema thoroughly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ac353",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/run_finetuning.py \\\n",
    "    --dataset-path \"data/training/gold_standard_finetune_ready\" \\\n",
    "    --base-model \"meta-llama/Llama-2-7b-chat-hf\" \\\n",
    "    --output-path \"models/housebrain-llama2-7b-v0.1\" \\\n",
    "    --epochs 200 \\\n",
    "    --batch-size 2 \\\n",
    "    --learning-rate 2e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ed78e",
   "metadata": {},
   "source": [
    "## Step 5: Next Steps - Using Your Fine-Tuned Model\n",
    "\n",
    "Once training is complete, the new model is saved in the `models/housebrain-llama2-7b-v0.1` directory. \n",
    "\n",
    "You can now use this specialized model in your `generate_validated_silver_data.py` script (by changing the model ID) to generate a large, high-quality dataset of thousands of examples. This is the path to a truly production-ready system.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
