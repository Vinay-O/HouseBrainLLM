{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain Model Fine-Tuning on Google Colab\n",
        "\n",
        "This notebook provides a step-by-step guide to fine-tune the HouseBrain model using Google Colab's free GPU resources. This is the recommended way to train the model, as it avoids local hardware limitations (RAM, GPU, CUDA drivers).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Set Up the Environment\n",
        "\n",
        "First, we need to connect to a Colab runtime (a T4 GPU is recommended and free). Then, we will clone the project repository from GitHub and install the required Python libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 1: Set Up the Environment\n",
        "# -----------------\n",
        "# IMPORTANT: PASTE YOUR GITHUB TOKEN HERE\n",
        "# -----------------\n",
        "import os\n",
        "GITHUB_TOKEN = \"\" # PASTE YOUR GITHUB TOKEN HERE\n",
        "os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n",
        "\n",
        "# Clone the repository using your token for private access\n",
        "!git clone https://$GITHUB_TOKEN@github.com/Vinay-O/HouseBrainLLM.git housebrain_v1_1\n",
        "%cd housebrain_v1_1\n",
        "\n",
        "# Install the necessary libraries\n",
        "# We let the libraries automatically install the correct, compatible version of PyTorch\n",
        "!pip install --upgrade transformers peft trl accelerate datasets bitsandbytes sentencepiece jsonschema pydantic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Authenticate with Hugging Face\n",
        "\n",
        "To download the base model from Hugging Face, you need to provide an access token. You can get a token from your Hugging Face account settings.\n",
        "\n",
        "When you run the cell below, a login box will appear. Paste your token there.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Generate High-Quality \"Silver Standard\" Data\n",
        "\n",
        "Before training, we will leverage the Colab A100 GPU to generate a larger, high-quality dataset. The `generate_silver_standard_data.py` script uses a powerful generate-and-refine loop to create architecturally sound examples automatically. We will generate 100 new examples.\n",
        "\n",
        "**Note:** This step will take some time as it involves hundreds of LLM calls, but it is crucial for model quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Ollama in the Colab environment if it's not already present\n",
        "!if ! command -v ollama &> /dev/null; then curl -fsSL https://ollama.com/install.sh | sh; fi\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Start the Ollama server as a background process\n",
        "with open(\"ollama_server.log\", \"w\") as log_file:\n",
        "    ollama_process = subprocess.Popen([\"ollama\", \"serve\"], stdout=log_file, stderr=subprocess.STDOUT)\n",
        "\n",
        "print(\"🚀 Starting Ollama server in the background...\")\n",
        "time.sleep(5) # Give it a moment to initialize\n",
        "\n",
        "# --- Health Check Loop ---\n",
        "# Wait for the Ollama server to be ready by polling the API endpoint\n",
        "max_wait_time = 180  # 3 minutes\n",
        "start_time = time.time()\n",
        "server_ready = False\n",
        "print(\"... Waiting for Ollama server to become available...\")\n",
        "while time.time() - start_time < max_wait_time:\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434\")\n",
        "        if response.status_code == 200:\n",
        "            server_ready = True\n",
        "            print(\"✅ Ollama server is up and running!\")\n",
        "            break\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        time.sleep(5) # Wait 5 seconds before retrying\n",
        "else:\n",
        "    print(\"❌ Timed out waiting for Ollama server to start.\")\n",
        "    # You might want to handle this error, e.g., by raising an exception\n",
        "    # For now, we'll let it proceed and likely fail on the next step, which will show the error.\n",
        "\n",
        "# --- Model Download and Verification ---\n",
        "if server_ready:\n",
        "    print(\"\\\\n⏳ Downloading the deepseek-coder model (approx. 4-5 GB)...\")\n",
        "    !ollama pull deepseek-coder:6.7b-instruct\n",
        "    print(\"✅ Model download complete.\")\n",
        "\n",
        "    print(\"\\\\n📋 Verifying installed models...\")\n",
        "    !ollama list\n",
        "    print(\"------------------------------------\\\\n\")\n",
        "\n",
        "    print(\"⏳ Starting the Silver Standard data generation process...\")\n",
        "    !python scripts/generate_silver_standard_data.py --num-examples 100\n",
        "else:\n",
        "    print(\"🔴 Ollama server failed to start. Cannot proceed with data generation.\")\n",
        "    print(\"📜 Server logs:\")\n",
        "    !cat ollama_server.log\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Prepare All Datasets for Fine-Tuning\n",
        "\n",
        "The fine-tuning script requires the `output` field in our JSON examples to be a string. The `prepare_data_for_finetuning.py` script handles this conversion. We will run it on both our original \"Gold\" dataset and our newly generated \"Silver\" dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare the Gold Standard dataset\n",
        "!python scripts/prepare_data_for_finetuning.py \\\n",
        "    --input-dir data/training/gold_standard \\\n",
        "    --output-dir data/training/gold_standard_finetune_ready\n",
        "\n",
        "# Prepare the newly generated Silver Standard dataset\n",
        "!python scripts/prepare_data_for_finetuning.py \\\n",
        "    --input-dir data/training/silver_standard \\\n",
        "    --output-dir data/training/silver_standard_finetune_ready\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Run the Fine-Tuning Script\n",
        "\n",
        "Now we are ready to fine-tune the model on our combined dataset. For a Colab Pro+ A100 environment, we can use a larger batch size and sequence length to accelerate training and improve performance.\n",
        "\n",
        "We will point the training script to both the `gold_standard_finetune_ready` and `silver_standard_finetune_ready` directories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python scripts/run_finetuning.py \\\n",
        "    --model_id deepseek-ai/deepseek-coder:6.7b-instruct \\\n",
        "    --dataset_path data/training/gold_standard_finetune_ready data/training/silver_standard_finetune_ready \\\n",
        "    --output_dir models/housebrain-v1.0-silver \\\n",
        "    --epochs 15 \\\n",
        "    --batch_size 4 \\\n",
        "    --learning_rate 0.0002 \\\n",
        "    --use_4bit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: (Optional) Download the Trained Model\n",
        "\n",
        "After training is complete, the new model adapter will be saved in the `models/housebrain-v1.0-silver` directory. You can zip it and download it to your local machine for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r housebrain-v1.0-silver-adapter.zip models/housebrain-v1.0-silver\n",
        "\n",
        "from google.colab import files\n",
        "files.download('housebrain-v1.0-silver-adapter.zip')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Run the Fine-Tuning Script\n",
        "\n",
        "Now we are ready to run the fine-tuning script. We will use 4-bit quantization (`--use_4bit`) to ensure the model fits comfortably within the Colab GPU's memory. The script will train the model on our 10 Gold Standard examples for 10 epochs and save the resulting LoRA adapter to the `models/housebrain-v0.1` directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python scripts/run_finetuning.py \\\n",
        "    --model_id deepseek-ai/deepseek-coder-6.7b-instruct \\\n",
        "    --dataset_path data/training/gold_standard_finetune_ready \\\n",
        "    --output_dir models/housebrain-v0.1 \\\n",
        "    --epochs 10 \\\n",
        "    --batch_size 1 \\\n",
        "    --learning_rate 0.0002 \\\n",
        "    --use_4bit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: (Optional) Download the Trained Model\n",
        "\n",
        "After training is complete, the new model adapter will be saved in the `models/housebrain-v0.1` directory inside the Colab environment. If you want to save it permanently, you can zip it and download it to your local machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r housebrain-v0.1-adapter.zip models/housebrain-v0.1\n",
        "\n",
        "from google.colab import files\n",
        "files.download('housebrain-v0.1-adapter.zip')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
