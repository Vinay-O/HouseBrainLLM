{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain Model Fine-Tuning on Google Colab (A100 Optimized)\n",
        "\n",
        "This notebook provides the definitive workflow for fine-tuning the HouseBrain model using a Google Colab Pro+ A100 environment. It includes a parallelized data generation step to maximize resource utilization and speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 1: Set Up the Environment\n",
        "import os\n",
        "GITHUB_TOKEN = \"\" # PASTE YOUR GITHUB TOKEN HERE\n",
        "os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n",
        "\n",
        "# Clone the repository using your token\n",
        "!git clone https://$GITHUB_TOKEN@github.com/Vinay-O/HouseBrainLLM.git housebrain_v1_1\n",
        "%cd housebrain_v1_1\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install --upgrade transformers peft trl accelerate datasets bitsandbytes sentencepiece jsonschema pydantic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 2: Authenticate with Hugging Face\n",
        "from huggingface_hub import login\n",
        "login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 3: Generate \"Silver Standard\" Dataset in Parallel (A100 Optimized)\n",
        "\n",
        "# Install Ollama if not present\n",
        "!if ! command -v ollama &> /dev/null; then curl -fsSL https://ollama.com/install.sh | sh; fi\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import glob\n",
        "\n",
        "# Start Ollama server in the background\n",
        "with open(\"ollama_server.log\", \"w\") as log_file:\n",
        "    ollama_process = subprocess.Popen([\"ollama\", \"serve\"], stdout=log_file, stderr=subprocess.STDOUT)\n",
        "\n",
        "print(\"üöÄ Starting Ollama server...\")\n",
        "time.sleep(5)\n",
        "\n",
        "# Health check loop\n",
        "print(\"... Waiting for Ollama server to become available...\")\n",
        "server_ready = False\n",
        "for _ in range(36): # Wait up to 3 minutes\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434\")\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ Ollama server is up and running!\")\n",
        "            server_ready = True\n",
        "            break\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        time.sleep(5)\n",
        "\n",
        "if server_ready:\n",
        "    print(\"\\\\n‚è≥ Downloading deepseek-coder model...\")\n",
        "    !ollama pull deepseek-coder:6.7b-instruct\n",
        "    print(\"‚úÖ Model download complete.\")\n",
        "    !ollama list\n",
        "\n",
        "    print(\"\\\\n‚è≥ Starting Silver Standard data generation (8 parallel workers)...\")\n",
        "    processes = []\n",
        "    num_workers = 8\n",
        "    num_examples = 100\n",
        "    # Ensure the output directory exists\n",
        "    !mkdir -p data/training/silver_standard\n",
        "\n",
        "    for i in range(num_workers):\n",
        "        command = f\"python scripts/generate_silver_standard_data.py --num-examples {num_examples} --num-workers {num_workers} --worker-id {i}\"\n",
        "        log_file = open(f\"worker_{i}.log\", \"w\")\n",
        "        proc = subprocess.Popen(command, shell=True, stdout=log_file, stderr=subprocess.STDOUT)\n",
        "        processes.append((proc, log_file))\n",
        "\n",
        "    print(\"\\\\n-- Monitoring data generation progress --\")\n",
        "    total_examples_to_generate = num_examples\n",
        "\n",
        "    # Loop until all worker processes have completed\n",
        "    while any(p.poll() is None for p, _ in processes):\n",
        "        # Count the number of .json files in the output directory\n",
        "        generated_files = glob.glob(\"data/training/silver_standard/*.json\")\n",
        "        \n",
        "        # Calculate and display progress\n",
        "        progress_percentage = (len(generated_files) / total_examples_to_generate) * 100\n",
        "        progress_bar = f\"[{'#' * int(progress_percentage / 4)}{'.' * (25 - int(progress_percentage / 4))}]\"\n",
        "        print(f\"Progress: {progress_bar} {len(generated_files)}/{total_examples_to_generate} files generated ({progress_percentage:.2f}%)\", end='\\\\r')\n",
        "        \n",
        "        # Wait for 30 seconds before the next update\n",
        "        time.sleep(30)\n",
        "    \n",
        "    # Final check to ensure we print 100% completion\n",
        "    generated_files = glob.glob(\"data/training/silver_standard/*.json\")\n",
        "    progress_percentage = (len(generated_files) / total_examples_to_generate) * 100\n",
        "    progress_bar = f\"[{'#' * int(progress_percentage / 4)}{'.' * (25 - int(progress_percentage / 4))}]\"\n",
        "    print(f\"Progress: {progress_bar} {len(generated_files)}/{total_examples_to_generate} files generated ({progress_percentage:.2f}%)\")\n",
        "\n",
        "    # Final wait to ensure all processes are truly finished and logs are written and closed\n",
        "    for proc, log_file in processes:\n",
        "        proc.wait()\n",
        "        log_file.close()\n",
        "\n",
        "    print(\"\\\\n\\\\n‚úÖ All data generation workers have finished.\")\n",
        "\n",
        "else:\n",
        "    print(\"üî¥ Ollama server failed to start. Cannot proceed.\")\n",
        "    !cat ollama_server.log\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 4: Prepare All Datasets for Fine-Tuning\n",
        "# Prepare the Gold Standard dataset\n",
        "!python scripts/prepare_data_for_finetuning.py \\\n",
        "    --input-dir data/training/gold_standard \\\n",
        "    --output-dir data/training/gold_standard_finetune_ready\n",
        "\n",
        "# Prepare the newly generated Silver Standard dataset\n",
        "!python scripts/prepare_data_for_finetuning.py \\\n",
        "    --input-dir data/training/silver_standard \\\n",
        "    --output-dir data/training/silver_standard_finetune_ready\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 5: Run Fine-Tuning (A100 Optimized)\n",
        "!python scripts/run_finetuning.py \\\n",
        "    --model_id deepseek-ai/deepseek-coder:6.7b-instruct \\\n",
        "    --dataset_path data/training/gold_standard_finetune_ready data/training/silver_standard_finetune_ready \\\n",
        "    --output_dir models/housebrain-v1.0-silver \\\n",
        "    --epochs 15 \\\n",
        "    --batch_size 4 \\\n",
        "    --learning_rate 0.0002 \\\n",
        "    --use_4bit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 6: (Optional) Download the Trained Model\n",
        "!zip -r housebrain-v1.0-silver-adapter.zip models/housebrain-v1.0-silver\n",
        "\n",
        "from google.colab import files\n",
        "files.download('housebrain-v1.0-silver-adapter.zip')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
