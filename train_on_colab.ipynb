{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain Model Fine-Tuning on Google Colab (A100 Optimized)\n",
        "\n",
        "This notebook provides the definitive workflow for fine-tuning the HouseBrain model using a Google Colab Pro+ A100 environment. It includes a parallelized data generation step with a live dashboard to monitor progress and worker status.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 1: Set Up the Environment\n",
        "import os\n",
        "# IMPORTANT: PASTE YOUR GITHUB PERSONAL ACCESS TOKEN HERE\n",
        "GITHUB_TOKEN = \"\"\n",
        "os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n",
        "\n",
        "# Clone the repository using your token\n",
        "!git clone https://$GITHUB_TOKEN@github.com/Vinay-O/HouseBrainLLM.git housebrain_v1_1\n",
        "%cd housebrain_v1_1\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install --upgrade transformers peft trl accelerate datasets bitsandbytes sentencepiece jsonschema pydantic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 2: Authenticate with Hugging Face\n",
        "from huggingface_hub import login\n",
        "# You will be prompted to enter your Hugging Face token.\n",
        "login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 3: Generate Raw Drafts for Human Refinement (A100 Optimized)\n",
        "# This step now focuses on generating a large volume of unvalidated drafts as quickly as possible.\n",
        "# The actual data refinement and validation will be done offline using the `refine_drafts.py` script.\n",
        "\n",
        "# Install Ollama if not present\n",
        "!if ! command -v ollama &> /dev/null; then curl -fsSL https://ollama.com/install.sh | sh; fi\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import glob\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "\n",
        "# Start Ollama server in the background\n",
        "with open(\"ollama_server.log\", \"w\") as log_file:\n",
        "    ollama_process = subprocess.Popen([\"ollama\", \"serve\"], stdout=log_file, stderr=subprocess.STDOUT)\n",
        "\n",
        "print(\"üöÄ Starting Ollama server...\")\n",
        "time.sleep(5)\n",
        "\n",
        "# Health check loop\n",
        "print(\"... Waiting for Ollama server to become available...\")\n",
        "server_ready = False\n",
        "for _ in range(36):\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:11434\")\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ Ollama server is up and running!\")\n",
        "            server_ready = True\n",
        "            break\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        time.sleep(5)\n",
        "\n",
        "if server_ready:\n",
        "    print(\"\\\\n‚è≥ Downloading deepseek-coder model...\")\n",
        "    !ollama pull deepseek-coder:6.7b-instruct\n",
        "    print(\"‚úÖ Model download complete.\")\n",
        "    !ollama list\n",
        "\n",
        "    print(\"\\\\n‚è≥ Starting Raw Draft generation (8 parallel workers)...\")\n",
        "    processes = []\n",
        "    num_workers = 8\n",
        "    num_examples = 200 # Let's generate a larger pool of raw drafts\n",
        "    output_dir = \"data/training/silver_standard_raw\"\n",
        "\n",
        "    # Ensure the output directory exists and is empty\n",
        "    if os.path.exists(output_dir):\n",
        "        get_ipython().system(f'rm -rf {output_dir}')\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "    for i in range(num_workers):\n",
        "        command = f\"python scripts/generate_raw_drafts.py --num-examples {num_examples} --num-workers {num_workers} --worker-id {i}\"\n",
        "        log_file = open(f\"worker_{i}.log\", \"w\")\n",
        "        proc = subprocess.Popen(command, shell=True, stdout=log_file, stderr=subprocess.STDOUT)\n",
        "        processes.append((proc, log_file))\n",
        "\n",
        "    total_examples_to_generate = num_examples\n",
        "    \n",
        "    while any(p.poll() is None for p, _ in processes):\n",
        "        clear_output(wait=True)\n",
        "        generated_files = glob.glob(f\"{output_dir}/*.json\")\n",
        "        progress_percentage = (len(generated_files) / total_examples_to_generate) * 100\n",
        "        progress_bar = f\"[{'#' * int(progress_percentage / 4)}{'.' * (25 - int(progress_percentage / 4))}]\"\n",
        "        \n",
        "        print(\"--- Generating Raw Drafts ---\")\n",
        "        print(f\"Progress: {progress_bar} {len(generated_files)}/{total_examples_to_generate} raw drafts generated ({progress_percentage:.2f}%)\\\\n\")\n",
        "        print(\"--- Live Worker Status (last 3 lines of logs) ---\")\n",
        "        get_ipython().system('tail -n 3 worker_*.log')\n",
        "        \n",
        "        time.sleep(20)\n",
        "    \n",
        "    # Final update\n",
        "    clear_output(wait=True)\n",
        "    generated_files = glob.glob(f\"{output_dir}/*.json\")\n",
        "    print(f\"--- Final Count ---\")\n",
        "    print(f\"‚úÖ Generated a total of {len(generated_files)} raw drafts.\")\n",
        "    \n",
        "    for proc, log_file in processes:\n",
        "        proc.wait()\n",
        "        log_file.close()\n",
        "\n",
        "    print(\"\\\\n\\\\n‚úÖ All raw draft generation workers have finished.\")\n",
        "    print(\"NEXT STEP: Download the 'data/training/silver_standard_raw' directory and use 'scripts/refine_drafts.py' locally to create the final dataset.\")\n",
        "\n",
        "else:\n",
        "    print(\"üî¥ Ollama server failed to start. Cannot proceed.\")\n",
        "    get_ipython().system('cat ollama_server.log')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 4: Prepare All Datasets for Fine-Tuning\n",
        "!python scripts/prepare_data_for_finetuning.py \\\n",
        "    --input-dir data/training/gold_standard \\\n",
        "    --output-dir data/training/gold_standard_finetune_ready\n",
        "\n",
        "!python scripts/prepare_data_for_finetuning.py \\\n",
        "    --input-dir data/training/silver_standard \\\n",
        "    --output-dir data/training/silver_standard_finetune_ready\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 5: Run Fine-Tuning (A100 Optimized)\n",
        "!python scripts/run_finetuning.py \\\n",
        "    --model_id \"deepseek-ai/deepseek-coder-6.7b-instruct\" \\\n",
        "    --dataset_path \"data/training/gold_standard_finetune_ready\" \"data/training/silver_standard_finetune_ready\" \\\n",
        "    --output_dir \"models/housebrain-v1.0-silver\" \\\n",
        "    --epochs 15 \\\n",
        "    --batch_size 4 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --use_4bit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Step 6: (Optional) Download the Trained Model Adapter\n",
        "!zip -r housebrain-v1.0-silver-adapter.zip models/housebrain-v1.0-silver\n",
        "\n",
        "from google.colab import files\n",
        "files.download('housebrain-v1.0-silver-adapter.zip')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
