{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain Model Fine-Tuning on Google Colab\n",
        "\n",
        "This notebook provides a step-by-step guide to fine-tune the HouseBrain model using Google Colab's free GPU resources. This is the recommended way to train the model, as it avoids local hardware limitations (RAM, GPU, CUDA drivers).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Set Up the Environment\n",
        "\n",
        "First, we need to connect to a Colab runtime (a T4 GPU is recommended and free). Then, we will clone the project repository from GitHub and install the required Python libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository (replace with your repository URL)\n",
        "!git clone https://github.com/your-username/housebrain_v1_1.git\n",
        "%cd housebrain_v1_1\n",
        "\n",
        "# Install the necessary libraries\n",
        "# We install a specific version of torch compatible with Colab's CUDA version\n",
        "!pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install --upgrade transformers peft trl accelerate datasets bitsandbytes sentencepiece jsonschema pydantic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Authenticate with Hugging Face\n",
        "\n",
        "To download the base model from Hugging Face, you need to provide an access token. You can get a token from your Hugging Face account settings.\n",
        "\n",
        "When you run the cell below, a login box will appear. Paste your token there.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Generate High-Quality \"Silver Standard\" Data\n",
        "\n",
        "Before training, we will leverage the Colab A100 GPU to generate a larger, high-quality dataset. The `generate_silver_standard_data.py` script uses a powerful generate-and-refine loop to create architecturally sound examples automatically. We will generate 100 new examples.\n",
        "\n",
        "**Note:** This step will take some time as it involves hundreds of LLM calls, but it is crucial for model quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, ensure the local Ollama model is being served correctly in the background\n",
        "# This command starts the server. You may need to run the next cell in a few moments.\n",
        "!ollama serve &\n",
        "\n",
        "# Wait a few seconds for the server to initialize\n",
        "import time\n",
        "time.sleep(15)\n",
        "\n",
        "# Pull the model to ensure it's available\n",
        "!ollama pull deepseek-coder:6.7b-instruct\n",
        "\n",
        "# Now, run the data generation script\n",
        "!python scripts/generate_silver_standard_data.py --num-examples 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Prepare All Datasets for Fine-Tuning\n",
        "\n",
        "The fine-tuning script requires the `output` field in our JSON examples to be a string. The `prepare_data_for_finetuning.py` script handles this conversion. We will run it on both our original \"Gold\" dataset and our newly generated \"Silver\" dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare the Gold Standard dataset\n",
        "!python scripts/prepare_data_for_finetuning.py \\\n",
        "    --input-dir data/training/gold_standard \\\n",
        "    --output-dir data/training/gold_standard_finetune_ready\n",
        "\n",
        "# Prepare the newly generated Silver Standard dataset\n",
        "!python scripts/prepare_data_for_finetuning.py \\\n",
        "    --input-dir data/training/silver_standard \\\n",
        "    --output-dir data/training/silver_standard_finetune_ready\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Run the Fine-Tuning Script\n",
        "\n",
        "Now we are ready to fine-tune the model on our combined dataset. For a Colab Pro+ A100 environment, we can use a larger batch size and sequence length to accelerate training and improve performance.\n",
        "\n",
        "We will point the training script to both the `gold_standard_finetune_ready` and `silver_standard_finetune_ready` directories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python scripts/run_finetuning.py \\\n",
        "    --model_id deepseek-ai/deepseek-coder:6.7b-instruct \\\n",
        "    --dataset_path data/training/gold_standard_finetune_ready data/training/silver_standard_finetune_ready \\\n",
        "    --output_dir models/housebrain-v1.0-silver \\\n",
        "    --epochs 15 \\\n",
        "    --batch_size 4 \\\n",
        "    --learning_rate 0.0002 \\\n",
        "    --use_4bit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: (Optional) Download the Trained Model\n",
        "\n",
        "After training is complete, the new model adapter will be saved in the `models/housebrain-v1.0-silver` directory. You can zip it and download it to your local machine for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r housebrain-v1.0-silver-adapter.zip models/housebrain-v1.0-silver\n",
        "\n",
        "from google.colab import files\n",
        "files.download('housebrain-v1.0-silver-adapter.zip')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Run the Fine-Tuning Script\n",
        "\n",
        "Now we are ready to run the fine-tuning script. We will use 4-bit quantization (`--use_4bit`) to ensure the model fits comfortably within the Colab GPU's memory. The script will train the model on our 10 Gold Standard examples for 10 epochs and save the resulting LoRA adapter to the `models/housebrain-v0.1` directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python scripts/run_finetuning.py \\\n",
        "    --model_id deepseek-ai/deepseek-coder-6.7b-instruct \\\n",
        "    --dataset_path data/training/gold_standard_finetune_ready \\\n",
        "    --output_dir models/housebrain-v0.1 \\\n",
        "    --epochs 10 \\\n",
        "    --batch_size 1 \\\n",
        "    --learning_rate 0.0002 \\\n",
        "    --use_4bit\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: (Optional) Download the Trained Model\n",
        "\n",
        "After training is complete, the new model adapter will be saved in the `models/housebrain-v0.1` directory inside the Colab environment. If you want to save it permanently, you can zip it and download it to your local machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r housebrain-v0.1-adapter.zip models/housebrain-v0.1\n",
        "\n",
        "from google.colab import files\n",
        "files.download('housebrain-v0.1-adapter.zip')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
