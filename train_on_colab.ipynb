{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bc0b23",
   "metadata": {},
   "source": [
    "# HouseBrain: Production Fine-Tuning on Google Colab\n",
    "\n",
    "This notebook provides the complete, end-to-end workflow for fine-tuning a production-grade Large Language Model for architectural design. It uses our full **20-example \"Gold Standard\" dataset** to teach a powerful base model (`meta-llama/Llama-2-7b-chat-hf`) the specific schema and nuances of Indian residential architecture.\n",
    "\n",
    "**GPU Requirement:** This notebook is designed for a T4 GPU, which is available in the free tier of Google Colab. For faster results, you can use an A100 on Colab Pro+.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873d9dc",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "This step clones the project repository from GitHub and installs all the necessary Python packages for fine-tuning, including `transformers`, `peft`, `trl`, and `bitsandbytes` for memory-efficient 4-bit training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/your-username/housebrain_v1_1.git\n",
    "%cd housebrain_v1_1\n",
    "\n",
    "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b341d",
   "metadata": {},
   "source": [
    "## Step 3: (Optional) Generate New High-Quality Drafts\n",
    "\n",
    "This section is for creating new **Gold** or **Platinum** standard examples. It will set up an Ollama server within the Colab environment, download a powerful base model (`deepseek-r1:8b`), and use it to generate raw drafts based on expert prompts.\n",
    "\n",
    "**Workflow:**\n",
    "1.  Run the cells below to generate the raw `.json` draft files.\n",
    "2.  Download the generated files from the Colab file browser (under `data/training/gold_standard/` or `data/training/platinum_standard/`).\n",
    "3.  Use the AI assistant's \"Analyze and Repair\" process to perfect the drafts locally.\n",
    "4.  Upload the final, corrected `.json` files back to the appropriate directory before proceeding to the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama\n",
    "!if ! command -v ollama &> /dev/null; then curl -fsSL https://ollama.com/install.sh | sh; fi\n",
    "\n",
    "# Start Ollama in the background\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "async def run_process(cmd):\n",
    "  print('>>>', cmd)\n",
    "  proc = await asyncio.create_subprocess_shell(\n",
    "      cmd,\n",
    "      stdout=asyncio.subprocess.PIPE,\n",
    "      stderr=asyncio.subprocess.PIPE)\n",
    "  \n",
    "  # Log output in real-time\n",
    "  while True:\n",
    "      line = await proc.stdout.readline()\n",
    "      if not line:\n",
    "          break\n",
    "      print(line.decode().strip())\n",
    "      \n",
    "  await proc.wait()\n",
    "\n",
    "# Start ollama serve in the background\n",
    "asyncio.create_task(run_process('ollama serve > ollama.log 2>&1'))\n",
    "\n",
    "# Wait for Ollama to be ready\n",
    "import time\n",
    "import requests\n",
    "\n",
    "print(\"⏳ Waiting for Ollama server to start...\")\n",
    "time.sleep(5) # Initial wait\n",
    "for i in range(60): # Wait up to 60 seconds\n",
    "    try:\n",
    "        requests.get(\"http://localhost:11434\")\n",
    "        print(\"✅ Ollama server is running!\")\n",
    "        break\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    print(\"❌ Ollama server failed to start. Check ollama.log for errors.\")\n",
    "    !cat ollama.log\n",
    "\n",
    "# Download the model\n",
    "!ollama pull deepseek-r1:8b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Platinum Standard directory if it doesn't exist\n",
    "!mkdir -p data/training/platinum_standard\n",
    "\n",
    "# --- GENERATE GOLD STANDARD DRAFT #21 ---\n",
    "GOLD_PROMPT = \"Design a luxurious 4BHK G+1 duplex for a 40x60 feet west-facing plot in a gated community in Bangalore. The design must be Vastu-compliant and include a home office on the ground floor, a private family lounge on the first floor, and balconies for every bedroom. The client desires a contemporary architectural style with large windows for ample natural light.\"\n",
    "!python scripts/generate_draft_from_prompt.py --scenario \"{GOLD_PROMPT}\" --output-file \"data/training/gold_standard/gold_standard_21_draft.json\"\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
    "\n",
    "# --- GENERATE PLATINUM STANDARD DRAFT #01 ---\n",
    "PLATINUM_PROMPT = \"Design a one-of-a-kind, 'biophilic' 3BHK luxury retreat on a 50x80 feet plot overlooking the backwaters of Kerala. The design must seamlessly integrate indoor and outdoor spaces, featuring a central open-to-sky courtyard with a water body, extensive use of natural materials like laterite stone and teak wood, and a cantilevered infinity pool on the first floor. Prioritize sustainability with rainwater harvesting and solar panel provisions. The architectural style should be a modern interpretation of traditional Kerala design.\"\n",
    "!python scripts/generate_draft_from_prompt.py --scenario \"{PLATINUM_PROMPT}\" --output-file \"data/training/platinum_standard/platinum_standard_01_draft.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca665d",
   "metadata": {},
   "source": [
    "## Step 4: Prepare the Gold Standard Data\n",
    "\n",
    "This step runs our preparation script. It will process the 20 raw Gold Standard JSON files (plus any new ones you've generated and perfected) and create a new `gold_standard_finetune_ready` directory containing the data in the simple `{\"prompt\": \"...\", \"output\": \"...\"}` format required by the training script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/prepare_gold_standard_data.py\n",
    "!echo \"\\n✅ Data preparation complete. Verifying the new directory:\"\n",
    "!ls -l data/training/gold_standard_finetune_ready | wc -l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbc72b",
   "metadata": {},
   "source": [
    "## Step 5: Run the Fine-Tuning Script\n",
    "\n",
    "This is the core of the process. We execute the `run_finetuning.py` script, which will:\n",
    "\n",
    "1.  **Load** our 20 prepared Gold Standard examples.\n",
    "2.  **Download** the base `Llama-2-7b-chat-hf` model from Hugging Face.\n",
    "3.  **Configure** 4-bit quantization and LoRA for efficient training.\n",
    "4.  **Fine-tune** the model on our data.\n",
    "5.  **Save** the final, specialized `housebrain-llama2-7b-v0.1` model to the `models/` directory.\n",
    "\n",
    "We will use a high number of epochs (e.g., 200) because our dataset is very high-quality but small. This is necessary to ensure the model learns the schema thoroughly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ac353",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/run_finetuning.py \\\n",
    "    --dataset-path \"data/training/gold_standard_finetune_ready\" \\\n",
    "    --base-model \"meta-llama/Llama-2-7b-chat-hf\" \\\n",
    "    --output-path \"models/housebrain-llama2-7b-v0.1\" \\\n",
    "    --epochs 200 \\\n",
    "    --batch-size 2 \\\n",
    "    --learning-rate 2e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ed78e",
   "metadata": {},
   "source": [
    "## Step 6: Next Steps - Using Your Fine-Tuned Model\n",
    "\n",
    "Once training is complete, the new model is saved in the `models/housebrain-llama2-7b-v0.1` directory. \n",
    "\n",
    "You can now use this specialized model in your `generate_validated_silver_data.py` script (by changing the model ID) to generate a large, high-quality dataset of thousands of examples. This is the path to a truly production-ready system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e119bb7",
   "metadata": {},
   "source": [
    "## Step 7 (Optional): A/B Test with a State-of-the-Art Competitor (Qwen2)\n",
    "\n",
    "To ensure we are using the absolute best base model, you can run this second experiment to fine-tune the new, high-performing `Qwen2-7B-Instruct` model on our same Gold Standard dataset.\n",
    "\n",
    "Once this is trained, you will have two models: `housebrain-llama2-7b-v0.1` and `housebrain-qwen2-7b-v0.1`. You can then evaluate them head-to-head on a new set of prompts to see which one produces superior architectural designs. This data-driven approach guarantees we select the best possible foundation for our production system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e7519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python scripts/run_finetuning.py \\\n",
    "#     --dataset-path \"data/training/gold_standard_finetune_ready\" \\\n",
    "#     --base-model \"Qwen/Qwen2-7B-Instruct\" \\\n",
    "#     --output-path \"models/housebrain-qwen2-7b-v0.1\" \\\n",
    "#     --epochs 200 \\\n",
    "#     --batch-size 2 \\\n",
    "#     --learning-rate 2e-5\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
