{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. Install and Start Ollama ---\n",
        "# This is the most critical step. We must install the Ollama service in the Colab environment.\n",
        "# The '!' runs shell commands. The '&' at the end runs the server in the background.\n",
        "\n",
        "print(\"Installing Ollama...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "print(\"Starting Ollama server in the background...\")\n",
        "!ollama serve &\n",
        "\n",
        "# Give the server a moment to start up\n",
        "import time\n",
        "time.sleep(5)\n",
        "print(\"‚úÖ Ollama server should be running.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. Environment Setup ---\n",
        "# Install the essential libraries for connecting to Ollama and creating interactive widgets.\n",
        "%pip install -q ollama ipywidgets pandas tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1a. Mount Google Drive ---\n",
        "# This step is necessary to access files like our prompt list.\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Verify File Access ---\n",
        "# This path is now set based on your screenshot (My Drive > housebrain_prompts)\n",
        "PROMPT_FILE_PATH = \"/content/drive/MyDrive/housebrain_prompts/platinum_prompts.txt\" \n",
        "\n",
        "print(\"---\")\n",
        "if os.path.exists(PROMPT_FILE_PATH):\n",
        "    print(f\"‚úÖ Successfully found prompt file at: {PROMPT_FILE_PATH}\")\n",
        "else:\n",
        "    print(f\"‚ùå ERROR: Could not find prompt file at the specified path.\")\n",
        "    print(\"If your file is in a different location, please update the PROMPT_FILE_PATH variable.\")\n",
        "print(\"---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2. Configuration & Model Selection ---\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import os\n",
        "\n",
        "# --- Model Selection ---\n",
        "# Create a dropdown widget with all the models we want to test.\n",
        "model_options = [\n",
        "    \"phi4-reasoning:latest\", \"phi4-reasoning:plus\", \"phi4-reasoning:14b\",\n",
        "    \"phi4-reasoning:14b-plus-q4_K_M\", \"phi3:instruct\", \"llama3:instruct\", \"qwen2.5:3b\"\n",
        "]\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=model_options, value='phi4-reasoning:latest', description='Select Model:',\n",
        "    disabled=False, style={'description_width': 'initial'}\n",
        ")\n",
        "display(model_dropdown)\n",
        "\n",
        "# --- Tier Configuration ---\n",
        "# Define the tier for this generation run. This will create a subfolder.\n",
        "# Tiers: gold_tier, platinum_tier, diamond_tier\n",
        "DATASET_TIER = \"gold_tier\"\n",
        "\n",
        "# --- Generation Parameters ---\n",
        "# The root directory where all raw generated data will be saved.\n",
        "BASE_OUTPUT_DIR = \"raw_generated_data\"\n",
        "\n",
        "# How many architectural plans do you want to generate in this run?\n",
        "NUM_PLANS_TO_GENERATE = 15000 # Goal for the Gold Tier\n",
        "\n",
        "# --- Create Directory Structure ---\n",
        "# The final output path will be nested (e.g., raw_generated_data/gold_tier/)\n",
        "TIER_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, DATASET_TIER)\n",
        "os.makedirs(TIER_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Configuration loaded. Selected model will be: {model_dropdown.value}\")\n",
        "print(f\"Dataset Tier: {DATASET_TIER}\")\n",
        "print(f\"Number of samples to generate: {NUM_PLANS_TO_GENERATE}\")\n",
        "print(f\"Generated data will be saved under: {TIER_OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3. Setup & Verification ---\n",
        "import os\n",
        "\n",
        "# -- A. Load Prompts from File --\n",
        "def load_prompts_from_file(filepath):\n",
        "    \"\"\"Loads prompts from a text file, one prompt per line.\"\"\"\n",
        "    print(f\"Loading prompts from {filepath}...\")\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            prompts = [line.strip() for line in f if line.strip()]\n",
        "        print(f\"Successfully loaded {len(prompts)} prompts.\")\n",
        "        return prompts\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå ERROR: Prompt file not found at {filepath}. Please check the path in Cell 1a.\")\n",
        "        return []\n",
        "\n",
        "# Load the prompts using the path defined in Cell 1a\n",
        "ALL_PROMPTS = load_prompts_from_file(PROMPT_FILE_PATH)\n",
        "\n",
        "# -- B. Prepare Ollama Model --\n",
        "# Get the selected model from the dropdown widget in the previous cell.\n",
        "selected_model = model_dropdown.value\n",
        "print(f\"\\\\n--- Preparing Model: {selected_model} ---\")\n",
        "\n",
        "# This command will pull the model if it's not already present in the Colab instance.\n",
        "print(f\"\\\\nAttempting to download '{selected_model}' via Ollama... (This may take several minutes)\")\n",
        "!ollama pull {selected_model}\n",
        "\n",
        "# Verify the model was downloaded by listing all available models\n",
        "print(f\"\\\\n--- Verifying Model Installation ---\")\n",
        "!ollama list\n",
        "\n",
        "# -- C. Final Readiness Check --\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "if ALL_PROMPTS:\n",
        "    print(\"‚úÖ Prompt file loaded successfully.\")\n",
        "    print(f\"‚úÖ Model '{selected_model}' is ready.\")\n",
        "    print(\"üöÄ You are now ready to start the data generation process in the next cell.\")\n",
        "else:\n",
        "    print(\"‚ùå Generation HALTED. Please fix the prompt file path in Cell 1a.\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 4. Run Data Generation ---\n",
        "# This cell contains the core logic for generating the plans.\n",
        "# All setup (prompt loading, model download) is handled in the previous cells.\n",
        "\n",
        "# --- Core Generation Function ---\n",
        "# (This function remains unchanged)\n",
        "def generate_and_save_raw_plan(prompt, model_name, tier_dir):\n",
        "    \"\"\"\n",
        "    Generates a house plan using the specified model and saves the raw output.\n",
        "    \"\"\"\n",
        "    prompt_hash = hashlib.md5(prompt.encode()).hexdigest()[:10]\n",
        "    timestamp = int(time.time())\n",
        "    unique_id = f\"prompt_{prompt_hash}_{timestamp}\"\n",
        "    \n",
        "    run_output_dir = os.path.join(tier_dir, model_name, unique_id)\n",
        "    os.makedirs(run_output_dir, exist_ok=True)\n",
        "    \n",
        "    output_filename = os.path.join(run_output_dir, \"raw_output.json\")\n",
        "    \n",
        "    try:\n",
        "        structured_prompt = f\"\"\"\n",
        "        Please act as an expert architect specializing in Indian residential and commercial design...\n",
        "        (Full prompt text is the same as before)\n",
        "        \"\"\"\n",
        "        \n",
        "        response = ollama.chat(\n",
        "            model=model_name,\n",
        "            messages=[{'role': 'user', 'content': structured_prompt}],\n",
        "            format='json'\n",
        "        )\n",
        "        raw_output = response['message']['content']\n",
        "        with open(output_filename, 'w') as f:\n",
        "            f.write(raw_output)\n",
        "        return True, output_filename\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred: {str(e)}\"\n",
        "        with open(os.path.join(run_output_dir, \"error.log\"), 'w') as f:\n",
        "            f.write(error_message)\n",
        "        return False, None\n",
        "\n",
        "# --- Main Generation Loop ---\n",
        "# Check if prompts were loaded successfully in the PREVIOUS cell before starting\n",
        "if 'ALL_PROMPTS' in locals() and ALL_PROMPTS:\n",
        "    print(\"\\\\nStarting data generation process for the Gold Tier...\")\n",
        "\n",
        "    successful_generations = 0\n",
        "\n",
        "    for i in tqdm(range(NUM_PLANS_TO_GENERATE), desc=f\"Generating with {selected_model}\"):\n",
        "        current_prompt = random.choice(ALL_PROMPTS)\n",
        "        \n",
        "        success, filepath = generate_and_save_raw_plan(\n",
        "            prompt=current_prompt,\n",
        "            model_name=selected_model,\n",
        "            tier_dir=TIER_OUTPUT_DIR\n",
        "        )\n",
        "        \n",
        "        if success:\n",
        "            successful_generations += 1\n",
        "\n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    print(\"Gold Tier Data Generation Complete!\")\n",
        "    print(f\"Successfully generated {successful_generations} / {NUM_PLANS_TO_GENERATE} raw plan files.\")\n",
        "    print(f\"All outputs are saved in the '{TIER_OUTPUT_DIR}/{selected_model}' directory.\")\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(\"\\\\nüõë Generation HALTED. Please run the 'Setup & Verification' cell (Cell 3) successfully first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 4. Package and Download Results ---\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import time\n",
        "\n",
        "# This cell packages the output from the last generation run into a single zip file\n",
        "# for easy download.\n",
        "\n",
        "# Identify the output directory from the last run based on the tier\n",
        "# Note: This relies on the 'selected_model' and 'DATASET_TIER' variables from Cell 2.\n",
        "output_directory_path = os.path.join(BASE_OUTPUT_DIR, DATASET_TIER, selected_model)\n",
        "zip_filename = f\"{DATASET_TIER}_{selected_model}_raw_data_{int(time.time())}\"\n",
        "zip_filepath = f\"/content/{zip_filename}\"\n",
        "\n",
        "print(f\"Locating generated data in: {output_directory_path}...\")\n",
        "\n",
        "if os.path.isdir(output_directory_path):\n",
        "    print(f\"Found data. Compressing into '{zip_filename}.zip'...\")\n",
        "    \n",
        "    # Create the zip archive\n",
        "    shutil.make_archive(zip_filepath, 'zip', output_directory_path)\n",
        "    \n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ Success! Your data has been compressed.\")\n",
        "    print(f\"Your zip file is ready: {zip_filename}.zip\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Optional: Trigger automatic download in Google Colab\n",
        "    try:\n",
        "        print(\"\\\\nTriggering file download... (This may take a while for large datasets)\")\n",
        "        files.download(f\"{zip_filepath}.zip\")\n",
        "    except NameError:\n",
        "        print(\"\\\\nNOTE: Automatic download is only available in Google Colab.\")\n",
        "        print(f\"You can find your archive at: {zip_filepath}.zip\")\n",
        "\n",
        "else:\n",
        "    print(f\"‚ùå Error: Could not find the output directory '{output_directory_path}'.\")\n",
        "    print(\"Please make sure you have run the generation cell (Cell 3) first.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
