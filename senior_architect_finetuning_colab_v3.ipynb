{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain Architect - Fine-Tuning on Colab\n",
        "\n",
        "This notebook fine-tunes the `Qwen/Qwen2.5-3B-Instruct` model on our curated house plan dataset.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Set up the GPU:** Go to `Runtime` > `Change runtime type` and select `T4 GPU` or another available GPU.\n",
        "2.  **Run each cell in order:** Execute the cells one by one from top to bottom.\n",
        "3.  **Hugging Face Token (Optional but Recommended):** The second code cell will ask for a Hugging Face token. While not always required for public models, it's good practice. You can create a token in your Hugging Face account settings and add it as a \"Secret\" in Colab (click the ðŸ”‘ icon on the left). Name the secret `HF_TOKEN`.\n",
        "4.  **Download the result:** After the final cell runs, a file named `housebrain_v1_adapters.zip` will appear in the file browser on the left. Right-click it and select `Download` to save your trained model adapters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. Clone the Repository ---\n",
        "# This will download your project files, including the necessary datasets.\n",
        "\n",
        "!git clone https://github.com/Vinay-O/HouseBrainLLM.git\n",
        "%cd HouseBrainLLM\n",
        "\n",
        "print(\"âœ… Repository cloned and directory changed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2. Install Dependencies ---\n",
        "# This multi-step process ensures that all libraries, especially torch, bitsandbytes,\n",
        "# and triton, are installed and compiled correctly for the Colab A100 GPU environment.\n",
        "\n",
        "# Step A: Install the main application libraries\n",
        "!pip install -U \"transformers==4.41.2\" \"peft==0.10.0\" \"accelerate==0.30.1\" \"datasets==2.19.1\"\n",
        "\n",
        "# Step B: Force a re-install of torch and a compatible triton from the PyTorch CUDA 12.1 index\n",
        "# This is the critical step for A100 GPUs.\n",
        "!pip install -U --force-reinstall \"torch==2.3.1\" \"triton==2.3.1\" --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Step C: Install the correct version of bitsandbytes\n",
        "!pip install -U \"bitsandbytes==0.43.1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3. Imports & Login ---\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig\n",
        "import os\n",
        "\n",
        "# Optional: Login to Hugging Face\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "  hf_token = userdata.get('HF_TOKEN')\n",
        "  login(token=hf_token)\n",
        "  print(\"âœ… Successfully logged into Hugging Face.\")\n",
        "except Exception as e:\n",
        "  print(\"Proceeding without Hugging Face login. Add a secret named HF_TOKEN if you encounter download issues.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 4. Load and Prepare Dataset ---\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "# The tokenizer is loaded first to be used in the data preparation.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define the dataset file we want to use\n",
        "dataset_file = \"finetune_dataset_senior_architect_v3.jsonl\"\n",
        "\n",
        "# Load the dataset from the cloned repository\n",
        "# For the Senior Architect, we'll train on the entire dataset and not use a separate validation set for now.\n",
        "full_dataset = load_dataset(\"json\", data_files=dataset_file, split=\"train\")\n",
        "\n",
        "\n",
        "# This function formats the prompts and tokenizes them for the Senior Architect\n",
        "def formatting_and_tokenizing_func(examples):\n",
        "    texts = []\n",
        "    # The dataset is already in the OpenAI \"messages\" format.\n",
        "    # We just need to apply the chat template.\n",
        "    for conversation in examples[\"messages\"]:\n",
        "        # CRITICAL: Update the system prompt to be extremely direct for the correction task\n",
        "        system_prompt_updated = {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an expert AI architect. Your only task is to review a flawed house plan and output the corrected version as a single, raw JSON object. Do not add any conversational text or markdown.\"\n",
        "        }\n",
        "        \n",
        "        # Find the user and assistant messages\n",
        "        user_msg = next((msg for msg in conversation if msg['role'] == 'user'), None)\n",
        "        assistant_msg = next((msg for msg in conversation if msg['role'] == 'assistant'), None)\n",
        "\n",
        "        if user_msg and assistant_msg:\n",
        "            # Reconstruct the conversation with the updated, stricter system prompt\n",
        "            templated_chat = tokenizer.apply_chat_template(\n",
        "                [system_prompt_updated, user_msg, assistant_msg],\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=False\n",
        "            )\n",
        "            texts.append(templated_chat)\n",
        "\n",
        "    # Tokenize the formatted text\n",
        "    return tokenizer(texts, truncation=True, max_length=2048, padding=False) # Increased max_length for complex plans\n",
        "\n",
        "\n",
        "# Apply the function to the dataset\n",
        "processed_dataset = full_dataset.map(formatting_and_tokenizing_func, batched=True, remove_columns=full_dataset.column_names)\n",
        "\n",
        "print(\"âœ… Senior Architect dataset loaded and prepared.\")\n",
        "print(f\"Total training samples: {len(processed_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 5. Configure and Load Model ---\n",
        "\n",
        "# Configure 4-bit quantization to save memory\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the base model with our quantization config\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Configure LoRA for efficient fine-tuning\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"âœ… Model configured with 4-bit quantization and LoRA.\")\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 6. Start Fine-Tuning ---\n",
        "\n",
        "output_dir = \"housebrain_senior_architect_v3_adapters\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=25, # Log every 25 steps\n",
        "    num_train_epochs=2, # Two epochs should be sufficient with the larger dataset\n",
        "    save_strategy=\"epoch\", # Save at the end of each epoch\n",
        "    fp16=True, # Use fp16 for mixed-precision training\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_dataset, # Use our fully processed dataset\n",
        "    # eval_dataset is removed as we are not evaluating during this training run\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "print(\"ðŸš€ Starting Senior Architect (v3) training...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"âœ… Fine-tuning complete.\")\n",
        "trainer.save_model(output_dir)\n",
        "print(f\"ðŸ’¾ Final model adapters saved to '{output_dir}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 7. Package and Download ---\n",
        "import shutil\n",
        "\n",
        "# Zip the adapters folder\n",
        "archive_path = shutil.make_archive(\"housebrain_senior_architect_v3_adapters\", 'zip', \"housebrain_senior_architect_v3_adapters\")\n",
        "\n",
        "print(f\"âœ… Model adapters saved and zipped at: {archive_path}\")\n",
        "print(\"You can now download 'housebrain_senior_architect_v3_adapters.zip' from the Files panel on the left.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
