{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain: Senior Architect (v4 - Llama-3) Fine-tuning\n",
        "\n",
        "This notebook is dedicated to fine-tuning the `meta-llama/Meta-Llama-3-8B-Instruct` model to act as our **Senior Architect v4**. \n",
        "\n",
        "**Objective:** To create a model that can identify and correct architectural, geometric, and functional flaws in a given JSON house plan. This is part of a two-track experiment to compare against the Qwen model.\n",
        "\n",
        "**Dataset:** This training run uses `finetune_dataset_senior_architect_v4.jsonl`, which contains over 6,000 examples of `(flawed_plan -> corrected_plan)` pairs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Authentication\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install -q \"transformers[torch]\" peft bitsandbytes datasets accelerate\n",
        "\n",
        "# Login to Hugging Face Hub. You will need a token with write access.\n",
        "# Also, you MUST request access to the Llama-3 model on Hugging Face.\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Clone Repository & Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Clone the repository to access the dataset\n",
        "repo_path = '/content/HouseBrainLLM'\n",
        "if not os.path.exists(repo_path):\n",
        "    !git clone https://github.com/Vinay-O/HouseBrainLLM.git\n",
        "else:\n",
        "    print('Repository already cloned.')\n",
        "\n",
        "os.chdir(repo_path)\n",
        "# Make sure we are on the main branch and have the latest files\n",
        "!git checkout main\n",
        "!git pull\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load our v4 dataset\n",
        "dataset_path = '/content/HouseBrainLLM/finetune_dataset_senior_architect_v4.jsonl'\n",
        "full_dataset = load_dataset('json', data_files=dataset_path, split='train')\n",
        "\n",
        "print(\"\\nâœ… Dataset loaded successfully!\")\n",
        "print(f\"   Number of samples: {len(full_dataset)}\")\n",
        "print(\"\\nSample entry:\")\n",
        "print(full_dataset[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Preparation and Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "base_model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# Configure quantization to load the model in 4-bit, which saves memory\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "# Llama-3 does not have a padding token, so we set it to the end-of-sequence token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the base model with our quantization config\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# This function formats and tokenizes our dataset entries\n",
        "def formatting_and_tokenizing_func(example):\n",
        "    # This strict system prompt is crucial for ensuring the model ONLY outputs JSON\n",
        "    system_prompt = {\n",
        "        \"role\": \"system\", \n",
        "        \"content\": \"You are an expert AI architect. Your only task is to review a flawed house plan and output the corrected version as a single, raw JSON object. Do not add any conversational text or markdown.\"\n",
        "    }\n",
        "    \n",
        "    # The user provides the flawed plan\n",
        "    user_prompt = {\"role\": \"user\", \"content\": example['messages'][1]['content']}\n",
        "    \n",
        "    # The assistant provides the corrected plan\n",
        "    assistant_response = {\"role\": \"assistant\", \"content\": example['messages'][2]['content']}\n",
        "\n",
        "    # We create a single list of dictionaries for the conversation\n",
        "    conversation = [system_prompt, user_prompt, assistant_response]\n",
        "    \n",
        "    # The tokenizer's `apply_chat_template` method formats this conversation into the model's expected input format\n",
        "    tokenized_outputs = tokenizer.apply_chat_template(conversation, tokenize=True, add_generation_prompt=False, return_tensors=\"pt\")\n",
        "    \n",
        "    return {'input_ids': tokenized_outputs.squeeze(0)}\n",
        "\n",
        "# Apply the function to our entire dataset\n",
        "tokenized_dataset = full_dataset.map(formatting_and_tokenizing_func, remove_columns=list(full_dataset.features))\n",
        "\n",
        "print(\"\\nâœ… Dataset formatted and tokenized.\")\n",
        "print(\"\\nSample tokenized entry:\")\n",
        "print(tokenized_dataset[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. PEFT/LoRA Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "\n",
        "# Prepare the model for 4-bit training\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Configure LoRA for Llama-3. Note the different `target_modules`.\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply the LoRA config to our model\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"\\nâœ… PEFT model created successfully.\")\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Define the arguments for the training process\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"housebrain_senior_architect_v4_llama3_adapters\",\n",
        "    per_device_train_batch_size=4, # Adjust based on GPU memory (A100 can handle 4 or 8)\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=25, # Log metrics every 25 steps\n",
        "    num_train_epochs=1, # A single epoch is often sufficient for a large dataset\n",
        "    save_strategy=\"epoch\",\n",
        "    bf16=True, # Use bfloat16 for faster training on compatible GPUs (like A100)\n",
        "    push_to_hub=True, # Push the final adapters to Hugging Face Hub\n",
        "    report_to=\"wandb\" # Optional: for logging to Weights & Biases\n",
        ")\n",
        "\n",
        "# Create the Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"ðŸš€ Starting Senior Architect (v4 - Llama-3) training...\")\n",
        "# Start the training process\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. (Optional) Save and Archive Adapters\n",
        "\n",
        "If you want to save the adapters locally in Colab for later download.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# The directory where the adapters are saved\n",
        "adapter_dir = \"housebrain_senior_architect_v4_llama3_adapters\"\n",
        "\n",
        "# Create a zip archive of the adapters\n",
        "shutil.make_archive(adapter_dir, 'zip', adapter_dir)\n",
        "\n",
        "print(f\"\\nâœ… Adapters archived to {adapter_dir}.zip\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
