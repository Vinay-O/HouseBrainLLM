{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 1. Setup Environment (Fixed Version)\n",
        "# @markdown Mount Google Drive and clone the repository using a secure token.\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import getpass\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted.\")\n",
        "\n",
        "# --- GitHub Setup ---\n",
        "print(\"Setting up GitHub repository...\")\n",
        "try:\n",
        "    GITHUB_TOKEN = getpass.getpass('Enter your GitHub PAT: ')\n",
        "    REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/Vinay-O/HouseBrainLLM.git\"\n",
        "    REPO_DIR = \"/content/HouseBrainLLM\"\n",
        "\n",
        "    # Clone or update repository\n",
        "    if os.path.exists(REPO_DIR):\n",
        "        print(\"Repository already exists. Pulling latest changes...\")\n",
        "        os.chdir(REPO_DIR)\n",
        "        result = subprocess.run([\"git\", \"pull\"], capture_output=True, text=True)\n",
        "        if result.returncode != 0:\n",
        "            print(f\"Git pull failed: {result.stderr}\")\n",
        "        else:\n",
        "            print(\"‚úÖ Repository updated successfully.\")\n",
        "    else:\n",
        "        print(\"Cloning repository...\")\n",
        "        result = subprocess.run([\"git\", \"clone\", REPO_URL, REPO_DIR], capture_output=True, text=True)\n",
        "        if result.returncode != 0:\n",
        "            print(f\"Git clone failed: {result.stderr}\")\n",
        "            raise Exception(\"Failed to clone repository\")\n",
        "        else:\n",
        "            print(\"‚úÖ Repository cloned successfully.\")\n",
        "\n",
        "    # Add repository to Python path\n",
        "    sys.path.append(REPO_DIR)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error setting up repository: {e}\")\n",
        "    # Continue without repository for basic functionality\n",
        "    REPO_DIR = \"/content\"\n",
        "\n",
        "# --- Install Dependencies ---\n",
        "print(\"Installing dependencies...\")\n",
        "try:\n",
        "    # Install basic dependencies first\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"pydantic\", \"requests\"], check=True)\n",
        "    \n",
        "    # Try to install from requirements if available\n",
        "    requirements_path = os.path.join(REPO_DIR, \"requirements.txt\")\n",
        "    if os.path.exists(requirements_path):\n",
        "        print(\"Installing from requirements.txt...\")\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", requirements_path], check=True)\n",
        "        print(\"‚úÖ Requirements installed.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è requirements.txt not found. Using basic dependencies.\")\n",
        "        \n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"‚ö†Ô∏è Some dependencies failed to install: {e}\")\n",
        "    print(\"Continuing with basic setup...\")\n",
        "\n",
        "print(\"‚úÖ Environment setup complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 2. Configure and Start Ollama Server (Fixed Version)\n",
        "# @markdown This cell will download and start the Ollama server, then pull the specified model.\n",
        "\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "import sys\n",
        "import requests\n",
        "\n",
        "# Use a more compatible model - deepseek-r1:32b has known issues\n",
        "MODEL_NAME = \"llama3.1:8b\"  # More stable alternative\n",
        "# MODEL_NAME = \"qwen2.5:7b\"  # Another good option\n",
        "# MODEL_NAME = \"deepseek-r1:7b\"  # Smaller deepseek variant if you want to try\n",
        "\n",
        "print(\"Installing Ollama...\")\n",
        "try:\n",
        "    # Install Ollama\n",
        "    result = subprocess.run(\n",
        "        [\"curl\", \"-fsSL\", \"https://ollama.com/install.sh\"], \n",
        "        capture_output=True, text=True, check=True\n",
        "    )\n",
        "    \n",
        "    # Execute the install script\n",
        "    process = subprocess.run(\"bash\", input=result.stdout, text=True, capture_output=True)\n",
        "    if process.returncode == 0:\n",
        "        print(\"‚úÖ Ollama installed successfully.\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Ollama installation warning: {process.stderr}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Ollama installation failed: {e}\")\n",
        "\n",
        "# Install Ollama Python package\n",
        "print(\"Installing Ollama Python package...\")\n",
        "try:\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"ollama\"], check=True)\n",
        "    print(\"‚úÖ Ollama Python package installed.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to install Ollama package: {e}\")\n",
        "\n",
        "# Function to run Ollama server\n",
        "def run_ollama_serve():\n",
        "    \"\"\"Run Ollama server in background\"\"\"\n",
        "    try:\n",
        "        subprocess.run([\"ollama\", \"serve\"], check=False, capture_output=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Ollama server error: {e}\")\n",
        "\n",
        "# Start Ollama server in background\n",
        "print(\"üöÄ Starting Ollama server...\")\n",
        "ollama_thread = threading.Thread(target=run_ollama_serve, daemon=True)\n",
        "ollama_thread.start()\n",
        "\n",
        "# Wait for server to start\n",
        "print(\"‚è≥ Waiting for Ollama server to initialize...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# Test if server is running\n",
        "def test_ollama_server():\n",
        "    \"\"\"Test if Ollama server is responding\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True, timeout=10)\n",
        "        return result.returncode == 0\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "if test_ollama_server():\n",
        "    print(\"‚úÖ Ollama server is running.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Ollama server may not be fully ready yet.\")\n",
        "\n",
        "# Pull the model\n",
        "print(f\"üì¶ Pulling model: {MODEL_NAME}...\")\n",
        "try:\n",
        "    # Use a more robust model pulling approach\n",
        "    pull_process = subprocess.run(\n",
        "        [\"ollama\", \"pull\", MODEL_NAME],\n",
        "        capture_output=True, text=True, timeout=600  # 10 minute timeout\n",
        "    )\n",
        "    \n",
        "    if pull_process.returncode == 0:\n",
        "        print(f\"‚úÖ Model {MODEL_NAME} is ready.\")\n",
        "    else:\n",
        "        print(f\"‚ùå Failed to pull model: {pull_process.stderr}\")\n",
        "        # Try alternative model\n",
        "        alternative_model = \"llama3.1:3b\"\n",
        "        print(f\"Trying alternative model: {alternative_model}\")\n",
        "        alt_process = subprocess.run(\n",
        "            [\"ollama\", \"pull\", alternative_model],\n",
        "            capture_output=True, text=True, timeout=600\n",
        "        )\n",
        "        if alt_process.returncode == 0:\n",
        "            MODEL_NAME = alternative_model\n",
        "            print(f\"‚úÖ Alternative model {MODEL_NAME} is ready.\")\n",
        "        else:\n",
        "            raise Exception(\"Failed to pull any model\")\n",
        "            \n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"‚ö†Ô∏è Model pull timed out. The model might be very large.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error pulling model: {e}\")\n",
        "\n",
        "# Final verification\n",
        "print(\"Final verification...\")\n",
        "subprocess.run([\"ollama\", \"list\"], check=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 3. Run the Data Factory (Live LLM Version)\n",
        "\n",
        "import json\n",
        "import uuid\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import os\n",
        "from pydantic import BaseModel, ValidationError\n",
        "from typing import List, Optional, Dict, Any\n",
        "from enum import Enum\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "DATASET_PATH = \"/content/drive/MyDrive/housebrain_platinum_dataset\"\n",
        "MASTER_PROMPT_LIST_PATH = \"/content/drive/MyDrive/housebrain_prompts/platinum_prompts.txt\"\n",
        "NUM_PROMPTS_TO_GENERATE = 5  # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "\n",
        "# --- Schema Handling ---\n",
        "SCHEMA_LOADED = False\n",
        "try:\n",
        "    # Attempt to import the official schema from the repository\n",
        "    # FIX: Removed 'BasicDetails' as it does not exist in the current schema.py\n",
        "    from src.housebrain.schema import HouseOutput, HouseInput, RoomType\n",
        "    SCHEMA_LOADED = True\n",
        "    print(\"‚úÖ Successfully imported HouseBrain schema from repository.\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è Could not import official HouseBrain schema: {e}\")\n",
        "    print(\"   Creating a minimal fallback schema for operation.\")\n",
        "    \n",
        "    # Define a minimal, compatible schema if the import fails\n",
        "    class RoomType(str, Enum):\n",
        "        living_room = \"living_room\"; dining_room = \"dining_room\"; kitchen = \"kitchen\"\n",
        "        bedroom = \"bedroom\"; bathroom = \"bathroom\"; balcony = \"balcony\"; garage = \"garage\"\n",
        "        storage = \"storage\"; study = \"study\"; utility = \"utility\"; entrance = \"entrance\"\n",
        "        hallway = \"hallway\"; patio = \"patio\"; laundry = \"laundry\"\n",
        "    \n",
        "    class BasicDetails(BaseModel): prompt: str\n",
        "    class HouseInput(BaseModel): basicDetails: BasicDetails\n",
        "    class Rectangle(BaseModel): x: float; y: float; width: float; height: float\n",
        "    class Door(BaseModel): id: str; type: str; bounds: Rectangle; room1: str; room2: str\n",
        "    class Window(BaseModel): id: str; type: str; bounds: Rectangle\n",
        "    class Furniture(BaseModel): id: str; type: str; bounds: Rectangle\n",
        "    class Room(BaseModel): id: str; room_type: RoomType; bounds: Rectangle; doors: List[Door] = []; windows: List[Window] = []; furniture: List[Furniture] = []\n",
        "    class Level(BaseModel): id: str; level_number: int; rooms: List[Room]\n",
        "    class HouseOutput(BaseModel): id: str; input: HouseInput; total_area: float = 0.0; levels: List[Level]\n",
        "    \n",
        "    SCHEMA_LOADED = True\n",
        "    print(\"‚úÖ Minimal fallback schema created.\")\n",
        "\n",
        "# --- Ollama Integration ---\n",
        "try:\n",
        "    import ollama\n",
        "    print(\"‚úÖ Ollama package imported successfully.\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå Ollama package not found. Please run Cell 2 to install.\")\n",
        "    ollama = None # Ensure ollama is defined to prevent NameError\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def call_ollama_safe(model, prompt, retries=3, delay=5):\n",
        "    \"\"\"Safely call Ollama with retry logic and error handling.\"\"\"\n",
        "    if not ollama: return None\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=model, \n",
        "                messages=[{'role': 'user', 'content': prompt}],\n",
        "                options={'temperature': 0.7, 'top_p': 0.9}\n",
        "            )\n",
        "            return response['message']['content']\n",
        "        except Exception as e:\n",
        "            print(f\"    ‚ùå Attempt {attempt + 1}/{retries} failed: {e}\")\n",
        "            if attempt < retries - 1: time.sleep(delay)\n",
        "    print(\"    ‚ùå All Ollama attempts failed.\")\n",
        "    return None\n",
        "\n",
        "def extract_json_from_text(text):\n",
        "    \"\"\"Robustly extract a JSON object from a string, ignoring surrounding text.\"\"\"\n",
        "    if not text: return None\n",
        "    match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
        "    if not match: return None\n",
        "    try:\n",
        "        return json.loads(match.group(0))\n",
        "    except json.JSONDecodeError:\n",
        "        return None\n",
        "\n",
        "def heal_plan_data(raw_data: dict, prompt_text: str):\n",
        "    \"\"\"Applies deterministic fixes to the raw LLM output to conform to schema.\"\"\"\n",
        "    # Add the original prompt to the input data\n",
        "    if 'input' not in raw_data or not isinstance(raw_data.get('input'), dict):\n",
        "        raw_data['input'] = {}\n",
        "    if 'basicDetails' not in raw_data['input'] or not isinstance(raw_data['input'].get('basicDetails'), dict):\n",
        "        raw_data['input']['basicDetails'] = {}\n",
        "        \n",
        "    raw_data['input']['basicDetails']['prompt'] = prompt_text\n",
        "\n",
        "    # Ensure other required fields exist for validation, even if they are dummy values\n",
        "    # The LLM is expected to provide these, but this prevents crashes if it doesn't.\n",
        "    details = raw_data['input']['basicDetails']\n",
        "    details.setdefault('totalArea', raw_data.get('total_area', 1000))\n",
        "    details.setdefault('unit', 'sqft')\n",
        "    details.setdefault('floors', len(raw_data.get('levels', [1])))\n",
        "    details.setdefault('bedrooms', 3)\n",
        "    details.setdefault('bathrooms', 2)\n",
        "    details.setdefault('style', 'Modern')\n",
        "    details.setdefault('budget', 500000)\n",
        "\n",
        "    if 'plot' not in raw_data['input'] or not isinstance(raw_data['input'].get('plot'), dict):\n",
        "        raw_data['input']['plot'] = {'shape': 'rectangular', 'length': 80, 'width': 50}\n",
        "\n",
        "    if 'roomBreakdown' not in raw_data['input'] or not isinstance(raw_data['input'].get('roomBreakdown'), list):\n",
        "        raw_data['input']['roomBreakdown'] = []\n",
        "\n",
        "\n",
        "    # Calculate total area if missing\n",
        "    if 'total_area' not in raw_data or not isinstance(raw_data.get('total_area'), (int, float)):\n",
        "        total_area = 0\n",
        "        if 'levels' in raw_data and isinstance(raw_data['levels'], list):\n",
        "            for level in raw_data['levels']:\n",
        "                if 'rooms' in level and isinstance(level['rooms'], list):\n",
        "                    for room in level['rooms']:\n",
        "                        if 'bounds' in room and all(k in room['bounds'] for k in ['width', 'height']):\n",
        "                             total_area += room['bounds']['width'] * room['bounds']['height']\n",
        "        raw_data['total_area'] = total_area\n",
        "\n",
        "    # Add dummy values for other required top-level fields if they're missing\n",
        "    raw_data.setdefault('construction_cost', raw_data['total_area'] * 200) # Estimate\n",
        "    raw_data.setdefault('materials', {})\n",
        "    raw_data.setdefault('render_paths', {})\n",
        "    \n",
        "    return raw_data\n",
        "\n",
        "# --- Master Prompt ---\n",
        "ROOM_TYPES_STR = \", \".join([f'\"{rt.value}\"' for rt in RoomType])\n",
        "MASTER_PROMPT_TEMPLATE = f\"\"\"\n",
        "You are an expert architectural AI. Generate a house plan in JSON format based on the user request.\n",
        "Adhere strictly to the schema provided in the documentation. Output only the raw JSON. The `room_type` must be one of {ROOM_TYPES_STR}.\n",
        "The output must include 'input', 'levels', 'total_area', and 'construction_cost' fields.\n",
        "\n",
        "User Request: {{user_prompt}}\n",
        "\n",
        "Your JSON Output:\n",
        "\"\"\"\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if SCHEMA_LOADED and ollama:\n",
        "    print(f\"\\n--- Starting Data Factory (Live LLM Mode) ---\")\n",
        "    \n",
        "    try:\n",
        "        with open(MASTER_PROMPT_LIST_PATH, 'r') as f:\n",
        "            all_prompts = [line.strip() for line in f.readlines() if line.strip()]\n",
        "        print(f\"‚úÖ Loaded {len(all_prompts)} prompts.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Prompt file not found. Run Cell 4 to generate it.\")\n",
        "        all_prompts = []\n",
        "\n",
        "    if all_prompts:\n",
        "        prompts_to_process = random.sample(all_prompts, min(NUM_PROMPTS_TO_GENERATE, len(all_prompts)))\n",
        "        print(f\"‚úÖ Processing a random batch of {len(prompts_to_process)} prompts...\")\n",
        "        \n",
        "        successful_generations = 0\n",
        "        for i, prompt_text in enumerate(prompts_to_process):\n",
        "            print(f\"\\n================== PROMPT {i+1}/{len(prompts_to_process)} ==================\")\n",
        "            print(f\"'{prompt_text[:100]}...'\")\n",
        "            \n",
        "            try:\n",
        "                # 1. Generate with LLM\n",
        "                final_prompt = MASTER_PROMPT_TEMPLATE.format(user_prompt=prompt_text)\n",
        "                llm_output = call_ollama_safe(MODEL_NAME, final_prompt)\n",
        "                if not llm_output: continue\n",
        "\n",
        "                # 2. Extract JSON\n",
        "                raw_plan = extract_json_from_text(llm_output)\n",
        "                if not raw_plan: \n",
        "                    print(\"    ‚ùå Failed to extract valid JSON from LLM output.\")\n",
        "                    continue\n",
        "                \n",
        "                # 3. Heal and Add Metadata\n",
        "                healed_plan = heal_plan_data(raw_plan, prompt_text)\n",
        "                \n",
        "                # 4. Validate\n",
        "                validated_plan = HouseOutput.model_validate(healed_plan)\n",
        "                \n",
        "                # 5. Save to File\n",
        "                plan_id = str(uuid.uuid4()).replace('-', '_')\n",
        "                file_path = os.path.join(DATASET_PATH, f\"plan_{plan_id}.json\")\n",
        "                with open(file_path, 'w') as f:\n",
        "                    f.write(validated_plan.model_dump_json(indent=2))\n",
        "                \n",
        "                print(f\"    ‚úÖ SUCCESS! Saved validated plan to {os.path.basename(file_path)}\")\n",
        "                successful_generations += 1\n",
        "                \n",
        "            except ValidationError as e:\n",
        "                print(f\"    ‚ùå Pydantic Validation Failed:\\n{e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"    ‚ùå An unexpected error occurred: {e}\")\n",
        "        \n",
        "        print(f\"\\nüéâ Data Factory completed!\")\n",
        "        print(f\"Successfully generated {successful_generations}/{len(prompts_to_process)} plans.\")\n",
        "else:\n",
        "    print(\"\\nHALTING: Prerequisites not met. Please ensure Cells 1 & 2 ran successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 4. Generate Master Prompt File (Fixed)\n",
        "# @markdown Generate a comprehensive list of prompts for house design\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "# Configuration\n",
        "DRIVE_PROMPT_FILE = \"/content/drive/MyDrive/housebrain_prompts/platinum_prompts.txt\"\n",
        "NUM_PROMPTS_TO_GENERATE = 100  # Reduced for testing\n",
        "\n",
        "# Create directory\n",
        "Path(DRIVE_PROMPT_FILE).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Generate varied prompts programmatically\n",
        "def generate_house_prompts(num_prompts=100):\n",
        "    \"\"\"Generate diverse house design prompts\"\"\"\n",
        "    \n",
        "    # Base components\n",
        "    sizes = [\"small\", \"medium\", \"large\", \"compact\", \"spacious\", \"cozy\"]\n",
        "    styles = [\"modern\", \"traditional\", \"contemporary\", \"minimalist\", \"rustic\", \"colonial\"]\n",
        "    bedroom_counts = [\"1-bedroom\", \"2-bedroom\", \"3-bedroom\", \"4-bedroom\", \"studio\"]\n",
        "    special_features = [\n",
        "        \"with garage\", \"with balcony\", \"with study room\", \"with utility room\",\n",
        "        \"with dining area\", \"with open kitchen\", \"with master suite\", \"with guest room\",\n",
        "        \"with patio\", \"with storage room\", \"with laundry room\", \"with home office\"\n",
        "    ]\n",
        "    themes = [\n",
        "        \"family home\", \"bachelor pad\", \"retirement home\", \"starter home\",\n",
        "        \"vacation house\", \"urban apartment\", \"suburban house\", \"countryside home\"\n",
        "    ]\n",
        "    \n",
        "    prompts = []\n",
        "    \n",
        "    for i in range(num_prompts):\n",
        "        # Random combinations\n",
        "        size = random.choice(sizes)\n",
        "        style = random.choice(styles)\n",
        "        bedrooms = random.choice(bedroom_counts)\n",
        "        feature = random.choice(special_features)\n",
        "        theme = random.choice(themes)\n",
        "        \n",
        "        # Generate different prompt structures\n",
        "        templates = [\n",
        "            f\"Design a {size} {style} {bedrooms} {theme} {feature}\",\n",
        "            f\"Create a {style} {bedrooms} house with {feature.replace('with ', '')}\",\n",
        "            f\"Build a {size} {theme} featuring {bedrooms} and {feature.replace('with ', '')}\",\n",
        "            f\"Design a {style} home with {bedrooms} {feature}\",\n",
        "            f\"Create a {size} {bedrooms} {style} house for a modern family\"\n",
        "        ]\n",
        "        \n",
        "        prompt = random.choice(templates)\n",
        "        prompts.append(prompt)\n",
        "    \n",
        "    # Add some specific prompts\n",
        "    specific_prompts = [\n",
        "        \"Design a house with an open floor plan and large windows\",\n",
        "        \"Create a two-story house with bedrooms upstairs\",\n",
        "        \"Build a single-story house suitable for elderly residents\", \n",
        "        \"Design a house with a central courtyard\",\n",
        "        \"Create a house with separate living and dining areas\",\n",
        "        \"Build a house with an attached garage and workshop\",\n",
        "        \"Design a house with a large kitchen island\",\n",
        "        \"Create a house with multiple bathrooms\",\n",
        "        \"Build a house with a master bedroom suite\",\n",
        "        \"Design a house with energy-efficient features\"\n",
        "    ]\n",
        "    \n",
        "    prompts.extend(specific_prompts)\n",
        "    \n",
        "    # Remove duplicates and return\n",
        "    return list(set(prompts))[:num_prompts]\n",
        "\n",
        "print(\"Generating house design prompts...\")\n",
        "generated_prompts = generate_house_prompts(NUM_PROMPTS_TO_GENERATE)\n",
        "\n",
        "# Write to file\n",
        "with open(DRIVE_PROMPT_FILE, 'w') as f:\n",
        "    for prompt in generated_prompts:\n",
        "        f.write(prompt + '\\n')\n",
        "\n",
        "print(f\"‚úÖ Generated {len(generated_prompts)} prompts\")\n",
        "print(f\"Saved to: {DRIVE_PROMPT_FILE}\")\n",
        "\n",
        "# Show first few prompts\n",
        "print(\"\\nFirst 5 prompts:\")\n",
        "for i, prompt in enumerate(generated_prompts[:5]):\n",
        "    print(f\"{i+1}. {prompt}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 5. Download Generated Dataset (Fixed)\n",
        "# @markdown Zip the entire generated dataset directory and download it to your local machine.\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "\n",
        "# Configuration\n",
        "source_dir = \"/content/drive/MyDrive/housebrain_platinum_dataset\"\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "zip_filename = f\"housebrain_dataset_{timestamp}.zip\"\n",
        "zip_filepath = f\"/content/{zip_filename}\"\n",
        "\n",
        "if os.path.exists(source_dir) and os.listdir(source_dir):\n",
        "    print(f\"Found {len(os.listdir(source_dir))} files in dataset directory\")\n",
        "    \n",
        "    # Create zip file manually for better control\n",
        "    print(f\"Creating zip file: {zip_filename}\")\n",
        "    with zipfile.ZipFile(zip_filepath, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files_in_dir in os.walk(source_dir):\n",
        "            for file in files_in_dir:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arc_name = os.path.relpath(file_path, source_dir)\n",
        "                zipf.write(file_path, arc_name)\n",
        "                print(f\"Added: {arc_name}\")\n",
        "    \n",
        "    print(f\"‚úÖ Zip file created: {zip_filepath}\")\n",
        "    \n",
        "    # Download the file\n",
        "    print(\"Downloading dataset...\")\n",
        "    try:\n",
        "        files.download(zip_filepath)\n",
        "        print(\"‚úÖ Download started successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Download failed: {e}\")\n",
        "        print(f\"You can manually download from: {zip_filepath}\")\n",
        "        \n",
        "else:\n",
        "    print(f\"‚ùå Dataset directory not found or empty: {source_dir}\")\n",
        "    print(\"Please run the data generation cell first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain Data Factory 3.0: The Diamond Series\n",
        "\n",
        "This notebook is for generating the **Diamond Dataset**. The goal of this dataset is to teach our fine-tuned model how to handle **complex, conflicting, and unconventional** architectural challenges.\n",
        "\n",
        "**Our Strategy:**\n",
        "1.  **Use a specialized script** to generate a smaller, more focused list of 2,500 \"Diamond-tier\" prompts.\n",
        "2.  **Use our best \"Journeyman\" model** (fine-tuned on the Platinum dataset) as the generator to create draft plans.\n",
        "3.  **Save the validated outputs** to a new `housebrain_diamond_dataset` folder in your Google Drive.\n",
        "4.  Use this dataset for a second round of fine-tuning to elevate our model from a \"Journeyman\" to a \"Master Architect.\"\n",
        "\n",
        "## Instructions\n",
        "1.  **Set Your GitHub PAT**: Ensure your GitHub token is ready.\n",
        "2.  **Run All Cells**: The notebook will set up the environment, generate the complex prompts, and begin the data generation process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 1. Setup Environment\n",
        "# @markdown Mount Google Drive and clone the repository using a secure token.\n",
        "from google.colab import drive\n",
        "import os\n",
        "import getpass\n",
        "import subprocess\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted.\")\n",
        "\n",
        "# --- GitHub Setup ---\n",
        "#@markdown Enter your GitHub Personal Access Token (PAT) with repo access.\n",
        "GITHUB_TOKEN = getpass.getpass('Enter your GitHub PAT: ')\n",
        "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/Vinay-O/HouseBrainLLM.git\"\n",
        "REPO_DIR = \"/content/HouseBrainLLM\"\n",
        "\n",
        "# Clone the repository\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(\"Repository already exists. Pulling latest changes...\")\n",
        "    subprocess.run(f\"cd {REPO_DIR} && git pull\", shell=True, check=True)\n",
        "else:\n",
        "    print(\"Cloning repository...\")\n",
        "    subprocess.run(f\"git clone {REPO_URL} {REPO_DIR}\", shell=True, check=True)\n",
        "\n",
        "print(\"‚úÖ Repository is ready.\")\n",
        "\n",
        "# --- Install Dependencies ---\n",
        "#@markdown Install necessary Python packages.\n",
        "!pip install -q pydantic GitPython\n",
        "\n",
        "print(\"‚úÖ Dependencies installed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 2. Configure and Start Ollama Server\n",
        "# @markdown This cell will download and start the Ollama server, then pull the specified model.\n",
        "# @markdown **Important:** For the Diamond run, we should ideally use our fine-tuned \"Journeyman\" model. For now, we will continue to use a powerful base model like Mixtral.\n",
        "\n",
        "MODEL_NAME = \"mixtral:instruct\" # @param [\"mixtral:instruct\", \"qwen2:7b\", \"llama3:8b\", \"mistral:7b-instruct\"]\n",
        "\n",
        "# Download and start Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama():\n",
        "    try:\n",
        "        subprocess.run(\"ollama serve\", shell=True, check=True, capture_output=True, text=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Ollama server failed: {e.stderr}\")\n",
        "\n",
        "print(\"üöÄ Starting Ollama server in the background...\")\n",
        "ollama_thread = threading.Thread(target=run_ollama)\n",
        "ollama_thread.daemon = True\n",
        "ollama_thread.start()\n",
        "\n",
        "# Wait for the server to be ready\n",
        "print(\"‚è≥ Waiting for Ollama server to initialize...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# Pull the model\n",
        "print(f\"üì¶ Pulling model: {MODEL_NAME}. This may take a while...\")\n",
        "try:\n",
        "    subprocess.run(f\"ollama pull {MODEL_NAME}\", shell=True, check=True, capture_output=True, text=True)\n",
        "    print(f\"‚úÖ Model {MODEL_NAME} is ready.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Failed to pull model. Trying default tag...\")\n",
        "    base_model = MODEL_NAME.split(':')[0]\n",
        "    subprocess.run(f\"ollama pull {base_model}\", shell=True, check=True)\n",
        "    print(f\"‚úÖ Model {base_model} is ready.\")\n",
        "\n",
        "# Verify Ollama is running\n",
        "!ollama list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 3. Generate Diamond Prompts & Run the Factory\n",
        "# @markdown This cell first generates 2,500 complex prompts and then runs the assembly line for each one.\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "import textwrap\n",
        "import subprocess\n",
        "\n",
        "# --- 1. Generate Diamond Prompts ---\n",
        "os.chdir(REPO_DIR)\n",
        "prompt_script_path = \"scripts/generate_diamond_prompts.py\"\n",
        "prompt_output_file = \"/content/diamond_prompts.txt\"\n",
        "num_diamond_prompts = 2500\n",
        "\n",
        "print(f\"--- Generating {num_diamond_prompts} Diamond-tier prompts ---\")\n",
        "prompt_command = [\n",
        "    \"python3\",\n",
        "    prompt_script_path,\n",
        "    \"--num-prompts\", str(num_diamond_prompts),\n",
        "    \"--output-file\", prompt_output_file\n",
        "]\n",
        "subprocess.run(prompt_command)\n",
        "print(f\"‚úÖ Diamond prompts saved to {prompt_output_file}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- 2. Load Prompts ---\n",
        "print(f\"Loading prompts from {prompt_output_file}...\")\n",
        "try:\n",
        "    with open(prompt_output_file, 'r') as f:\n",
        "        prompts = [line.strip() for line in f if line.strip()]\n",
        "    print(f\"‚úÖ Successfully loaded {len(prompts)} prompts.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Prompt file not found at {prompt_output_file}.\")\n",
        "    prompts = []\n",
        "\n",
        "# --- 3. Run the Data Factory ---\n",
        "#@markdown Specify the output directory in your Google Drive for the Diamond dataset.\n",
        "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/housebrain_diamond_dataset\"\n",
        "\n",
        "if prompts:\n",
        "    assembly_line_script_path = \"scripts/run_complete_assembly_line.py\"\n",
        "    run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_dir = os.path.join(DRIVE_OUTPUT_DIR, f\"run_{run_timestamp}\")\n",
        "\n",
        "    print(f\"\\nOutput directory is ready at: {output_dir}\")\n",
        "    print(f\"Found {len(prompts)} prompts to process.\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        print(f\"Processing prompt {i+1}/{len(prompts)}\")\n",
        "        prompt_short = textwrap.shorten(prompt, width=100, placeholder=\"...\")\n",
        "        print(f\"PROMPT: {prompt_short}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        run_name = f\"prompt_{i+1:04d}\"\n",
        "        command = [\n",
        "            \"python3\", assembly_line_script_path,\n",
        "            \"--prompt\", prompt,\n",
        "            \"--output-dir\", output_dir,\n",
        "            \"--run-name\", run_name,\n",
        "            \"--model\", MODEL_NAME,\n",
        "            \"--max-retries\", \"5\"\n",
        "        ]\n",
        "        subprocess.run(command)\n",
        "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"üéâ Diamond Data Factory run complete! Check your Google Drive for the generated files.\")\n",
        "else:\n",
        "    print(\"No prompts to process. Please check your configuration.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 4. (Optional) Download Generated Diamond Dataset\n",
        "# @markdown Run this cell after the data generation is complete to compress and download the entire output folder.\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Define the source directory in Google Drive and the target zip file path\n",
        "source_dir = \"/content/drive/MyDrive/housebrain_diamond_dataset\"\n",
        "zip_filename = \"housebrain_diamond_dataset.zip\"\n",
        "zip_filepath = f\"/content/{zip_filename}\"\n",
        "\n",
        "if os.path.exists(source_dir):\n",
        "    print(f\"Compressing '{source_dir}' into '{zip_filepath}'...\")\n",
        "    shutil.make_archive(zip_filepath.replace('.zip', ''), 'zip', source_dir)\n",
        "    print(\"‚úÖ Compression complete.\")\n",
        "\n",
        "    # Provide a download link\n",
        "    print(f\"\\nDownloading '{zip_filename}'...\")\n",
        "    files.download(zip_filepath)\n",
        "else:\n",
        "    print(f\"ERROR: The source directory '{source_dir}' was not found. Please ensure the Data Factory ran correctly.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
