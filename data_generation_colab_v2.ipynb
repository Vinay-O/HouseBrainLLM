{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain Data Factory 2.0: The Architect's Assembly Line (Parallel Mode)\n",
        "\n",
        "This notebook is the control center for generating a high-quality, architecturally-sound dataset for training the HouseBrain LLM. It is designed for **large-scale, parallel data generation.**\n",
        "\n",
        "**Our Strategy:**\n",
        "1.  **Generate a Master Prompt File (Once)**: Run Cell 4 a single time to generate a large (e.g., 30,000) list of prompts and save it to your Google Drive.\n",
        "2.  **Run Multiple Notebooks in Parallel**: You can open this same notebook using different Google accounts.\n",
        "3.  **Process Random Batches**: Each notebook instance will read from the master prompt list, select a random, unique batch of prompts to process, and save the results to a central dataset folder on your Drive.\n",
        "4.  **Avoid Duplicate Work**: The script checks if a plan for a given prompt already exists, allowing multiple instances to contribute to the same dataset without collisions.\n",
        "\n",
        "## Instructions\n",
        "1.  **Set Your GitHub PAT**: In Cell 1, you will be prompted to enter a GitHub Personal Access Token to clone the repository.\n",
        "2.  **(First Time Only) Run Cell 4**: Run Cell 4 to create your master `platinum_prompts.txt` file in Google Drive. You only need to do this once.\n",
        "3.  **Run the Factory (Cell 3)**: Run cells 1, 2, and 3. You can configure the number of plans you want the current notebook instance to generate in Cell 3.\n",
        "4.  **Repeat**: Open this notebook with other accounts, mount the same Google Drive, and run Cell 3 again to generate more data in parallel.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 1. Setup Environment\n",
        "# @markdown Mount Google Drive and clone the repository using a secure token.\n",
        "from google.colab import drive\n",
        "import os\n",
        "import getpass\n",
        "import subprocess\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted.\")\n",
        "\n",
        "# --- GitHub Setup ---\n",
        "#@markdown Enter your GitHub Personal Access Token (PAT) with repo access.\n",
        "GITHUB_TOKEN = getpass.getpass('Enter your GitHub PAT: ')\n",
        "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/Vinay-O/HouseBrainLLM.git\"\n",
        "REPO_DIR = \"/content/HouseBrainLLM\"\n",
        "\n",
        "# Clone the repository\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(\"Repository already exists. Pulling latest changes...\")\n",
        "    # Use subprocess.run for better error handling\n",
        "    subprocess.run(f\"cd {REPO_DIR} && git pull\", shell=True, check=True)\n",
        "else:\n",
        "    print(\"Cloning repository...\")\n",
        "    subprocess.run(f\"git clone {REPO_URL} {REPO_DIR}\", shell=True, check=True)\n",
        "\n",
        "print(\"‚úÖ Repository is ready.\")\n",
        "\n",
        "# --- Install Dependencies ---\n",
        "#@markdown Install necessary Python packages from the new requirements file.\n",
        "requirements_path = os.path.join(REPO_DIR, \"requirements.txt\")\n",
        "if os.path.exists(requirements_path):\n",
        "    print(\"Installing dependencies from requirements.txt...\")\n",
        "    !pip install -q -r {requirements_path}\n",
        "    print(\"‚úÖ Dependencies installed.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è requirements.txt not found. Installing default packages.\")\n",
        "    !pip install -q pydantic\n",
        "\n",
        "print(\"‚úÖ Environment setup complete.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 2. Configure and Start Ollama Server\n",
        "# @markdown This cell will download and start the Ollama server, then pull the specified model.\n",
        "# @markdown The process will take a few minutes.\n",
        "\n",
        "MODEL_NAME = \"mixtral:instruct\" # @param [\"mixtral:instruct\", \"qwen2:7b\", \"llama3:8b\", \"mistral:7b-instruct\", \"qwen2:72b\"]\n",
        "\n",
        "# Download and start Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama():\n",
        "    try:\n",
        "        subprocess.run(\"ollama serve\", shell=True, check=True, capture_output=True, text=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Ollama server failed: {e.stderr}\")\n",
        "\n",
        "print(\"üöÄ Starting Ollama server in the background...\")\n",
        "ollama_thread = threading.Thread(target=run_ollama)\n",
        "ollama_thread.daemon = True\n",
        "ollama_thread.start()\n",
        "\n",
        "# Wait for the server to be ready\n",
        "print(\"‚è≥ Waiting for Ollama server to initialize...\")\n",
        "time.sleep(15) # Increased wait time for stability\n",
        "\n",
        "# Pull the model\n",
        "print(f\"üì¶ Pulling model: {MODEL_NAME}. This may take a while...\")\n",
        "try:\n",
        "    process = subprocess.run(\n",
        "        f\"ollama pull {MODEL_NAME}\",\n",
        "        shell=True, check=True, capture_output=True, text=True, timeout=600\n",
        "    )\n",
        "    print(f\"‚úÖ Model {MODEL_NAME} is ready.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Error pulling model: {e.stderr}\")\n",
        "    print(\"This might happen if the model name is incorrect or the Ollama server is not ready.\")\n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"Timed out while pulling the model. The model might be very large or the connection slow.\")\n",
        "\n",
        "\n",
        "# Verify Ollama is running\n",
        "!ollama list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 3. Run the Data Factory (Parallel Mode)\n",
        "# @markdown This cell is designed for large-scale, parallel data generation.\n",
        "# @markdown It reads prompts from a central file in your Google Drive, processes a random batch, and saves to a central dataset folder.\n",
        "# @markdown You can run this notebook on multiple accounts simultaneously to accelerate data creation.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import textwrap\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import urllib.error\n",
        "from inspect import getsource\n",
        "from pydantic import BaseModel, ValidationError\n",
        "import random\n",
        "import hashlib\n",
        "\n",
        "# --- Configuration ---\n",
        "#@markdown The central location in your Google Drive for the master prompt file.\n",
        "DRIVE_PROMPT_FILE = \"/content/drive/MyDrive/housebrain_prompts/platinum_prompts.txt\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown The central location in your Google Drive to save the final dataset.\n",
        "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/housebrain_platinum_dataset\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown The number of plans this specific Colab instance should generate in this run.\n",
        "NUM_PLANS_TO_GENERATE = 100 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown The Ollama model to use for generation.\n",
        "MODEL_NAME = \"mixtral:instruct\" #@param [\"mixtral:instruct\", \"qwen2:7b\", \"llama3:8b\"]\n",
        "# --- End Configuration ---\n",
        "\n",
        "\n",
        "# --- Setup Paths and Logging ---\n",
        "REPO_DIR = \"/content/HouseBrainLLM\"\n",
        "if REPO_DIR not in sys.path:\n",
        "    sys.path.insert(0, REPO_DIR)\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Import Schema from Cloned Repo ---\n",
        "try:\n",
        "    from src.housebrain.schema import HouseOutput, RoomType\n",
        "    print(\"‚úÖ Successfully imported HouseBrain schema.\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import HouseBrain schema: {e}\")\n",
        "    print(\"Please ensure the repository was cloned correctly in Step 1.\")\n",
        "    # Stop execution if schema fails\n",
        "    raise e\n",
        "\n",
        "# --- Self-Contained Generation Logic (with A+ Prompts) ---\n",
        "VALID_ROOM_TYPES = [e.value for e in RoomType]\n",
        "\n",
        "# Using regular strings and .format() to avoid KeyError with f-strings\n",
        "STAGE_1_PROMPT_TEMPLATE = \"\"\"You are an expert AI architect. Your task is to generate ONLY the high-level geometric layout for a house based on a user's prompt.\n",
        "\n",
        "**CRITICAL INSTRUCTIONS:**\n",
        "1.  Focus ONLY on `levels` and `rooms`.\n",
        "2.  Rooms MUST have an `id`, `type`, and non-overlapping `bounds`.\n",
        "3.  The `type` for each room MUST be one of the following valid options: `{valid_room_types}`. Do NOT invent new types.\n",
        "4.  DO NOT include `doors` or `windows` in this stage.\n",
        "5.  Your output MUST be a single, valid JSON object with a root \"levels\" key.\n",
        "\n",
        "**Golden Example of a perfect room structure:**\n",
        "```json\n",
        "{{\n",
        "  \"id\": \"living_room_0\",\n",
        "  \"type\": \"living_room\",\n",
        "  \"bounds\": {{\"x\": 10, \"y\": 10, \"width\": 20, \"height\": 15}}\n",
        "}}\n",
        "```\n",
        "---\n",
        "**User Prompt:**\n",
        "{user_prompt}\n",
        "---\n",
        "Now, generate the JSON for the house layout, adhering strictly to the instructions provided.\"\"\"\n",
        "\n",
        "STAGE_2_PROMPT_TEMPLATE = \"\"\"You are an expert AI architect. Your task is to add `doors` and `windows` to a pre-existing house layout.\n",
        "\n",
        "**CRITICAL INSTRUCTIONS:**\n",
        "1.  Use the official `RoomType` enum for all room `type` fields. Valid types are: `{valid_room_types}`.\n",
        "2.  DO NOT change the existing `id`, `type`, or `bounds` of the rooms.\n",
        "3.  `Door` and `Window` objects MUST be complete and valid JSON objects, conforming to the schema.\n",
        "4.  Your final output must be a single JSON object containing ONLY the `levels` key.\n",
        "\n",
        "**Expert Design Hints:**\n",
        "-   Place doors to create a logical and efficient flow between connected rooms.\n",
        "-   Place windows on exterior walls to maximize natural light and capture views where appropriate.\n",
        "-   Ensure `Door` objects correctly link two adjacent rooms in the `room1` and `room2` fields.\n",
        "\n",
        "**Golden Example of a perfect room with openings (Pay close attention to the structure of Door and Window):**\n",
        "```json\n",
        "\"rooms\": [\n",
        "   {{\n",
        "     \"id\": \"living_room_0\",\n",
        "     \"type\": \"living_room\",\n",
        "     \"bounds\": {{ \"x\": 10, \"y\": 10, \"width\": 20, \"height\": 15 }},\n",
        "     \"doors\": [\n",
        "       {{\n",
        "         \"position\": {{ \"x\": 20, \"y\": 25 }},\n",
        "         \"width\": 3.0,\n",
        "         \"type\": \"interior\",\n",
        "         \"room1\": \"living_room_0\",\n",
        "         \"room2\": \"dining_room_0\"\n",
        "       }}\n",
        "     ],\n",
        "     \"windows\": [\n",
        "       {{\n",
        "         \"position\": {{ \"x\": 10, \"y\": 17.5 }},\n",
        "         \"width\": 8.0,\n",
        "         \"height\": 5.0,\n",
        "         \"type\": \"sliding\",\n",
        "         \"room_id\": \"living_room_0\"\n",
        "       }}\n",
        "     ]\n",
        "   }}\n",
        "]\n",
        "```\n",
        "---\n",
        "**Full Schema Reference for Door and Window:**\n",
        "```python\n",
        "class Point2D(BaseModel):\n",
        "    x: float\n",
        "    y: float\n",
        "\n",
        "class Door(BaseModel):\n",
        "    position: Point2D\n",
        "    width: float = 3.0\n",
        "    type: str = \"interior\"\n",
        "    room1: str\n",
        "    room2: str\n",
        "\n",
        "class Window(BaseModel):\n",
        "    position: Point2D\n",
        "    width: float\n",
        "    height: float = 4.0\n",
        "    type: str = \"fixed\"\n",
        "    room_id: str\n",
        "```\n",
        "---\n",
        "**Existing House Layout (Do not change this part):**\n",
        "```json\n",
        "{existing_layout}\n",
        "```\n",
        "---\n",
        "**Original User Prompt:**\n",
        "{user_prompt}\n",
        "---\n",
        "Now, add the doors and windows to the layout, following the format of the Golden Example and Schema Reference exactly.\"\"\"\n",
        "\n",
        "def call_ollama_colab(model_name: str, prompt: str):\n",
        "    \"\"\"A direct implementation of the Ollama API call for Colab.\"\"\"\n",
        "    url = \"http://localhost:11434/api/generate\"\n",
        "    data = {\"model\": model_name, \"prompt\": prompt, \"stream\": False, \"format\": \"json\"}\n",
        "    encoded_data = json.dumps(data).encode('utf-8')\n",
        "    req = urllib.request.Request(url, data=encoded_data, headers={'Content-Type': 'application/json'})\n",
        "    try:\n",
        "        with urllib.request.urlopen(req, timeout=900) as response:\n",
        "            if response.status == 200:\n",
        "                response_data = json.loads(response.read().decode('utf-8'))\n",
        "                return response_data.get(\"response\", \"\")\n",
        "    except urllib.error.HTTPError as e:\n",
        "        error_content = e.read().decode('utf-8')\n",
        "        logger.error(f\"HTTP Error: {e.code} {e.reason} - {error_content}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error calling Ollama: {e}\")\n",
        "    return None\n",
        "\n",
        "# --- Execution ---\n",
        "print(\"--- Starting Data Factory Run (Parallel Mode) ---\")\n",
        "Path(DRIVE_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1. Load all available prompts from central file\n",
        "all_prompts = []\n",
        "try:\n",
        "    with open(DRIVE_PROMPT_FILE, 'r') as f:\n",
        "        all_prompts = [line.strip() for line in f if line.strip()]\n",
        "    if not all_prompts:\n",
        "        raise FileNotFoundError\n",
        "    print(f\"‚úÖ Found {len(all_prompts)} total prompts in the master list.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå MASTER PROMPT FILE NOT FOUND at '{DRIVE_PROMPT_FILE}'.\")\n",
        "    print(\"Please run Cell 4 to generate it before running this cell.\")\n",
        "\n",
        "# 2. Select a random batch to process\n",
        "if all_prompts:\n",
        "    random.shuffle(all_prompts)\n",
        "    prompts_to_process = all_prompts[:NUM_PLANS_TO_GENERATE]\n",
        "    print(f\"‚úÖ This run will process a random batch of {len(prompts_to_process)} prompts.\")\n",
        "\n",
        "    for i, prompt_text in enumerate(prompts_to_process):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"Processing prompt {i+1}/{len(prompts_to_process)}\")\n",
        "        \n",
        "        # Create a unique filename from the prompt hash to avoid collisions\n",
        "        prompt_hash = hashlib.sha1(prompt_text.encode()).hexdigest()[:16]\n",
        "        run_name = f\"plan_{prompt_hash}\"\n",
        "        output_file = Path(DRIVE_OUTPUT_DIR) / f\"{run_name}.json\"\n",
        "\n",
        "        if output_file.exists():\n",
        "            print(f\"‚è≠Ô∏è Skipping prompt, output file already exists: {output_file.name}\")\n",
        "            continue\n",
        "\n",
        "        print(textwrap.shorten(prompt_text, width=100, placeholder=\"...\"))\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # --- STAGE 1 ---\n",
        "        print(\"Running Stage 1: Layout Generation...\")\n",
        "        stage_1_prompt = STAGE_1_PROMPT_TEMPLATE.format(\n",
        "            user_prompt=prompt_text,\n",
        "            valid_room_types=VALID_ROOM_TYPES\n",
        "        )\n",
        "        stage_1_response = call_ollama_colab(MODEL_NAME, stage_1_prompt)\n",
        "\n",
        "        if not stage_1_response:\n",
        "            print(\"‚ùå Stage 1 Failed: No response from model.\")\n",
        "            continue\n",
        "        \n",
        "        # --- STAGE 2 ---\n",
        "        print(\"Running Stage 2: Adding Openings...\")\n",
        "        stage_2_prompt = STAGE_2_PROMPT_TEMPLATE.format(\n",
        "            existing_layout=stage_1_response,\n",
        "            user_prompt=prompt_text,\n",
        "            valid_room_types=VALID_ROOM_TYPES\n",
        "        )\n",
        "        stage_2_response = call_ollama_colab(MODEL_NAME, stage_2_prompt)\n",
        "        \n",
        "        if not stage_2_response:\n",
        "            print(\"‚ùå Stage 2 Failed: No response from model.\")\n",
        "            continue\n",
        "\n",
        "        # --- STAGE 3: Finalize & Validate ---\n",
        "        print(\"Running Stage 3: Finalizing and Validating...\")\n",
        "        try:\n",
        "            layout_with_openings = json.loads(stage_2_response)\n",
        "            \n",
        "            total_area_sqft = sum(\n",
        "                r['bounds']['width'] * r['bounds']['height']\n",
        "                for l in layout_with_openings.get(\"levels\", [])\n",
        "                for r in l.get(\"rooms\", [])\n",
        "            )\n",
        "            \n",
        "            final_plan = {\n",
        "                \"input\": {\n",
        "                    \"basicDetails\": {\"prompt\": prompt_text, \"totalArea\": total_area_sqft},\n",
        "                    \"plot\": {}, \"roomBreakdown\": []\n",
        "                },\n",
        "                \"levels\": layout_with_openings.get(\"levels\", []),\n",
        "                \"total_area\": round(total_area_sqft, 2),\n",
        "                \"construction_cost\": 0.0, \"materials\": {}, \"render_paths\": {}\n",
        "            }\n",
        "            \n",
        "            HouseOutput.model_validate(final_plan)\n",
        "\n",
        "            with open(output_file, 'w') as f:\n",
        "                json.dump(final_plan, f, indent=2)\n",
        "            print(f\"‚úÖ SUCCESS! Saved validated plan to {output_file}\")\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"‚ùå Stage 3 Failed: Could not decode JSON from Stage 2.\")\n",
        "            with open(str(output_file).replace('.json', '_failed.txt'), 'w') as f: f.write(stage_2_response)\n",
        "        except ValidationError as e:\n",
        "            print(f\"‚ùå Stage 3 Failed: Pydantic validation error - {e}\")\n",
        "            with open(str(output_file).replace('.json', '_failed.txt'), 'w') as f: f.write(stage_2_response)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Stage 3 Failed: An unexpected error occurred - {e}\")\n",
        "            with open(str(output_file).replace('.json', '_failed.txt'), 'w') as f: f.write(stage_2_response)\n",
        "    \n",
        "    print(\"\\nüéâ Data Factory run complete!\")\n",
        "else:\n",
        "    print(\"No prompts to process.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 4. (One-Time Setup) Generate Master Prompt File\n",
        "# @markdown This cell uses the `generate_prompts.py` script to create your master prompt file in Google Drive.\n",
        "# @markdown **You only need to run this cell once.**\n",
        "# @markdown Once the file is created, Cell 3 will be able to read from it for all future runs.\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Configuration ---\n",
        "#@markdown The desired location in your Google Drive for the master prompt file. This MUST match the path in Cell 3.\n",
        "DRIVE_PROMPT_FILE = \"/content/drive/MyDrive/housebrain_prompts/platinum_prompts.txt\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown The total number of prompts to generate for your master list.\n",
        "NUM_PROMPTS_TO_GENERATE = 30000 #@param {type:\"integer\"}\n",
        "# --- End Configuration ---\n",
        "\n",
        "# --- Execution ---\n",
        "REPO_DIR = \"/content/HouseBrainLLM\"\n",
        "script_path = os.path.join(REPO_DIR, \"scripts/generate_prompts.py\")\n",
        "\n",
        "# Ensure the repository is in the correct directory\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# Ensure the target directory in Drive exists\n",
        "Path(DRIVE_PROMPT_FILE).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Running prompt generation script to create {NUM_PROMPTS_TO_GENERATE} prompts...\")\n",
        "# Use an f-string for safer command construction\n",
        "command = f'python3 \"{script_path}\" --num-prompts {NUM_PROMPTS_TO_GENERATE} --output-file \"{DRIVE_PROMPT_FILE}\"'\n",
        "!{command}\n",
        "\n",
        "print(\"\\n--- Verification ---\")\n",
        "if Path(DRIVE_PROMPT_FILE).exists():\n",
        "    print(f\"‚úÖ Master prompt file successfully created at: {DRIVE_PROMPT_FILE}\")\n",
        "    print(\"First 5 prompts in the file:\")\n",
        "    !head -n 5 \"{DRIVE_PROMPT_FILE}\"\n",
        "else:\n",
        "    print(f\"‚ùå ERROR: Master prompt file was not created. Please check for errors above.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 5. (Optional) Download Generated Dataset\n",
        "# @markdown Run this cell after the data generation is complete to compress and download the entire output folder.\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the source directory in Google Drive. This should match DRIVE_OUTPUT_DIR from Cell 3.\n",
        "source_dir = \"/content/drive/MyDrive/housebrain_platinum_dataset\"\n",
        "\n",
        "# Create a timestamped zip filename\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "zip_filename = f\"housebrain_dataset_{timestamp}.zip\"\n",
        "zip_filepath = f\"/content/{zip_filename}\"\n",
        "\n",
        "if os.path.exists(source_dir) and os.listdir(source_dir):\n",
        "    # Create the zip archive\n",
        "    print(f\"Compressing '{source_dir}' into '{zip_filepath}'...\")\n",
        "    shutil.make_archive(zip_filepath.replace('.zip', ''), 'zip', source_dir)\n",
        "    print(\"‚úÖ Compression complete.\")\n",
        "\n",
        "    # Provide a download link\n",
        "    print(f\"\\nDownloading '{zip_filename}'...\")\n",
        "    files.download(zip_filepath)\n",
        "else:\n",
        "    print(f\"‚ùå ERROR: The source directory '{source_dir}' was not found or is empty. Please ensure the Data Factory ran correctly.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain Data Factory 2.0: The Architect's Assembly Line (Parallel Mode)\n",
        "\n",
        "This notebook is the control center for generating a high-quality, architecturally-sound dataset for training the HouseBrain LLM. It is designed for **large-scale, parallel data generation.**\n",
        "\n",
        "**Our Strategy:**\n",
        "1.  **Generate a Master Prompt File (Once)**: Run Cell 4 a single time to generate a large (e.g., 30,000) list of prompts and save it to your Google Drive.\n",
        "2.  **Run Multiple Notebooks in Parallel**: You can open this same notebook using different Google accounts.\n",
        "3.  **Process Random Batches**: Each notebook instance will read from the master prompt list, select a random, unique batch of prompts to process, and save the results to a central dataset folder on your Drive.\n",
        "4.  **Avoid Duplicate Work**: The script checks if a plan for a given prompt already exists, allowing multiple instances to contribute to the same dataset without collisions.\n",
        "\n",
        "## Instructions\n",
        "1.  **Set Your GitHub PAT**: In Cell 1, you will be prompted to enter a GitHub Personal Access Token to clone the repository.\n",
        "2.  **(First Time Only) Run Cell 4**: Run Cell 4 to create your master `platinum_prompts.txt` file in Google Drive. You only need to do this once.\n",
        "3.  **Run the Factory (Cell 3)**: Run cells 1, 2, and 3. You can configure the number of plans you want the current notebook instance to generate in Cell 3.\n",
        "4.  **Repeat**: Open this notebook with other accounts, mount the same Google Drive, and run Cell 3 again to generate more data in parallel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 1. Setup Environment\n",
        "# @markdown Mount Google Drive and clone the repository using a secure token.\n",
        "from google.colab import drive\n",
        "import os\n",
        "import getpass\n",
        "import subprocess\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted.\")\n",
        "\n",
        "# --- GitHub Setup ---\n",
        "#@markdown Enter your GitHub Personal Access Token (PAT) with repo access.\n",
        "GITHUB_TOKEN = getpass.getpass('Enter your GitHub PAT: ')\n",
        "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/Vinay-O/HouseBrainLLM.git\"\n",
        "REPO_DIR = \"/content/HouseBrainLLM\"\n",
        "\n",
        "# Clone the repository\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(\"Repository already exists. Pulling latest changes...\")\n",
        "    # Use subprocess.run for better error handling\n",
        "    subprocess.run(f\"cd {REPO_DIR} && git pull\", shell=True, check=True)\n",
        "else:\n",
        "    print(\"Cloning repository...\")\n",
        "    subprocess.run(f\"git clone {REPO_URL} {REPO_DIR}\", shell=True, check=True)\n",
        "\n",
        "print(\"‚úÖ Repository is ready.\")\n",
        "\n",
        "# --- Install Dependencies ---\n",
        "#@markdown Install necessary Python packages from the new requirements file.\n",
        "requirements_path = os.path.join(REPO_DIR, \"requirements.txt\")\n",
        "if os.path.exists(requirements_path):\n",
        "    print(\"Installing dependencies from requirements.txt...\")\n",
        "    !pip install -q -r {requirements_path}\n",
        "    print(\"‚úÖ Dependencies installed.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è requirements.txt not found. Installing default packages.\")\n",
        "    !pip install -q pydantic\n",
        "\n",
        "print(\"‚úÖ Environment setup complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 2. Configure and Start Ollama Server\n",
        "# @markdown This cell will download and start the Ollama server, then pull the specified model.\n",
        "# @markdown The process will take a few minutes.\n",
        "\n",
        "MODEL_NAME = \"mixtral:instruct\" # @param [\"mixtral:instruct\", \"qwen2:7b\", \"llama3:8b\", \"mistral:7b-instruct\", \"qwen2:72b\"]\n",
        "\n",
        "# Download and start Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama():\n",
        "    try:\n",
        "        subprocess.run(\"ollama serve\", shell=True, check=True, capture_output=True, text=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Ollama server failed: {e.stderr}\")\n",
        "\n",
        "print(\"üöÄ Starting Ollama server in the background...\")\n",
        "ollama_thread = threading.Thread(target=run_ollama)\n",
        "ollama_thread.daemon = True\n",
        "ollama_thread.start()\n",
        "\n",
        "# Wait for the server to be ready\n",
        "print(\"‚è≥ Waiting for Ollama server to initialize...\")\n",
        "time.sleep(15) # Increased wait time for stability\n",
        "\n",
        "# Pull the model\n",
        "print(f\"üì¶ Pulling model: {MODEL_NAME}. This may take a while...\")\n",
        "try:\n",
        "    process = subprocess.run(\n",
        "        f\"ollama pull {MODEL_NAME}\",\n",
        "        shell=True, check=True, capture_output=True, text=True, timeout=600\n",
        "    )\n",
        "    print(f\"‚úÖ Model {MODEL_NAME} is ready.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Error pulling model: {e.stderr}\")\n",
        "    print(\"This might happen if the model name is incorrect or the Ollama server is not ready.\")\n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"Timed out while pulling the model. The model might be very large or the connection slow.\")\n",
        "\n",
        "\n",
        "# Verify Ollama is running\n",
        "!ollama list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain Data Factory 2.0: The Architect's Assembly Line\n",
        "\n",
        "This notebook is the control center for generating a high-quality, architecturally-sound dataset for training the HouseBrain LLM. It leverages the \"Architect's Assembly Line\" script, which is a robust, multi-stage pipeline designed to guide an LLM in creating valid house plans.\n",
        "\n",
        "**Our Strategy:**\n",
        "1.  **Use a powerful \"Generator\" model** (e.g., `qwen2:7b`, as recommended) within the pipeline to create draft plans.\n",
        "2.  **Run the pipeline at scale** on a large list of diverse architectural prompts.\n",
        "3.  **Save the validated, \"Gold Standard\" outputs** directly to your Google Drive.\n",
        "4.  Use this curated dataset in a separate notebook to fine-tune a more specialized \"Architect\" model (e.g., `Llama 3`).\n",
        "\n",
        "## Instructions\n",
        "1.  **Set Your GitHub PAT**: In the \"Setup\" section, you will be prompted to enter a GitHub Personal Access Token. This is required to clone the private `HouseBrainLLM` repository.\n",
        "2.  **Define Your Prompts**: In the \"Run Data Factory\" section, a default list of prompts is provided. You should replace or extend this with a much larger and more diverse list for a full data generation run.\n",
        "3.  **Run All Cells**: Once configured, select \"Runtime\" -> \"Run all\" from the menu. The notebook will set up the environment, download the necessary models, and begin the data generation process, saving the results to your Google Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain Data Factory 2.0: The Architect's Assembly Line\n",
        "\n",
        "This notebook is the control center for generating a high-quality, architecturally-sound dataset for training the HouseBrain LLM. It leverages the \"Architect's Assembly Line\" script, which is a robust, multi-stage pipeline designed to guide an LLM in creating valid house plans.\n",
        "\n",
        "**Our Strategy:**\n",
        "1.  **Use a powerful \"Generator\" model** (e.g., `qwen2:7b`, as recommended) within the pipeline to create draft plans.\n",
        "2.  **Run the pipeline at scale** on a large list of diverse architectural prompts.\n",
        "3.  **Save the validated, \"Gold Standard\" outputs** directly to your Google Drive.\n",
        "4.  Use this curated dataset in a separate notebook to fine-tune a more specialized \"Architect\" model (e.g., `Llama 3`).\n",
        "\n",
        "## Instructions\n",
        "1.  **Set Your GitHub PAT**: In the \"Setup\" section, you will be prompted to enter a GitHub Personal Access Token. This is required to clone the private `HouseBrainLLM` repository.\n",
        "2.  **Define Your Prompts**: In the \"Run Data Factory\" section, a default list of prompts is provided. You should replace or extend this with a much larger and more diverse list for a full data generation run.\n",
        "3.  **Run All Cells**: Once configured, select \"Runtime\" -> \"Run all\" from the menu. The notebook will set up the environment, download the necessary models, and begin the data generation process, saving the results to your Google Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain Data Factory 2.0: The Architect's Assembly Line\n",
        "\n",
        "This notebook is the control center for generating a high-quality, architecturally-sound dataset for training the HouseBrain LLM. It leverages the \"Architect's Assembly Line\" script, which is a robust, multi-stage pipeline designed to guide an LLM in creating valid house plans.\n",
        "\n",
        "**Our Strategy:**\n",
        "1.  **Use a powerful \"Generator\" model** (e.g., `qwen2:7b`, as recommended) within the pipeline to create draft plans.\n",
        "2.  **Run the pipeline at scale** on a large list of diverse architectural prompts.\n",
        "3.  **Save the validated, \"Gold Standard\" outputs** directly to your Google Drive.\n",
        "4.  Use this curated dataset in a separate notebook to fine-tune a more specialized \"Architect\" model (e.g., `Llama 3`).\n",
        "\n",
        "## Instructions\n",
        "1.  **Set Your GitHub PAT**: In the \"Setup\" section, you will be prompted to enter a GitHub Personal Access Token. This is required to clone the private `HouseBrainLLM` repository.\n",
        "2.  **Define Your Prompts**: In the \"Run Data Factory\" section, a default list of prompts is provided. You should replace or extend this with a much larger and more diverse list for a full data generation run.\n",
        "3.  **Run All Cells**: Once configured, select \"Runtime\" -> \"Run all\" from the menu. The notebook will set up the environment, download the necessary models, and begin the data generation process, saving the results to your Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 1. Setup Environment\n",
        "# @markdown Mount Google Drive and clone the repository using a secure token.\n",
        "from google.colab import drive\n",
        "import os\n",
        "import getpass\n",
        "import subprocess\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted.\")\n",
        "\n",
        "# --- GitHub Setup ---\n",
        "#@markdown Enter your GitHub Personal Access Token (PAT) with repo access.\n",
        "GITHUB_TOKEN = getpass.getpass('Enter your GitHub PAT: ')\n",
        "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/Vinay-O/HouseBrainLLM.git\"\n",
        "REPO_DIR = \"/content/HouseBrainLLM\"\n",
        "\n",
        "# Clone the repository\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(\"Repository already exists. Pulling latest changes...\")\n",
        "    subprocess.run(f\"cd {REPO_DIR} && git pull\", shell=True, check=True)\n",
        "else:\n",
        "    print(\"Cloning repository...\")\n",
        "    subprocess.run(f\"git clone {REPO_URL} {REPO_DIR}\", shell=True, check=True)\n",
        "\n",
        "print(\"‚úÖ Repository is ready.\")\n",
        "\n",
        "# --- Install Dependencies ---\n",
        "#@markdown Install necessary Python packages.\n",
        "!pip install -q pydantic GitPython\n",
        "\n",
        "print(\"‚úÖ Dependencies installed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 2. Configure and Start Ollama Server\n",
        "# @markdown This cell will download and start the Ollama server, then pull the specified model.\n",
        "# @markdown The process will take a few minutes.\n",
        "\n",
        "MODEL_NAME = \"mixtral:instruct\" # @param [\"mixtral:instruct\", \"qwen2:7b\", \"llama3:8b\", \"mistral:7b-instruct\", \"qwen2:72b\"]\n",
        "\n",
        "# Download and start Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama():\n",
        "    try:\n",
        "        subprocess.run(\"ollama serve\", shell=True, check=True, capture_output=True, text=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Ollama server failed: {e.stderr}\")\n",
        "\n",
        "print(\"üöÄ Starting Ollama server in the background...\")\n",
        "ollama_thread = threading.Thread(target=run_ollama)\n",
        "ollama_thread.daemon = True\n",
        "ollama_thread.start()\n",
        "\n",
        "# Wait for the server to be ready\n",
        "print(\"‚è≥ Waiting for Ollama server to initialize...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# Pull the model\n",
        "print(f\"üì¶ Pulling model: {MODEL_NAME}. This may take a while...\")\n",
        "try:\n",
        "    subprocess.run(f\"ollama pull {MODEL_NAME}\", shell=True, check=True, capture_output=True, text=True)\n",
        "    print(f\"‚úÖ Model {MODEL_NAME} is ready.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Failed to pull model. Trying default tag...\")\n",
        "    base_model = MODEL_NAME.split(':')[0]\n",
        "    subprocess.run(f\"ollama pull {base_model}\", shell=True, check=True)\n",
        "    print(f\"‚úÖ Model {base_model} is ready.\")\n",
        "\n",
        "# Verify Ollama is running\n",
        "!ollama list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 3. Run the Data Factory (Parallel Mode)\n",
        "# @markdown This cell is designed for large-scale, parallel data generation.\n",
        "# @markdown It reads prompts from a central file in your Google Drive, processes a random batch, and saves to a central dataset folder.\n",
        "# @markdown You can run this notebook on multiple accounts simultaneously to accelerate data creation.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import textwrap\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import urllib.error\n",
        "from inspect import getsource\n",
        "from pydantic import BaseModel, ValidationError\n",
        "import random\n",
        "import hashlib\n",
        "\n",
        "# --- Configuration ---\n",
        "#@markdown The central location in your Google Drive for the master prompt file.\n",
        "DRIVE_PROMPT_FILE = \"/content/drive/MyDrive/housebrain_prompts/platinum_prompts.txt\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown The central location in your Google Drive to save the final dataset.\n",
        "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/housebrain_platinum_dataset\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown The number of plans this specific Colab instance should generate in this run.\n",
        "NUM_PLANS_TO_GENERATE = 10 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown The Ollama model to use for generation.\n",
        "MODEL_NAME = \"mixtral:instruct\" #@param [\"mixtral:instruct\", \"qwen2:7b\", \"llama3:8b\"]\n",
        "# --- End Configuration ---\n",
        "\n",
        "\n",
        "# --- Setup Paths and Logging ---\n",
        "REPO_DIR = \"/content/HouseBrainLLM\"\n",
        "if REPO_DIR not in sys.path:\n",
        "    sys.path.insert(0, REPO_DIR)\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Import Schema from Cloned Repo ---\n",
        "try:\n",
        "    from src.housebrain.schema import HouseOutput, RoomType\n",
        "    print(\"‚úÖ Successfully imported HouseBrain schema.\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import HouseBrain schema: {e}\")\n",
        "    print(\"Please ensure the repository was cloned correctly in Step 1.\")\n",
        "    # Stop execution if schema fails\n",
        "    raise e\n",
        "\n",
        "# --- Self-Contained Generation Logic (with A+ Prompts) ---\n",
        "VALID_ROOM_TYPES = [e.value for e in RoomType]\n",
        "\n",
        "# --- TEMPLATE FIX ---\n",
        "# Removed the initial 'f' from the string definition to prevent premature formatting.\n",
        "# All placeholders are now handled by a single, robust .format() call later.\n",
        "STAGE_1_PROMPT_TEMPLATE = \"\"\"You are an expert AI architect. Your task is to generate ONLY the high-level geometric layout for a house based on a user's prompt.\n",
        "\n",
        "**CRITICAL INSTRUCTIONS:**\n",
        "1.  Focus ONLY on `levels` and `rooms`.\n",
        "2.  Rooms MUST have an `id`, `type`, and non-overlapping `bounds`.\n",
        "3.  The `type` for each room MUST be one of the following valid options: `{valid_room_types}`. Do NOT invent new types.\n",
        "4.  DO NOT include `doors` or `windows` in this stage.\n",
        "5.  Your output MUST be a single, valid JSON object with a root \"levels\" key.\n",
        "\n",
        "**Golden Example of a perfect room structure:**\n",
        "```json\n",
        "{{\n",
        "  \"id\": \"living_room_0\",\n",
        "  \"type\": \"living_room\",\n",
        "  \"bounds\": {{\"x\": 10, \"y\": 10, \"width\": 20, \"height\": 15}}\n",
        "}}\n",
        "```\n",
        "---\n",
        "**User Prompt:**\n",
        "{user_prompt}\n",
        "---\n",
        "Now, generate the JSON for the house layout, adhering strictly to the instructions provided.\"\"\"\n",
        "\n",
        "STAGE_2_PROMPT_TEMPLATE = \"\"\"You are an expert AI architect. Your task is to add `doors` and `windows` to a pre-existing house layout.\n",
        "\n",
        "**CRITICAL INSTRUCTIONS:**\n",
        "1.  Use the official `RoomType` enum for all room `type` fields. Valid types are: `{valid_room_types}`.\n",
        "2.  DO NOT change the existing `id`, `type`, or `bounds` of the rooms.\n",
        "3.  `Door` and `Window` objects MUST be complete and valid JSON objects, conforming to the schema.\n",
        "4.  Your final output must be a single JSON object containing ONLY the `levels` key.\n",
        "\n",
        "**Expert Design Hints:**\n",
        "-   Place doors to create a logical and efficient flow between connected rooms.\n",
        "-   Place windows on exterior walls to maximize natural light and capture views where appropriate.\n",
        "-   Ensure `Door` objects correctly link two adjacent rooms in the `room1` and `room2` fields.\n",
        "\n",
        "**Golden Example of a perfect room with openings (Pay close attention to the structure of Door and Window):**\n",
        "```json\n",
        "\"rooms\": [\n",
        "   {{\n",
        "     \"id\": \"living_room_0\",\n",
        "     \"type\": \"living_room\",\n",
        "     \"bounds\": {{ \"x\": 10, \"y\": 10, \"width\": 20, \"height\": 15 }},\n",
        "     \"doors\": [\n",
        "       {{\n",
        "         \"position\": {{ \"x\": 20, \"y\": 25 }},\n",
        "         \"width\": 3.0,\n",
        "         \"type\": \"interior\",\n",
        "         \"room1\": \"living_room_0\",\n",
        "         \"room2\": \"dining_room_0\"\n",
        "       }}\n",
        "     ],\n",
        "     \"windows\": [\n",
        "       {{\n",
        "         \"position\": {{ \"x\": 10, \"y\": 17.5 }},\n",
        "         \"width\": 8.0,\n",
        "         \"height\": 5.0,\n",
        "         \"type\": \"sliding\",\n",
        "         \"room_id\": \"living_room_0\"\n",
        "       }}\n",
        "     ]\n",
        "   }}\n",
        "]\n",
        "```\n",
        "---\n",
        "**Full Schema Reference for Door and Window:**\n",
        "```python\n",
        "class Point2D(BaseModel):\n",
        "    x: float\n",
        "    y: float\n",
        "\n",
        "class Door(BaseModel):\n",
        "    position: Point2D\n",
        "    width: float = 3.0\n",
        "    type: str = \"interior\"\n",
        "    room1: str\n",
        "    room2: str\n",
        "\n",
        "class Window(BaseModel):\n",
        "    position: Point2D\n",
        "    width: float\n",
        "    height: float = 4.0\n",
        "    type: str = \"fixed\"\n",
        "    room_id: str\n",
        "```\n",
        "---\n",
        "**Existing House Layout (Do not change this part):**\n",
        "```json\n",
        "{existing_layout}\n",
        "```\n",
        "---\n",
        "**Original User Prompt:**\n",
        "{user_prompt}\n",
        "---\n",
        "Now, add the doors and windows to the layout, following the format of the Golden Example and Schema Reference exactly.\"\"\"\n",
        "\n",
        "\n",
        "def call_ollama_colab(model_name: str, prompt: str):\n",
        "    \"\"\"A direct implementation of the Ollama API call for Colab.\"\"\"\n",
        "    url = \"http://localhost:11434/api/generate\"\n",
        "    data = {\"model\": model_name, \"prompt\": prompt, \"stream\": False, \"format\": \"json\"}\n",
        "    encoded_data = json.dumps(data).encode('utf-8')\n",
        "    req = urllib.request.Request(url, data=encoded_data, headers={'Content-Type': 'application/json'})\n",
        "    try:\n",
        "        with urllib.request.urlopen(req, timeout=900) as response:\n",
        "            if response.status == 200:\n",
        "                response_data = json.loads(response.read().decode('utf-8'))\n",
        "                return response_data.get(\"response\", \"\")\n",
        "    except urllib.error.HTTPError as e:\n",
        "        error_content = e.read().decode('utf-8')\n",
        "        logger.error(f\"HTTP Error: {e.code} {e.reason} - {error_content}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Unexpected error calling Ollama: {e}\")\n",
        "    return None\n",
        "\n",
        "# --- Execution ---\n",
        "print(\"--- Starting Data Factory Run (Parallel Mode) ---\")\n",
        "Path(DRIVE_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1. Load all available prompts from central file\n",
        "all_prompts = []\n",
        "try:\n",
        "    with open(DRIVE_PROMPT_FILE, 'r') as f:\n",
        "        all_prompts = [line.strip() for line in f if line.strip()]\n",
        "    if not all_prompts:\n",
        "        raise FileNotFoundError\n",
        "    print(f\"‚úÖ Found {len(all_prompts)} total prompts in the master list.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå MASTER PROMPT FILE NOT FOUND at '{DRIVE_PROMPT_FILE}'.\")\n",
        "    print(\"Please run Cell 4 to generate it before running this cell.\")\n",
        "\n",
        "# 2. Select a random batch to process\n",
        "if all_prompts:\n",
        "    random.shuffle(all_prompts)\n",
        "    prompts_to_process = all_prompts[:NUM_PLANS_TO_GENERATE]\n",
        "    print(f\"‚úÖ This run will process a random batch of {len(prompts_to_process)} prompts.\")\n",
        "\n",
        "    for i, prompt_text in enumerate(prompts_to_process):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"Processing prompt {i+1}/{len(prompts_to_process)}\")\n",
        "        \n",
        "        # Create a unique filename from the prompt hash to avoid collisions\n",
        "        prompt_hash = hashlib.sha1(prompt_text.encode()).hexdigest()[:16]\n",
        "        run_name = f\"plan_{prompt_hash}\"\n",
        "        output_file = Path(DRIVE_OUTPUT_DIR) / f\"{run_name}.json\"\n",
        "\n",
        "        if output_file.exists():\n",
        "            print(f\"‚è≠Ô∏è Skipping prompt, output file already exists: {output_file.name}\")\n",
        "            continue\n",
        "\n",
        "        print(textwrap.shorten(prompt_text, width=100, placeholder=\"...\"))\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # --- STAGE 1 ---\n",
        "        print(\"Running Stage 1: Layout Generation...\")\n",
        "        # --- FIX ---\n",
        "        # Pass all required values into the .format() call at once.\n",
        "        stage_1_prompt = STAGE_1_PROMPT_TEMPLATE.format(\n",
        "            user_prompt=prompt_text,\n",
        "            valid_room_types=VALID_ROOM_TYPES\n",
        "        )\n",
        "        stage_1_response = call_ollama_colab(MODEL_NAME, stage_1_prompt)\n",
        "\n",
        "        if not stage_1_response:\n",
        "            print(\"‚ùå Stage 1 Failed: No response from model.\")\n",
        "            continue\n",
        "        \n",
        "        # --- STAGE 2 ---\n",
        "        print(\"Running Stage 2: Adding Openings...\")\n",
        "        # --- FIX ---\n",
        "        # Pass all required values into the .format() call at once.\n",
        "        stage_2_prompt = STAGE_2_PROMPT_TEMPLATE.format(\n",
        "            existing_layout=stage_1_response,\n",
        "            user_prompt=prompt_text,\n",
        "            valid_room_types=VALID_ROOM_TYPES\n",
        "        )\n",
        "        stage_2_response = call_ollama_colab(MODEL_NAME, stage_2_prompt)\n",
        "        \n",
        "        if not stage_2_response:\n",
        "            print(\"‚ùå Stage 2 Failed: No response from model.\")\n",
        "            continue\n",
        "\n",
        "        # --- STAGE 3: Finalize & Validate ---\n",
        "        print(\"Running Stage 3: Finalizing and Validating...\")\n",
        "        try:\n",
        "            layout_with_openings = json.loads(stage_2_response)\n",
        "            \n",
        "            total_area_sqft = sum(\n",
        "                r['bounds']['width'] * r['bounds']['height']\n",
        "                for l in layout_with_openings.get(\"levels\", [])\n",
        "                for r in l.get(\"rooms\", [])\n",
        "            )\n",
        "            \n",
        "            final_plan = {\n",
        "                \"input\": {\n",
        "                    \"basicDetails\": {\"prompt\": prompt_text, \"totalArea\": total_area_sqft},\n",
        "                    \"plot\": {}, \"roomBreakdown\": []\n",
        "                },\n",
        "                \"levels\": layout_with_openings.get(\"levels\", []),\n",
        "                \"total_area\": round(total_area_sqft, 2),\n",
        "                \"construction_cost\": 0.0, \"materials\": {}, \"render_paths\": {}\n",
        "            }\n",
        "            \n",
        "            HouseOutput.model_validate(final_plan)\n",
        "\n",
        "            with open(output_file, 'w') as f:\n",
        "                json.dump(final_plan, f, indent=2)\n",
        "            print(f\"‚úÖ SUCCESS! Saved validated plan to {output_file}\")\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"‚ùå Stage 3 Failed: Could not decode JSON from Stage 2.\")\n",
        "            with open(str(output_file).replace('.json', '_failed.txt'), 'w') as f: f.write(stage_2_response)\n",
        "        except ValidationError as e:\n",
        "            print(f\"‚ùå Stage 3 Failed: Pydantic validation error - {e}\")\n",
        "            with open(str(output_file).replace('.json', '_failed.txt'), 'w') as f: f.write(stage_2_response)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Stage 3 Failed: An unexpected error occurred - {e}\")\n",
        "            with open(str(output_file).replace('.json', '_failed.txt'), 'w') as f: f.write(stage_2_response)\n",
        "    \n",
        "    print(\"\\nüéâ Data Factory run complete!\")\n",
        "else:\n",
        "    print(\"No prompts to process.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 4. (Optional) Create `platinum_prompts.txt` for Large-Scale Runs\n",
        "# @markdown Run this cell **once** to create the `platinum_prompts.txt` file in your Colab environment.\n",
        "# @markdown You can then modify the cell above (Cell 3) to read prompts from this file instead of the small list.\n",
        "\n",
        "import random\n",
        "\n",
        "# --- Components for Prompt Generation ---\n",
        "STYLES = [\n",
        "    \"Modern\", \"Contemporary\", \"Minimalist\", \"Traditional Kerala-style 'Nalukettu'\",\n",
        "    \"Colonial\", \"Industrial\", \"Scandinavian\", \"Bohemian\", \"Farmhouse\", \"Chettinad-style\",\n",
        "    \"Eco-friendly\", \"Brutalist\", \"Art Deco\", \"Mediterranean\"\n",
        "]\n",
        "SIZES_BHK = [\"1BHK\", \"2BHK\", \"3BHK\", \"4BHK\", \"5BHK\", \"6BHK\", \"studio apartment\"]\n",
        "SIZES_SQFT = [\n",
        "    \"800 sqft\", \"1000 sqft\", \"1200 sqft\", \"1500 sqft\", \"1800 sqft\",\n",
        "    \"2000 sqft\", \"2500 sqft\", \"3000 sqft\", \"4000 sqft\", \"5000 sqft\"\n",
        "]\n",
        "FLOORS = [\n",
        "    \"single-story\", \"two-story\", \"G+1\", \"G+2\", \"duplex\", \"triplex\",\n",
        "    \"split-level\", \"penthouse\"\n",
        "]\n",
        "STRUCTURE_TYPES = [\"house\", \"villa\", \"apartment\", \"bungalow\", \"farmhouse\", \"townhouse\", \"cottage\"]\n",
        "PLOT_SIZES = [\n",
        "    \"30x40 feet\", \"30x50 feet\", \"40x60 feet\", \"50x80 feet\", \"60x90 feet\",\n",
        "    \"80x100 feet\", \"100x100 feet\", \"corner\", \"irregular\"\n",
        "]\n",
        "FEATURES = [\n",
        "    \"with an open-plan kitchen and living area\", \"with a swimming pool\", \"with a home theater\",\n",
        "    \"with a large garden\", \"with a central courtyard\", \"with a dedicated home office\",\n",
        "    \"with a private gym\", \"featuring floor-to-ceiling windows\", \"with a rooftop terrace\",\n",
        "    \"with a two-car garage\", \"with servant's quarters\", \"with a library\", \"with a spacious balcony for each bedroom\"\n",
        "]\n",
        "CONSTRAINTS = [\n",
        "    \"and be Vastu-compliant\", \"with a North-facing entrance\", \"with a West-facing plot\",\n",
        "    \"on a tight budget\", \"for a luxury segment\", \"designed for a family of four\",\n",
        "    \"with a focus on natural light and ventilation\", \"for a joint family\", \"as a bachelor pad\",\n",
        "    \"to be wheelchair accessible\"\n",
        "]\n",
        "\n",
        "def generate_prompt():\n",
        "    prompt_parts = []\n",
        "    style = random.choice(STYLES)\n",
        "    num_floors = random.choice(FLOORS)\n",
        "    bhk = random.choice(SIZES_BHK)\n",
        "    structure = random.choice(STRUCTURE_TYPES)\n",
        "    prompt_parts.append(f\"Design a {style}, {num_floors} {bhk} {structure}\")\n",
        "    if random.random() < 0.7:\n",
        "        plot = random.choice(PLOT_SIZES)\n",
        "        prompt_parts.append(f\"for a {plot} plot\")\n",
        "    else:\n",
        "        area = random.choice(SIZES_SQFT)\n",
        "        prompt_parts.append(f\"with a total area of {area}\")\n",
        "    num_features = random.randint(1, 3)\n",
        "    selected_features = random.sample(FEATURES, num_features)\n",
        "    prompt_parts.extend(selected_features)\n",
        "    if random.random() < 0.6:\n",
        "        num_constraints = random.randint(1, 2)\n",
        "        selected_constraints = random.sample(CONSTRAINTS, num_constraints)\n",
        "        prompt_parts.extend(selected_constraints)\n",
        "    return \". \".join(prompt_parts) + \".\"\n",
        "\n",
        "NUM_PROMPTS = 10000\n",
        "OUTPUT_FILE = \"/content/platinum_prompts.txt\"\n",
        "\n",
        "print(f\"Generating {NUM_PROMPTS} prompts and saving to {OUTPUT_FILE}...\")\n",
        "with open(OUTPUT_FILE, 'w') as f:\n",
        "    for i in range(NUM_PROMPTS):\n",
        "        prompt = generate_prompt()\n",
        "        f.write(f\"{prompt}\\\\n\")\n",
        "print(f\"‚úÖ Successfully generated and saved {NUM_PROMPTS} prompts.\")\n",
        "\n",
        "# Display the first 5 prompts to verify\n",
        "!head -n 5 {OUTPUT_FILE}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 5. (Optional) Download Generated Dataset\n",
        "# @markdown Run this cell after the data generation is complete to compress and download the entire output folder.\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Define the source directory in Google Drive and the target zip file path\n",
        "# This should match the DRIVE_OUTPUT_DIR from Cell 3\n",
        "source_dir = \"/content/drive/MyDrive/housebrain_dataset\"\n",
        "zip_filename = \"housebrain_dataset.zip\"\n",
        "zip_filepath = f\"/content/{zip_filename}\"\n",
        "\n",
        "if os.path.exists(source_dir):\n",
        "    # Create the zip archive\n",
        "    print(f\"Compressing '{source_dir}' into '{zip_filepath}'...\")\n",
        "    shutil.make_archive(zip_filepath.replace('.zip', ''), 'zip', source_dir)\n",
        "    print(\"‚úÖ Compression complete.\")\n",
        "\n",
        "    # Provide a download link\n",
        "    print(f\"\\nDownloading '{zip_filename}'...\")\n",
        "    files.download(zip_filepath)\n",
        "else:\n",
        "    print(f\"ERROR: The source directory '{source_dir}' was not found. Please ensure the Data Factory ran correctly.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 1. Setup Environment\n",
        "# @markdown Mount Google Drive and clone the repository using a secure token.\n",
        "from google.colab import drive\n",
        "import os\n",
        "import getpass\n",
        "import subprocess\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted.\")\n",
        "\n",
        "# --- GitHub Setup ---\n",
        "#@markdown Enter your GitHub Personal Access Token (PAT) with repo access.\n",
        "GITHUB_TOKEN = getpass.getpass('Enter your GitHub PAT: ')\n",
        "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/Vinay-O/HouseBrainLLM.git\"\n",
        "REPO_DIR = \"/content/HouseBrainLLM\"\n",
        "\n",
        "# Clone the repository\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(\"Repository already exists. Pulling latest changes...\")\n",
        "    subprocess.run(f\"cd {REPO_DIR} && git pull\", shell=True, check=True)\n",
        "else:\n",
        "    print(\"Cloning repository...\")\n",
        "    subprocess.run(f\"git clone {REPO_URL} {REPO_DIR}\", shell=True, check=True)\n",
        "\n",
        "print(\"‚úÖ Repository is ready.\")\n",
        "\n",
        "# --- Install Dependencies ---\n",
        "#@markdown Install necessary Python packages.\n",
        "!pip install -q pydantic GitPython\n",
        "\n",
        "print(\"‚úÖ Dependencies installed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 2. Configure and Start Ollama Server\n",
        "# @markdown This cell will download and start the Ollama server, then pull the specified model.\n",
        "# @markdown The process will take a few minutes.\n",
        "\n",
        "MODEL_NAME = \"mixtral:instruct\" # @param [\"mixtral:instruct\", \"qwen2:7b\", \"llama3:8b\", \"mistral:7b-instruct\", \"qwen2:72b\"]\n",
        "\n",
        "# Download and start Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama():\n",
        "    try:\n",
        "        subprocess.run(\"ollama serve\", shell=True, check=True, capture_output=True, text=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Ollama server failed: {e.stderr}\")\n",
        "\n",
        "print(\"üöÄ Starting Ollama server in the background...\")\n",
        "ollama_thread = threading.Thread(target=run_ollama)\n",
        "ollama_thread.daemon = True\n",
        "ollama_thread.start()\n",
        "\n",
        "# Wait for the server to be ready\n",
        "print(\"‚è≥ Waiting for Ollama server to initialize...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# Pull the model\n",
        "print(f\"üì¶ Pulling model: {MODEL_NAME}. This may take a while...\")\n",
        "try:\n",
        "    subprocess.run(f\"ollama pull {MODEL_NAME}\", shell=True, check=True, capture_output=True, text=True)\n",
        "    print(f\"‚úÖ Model {MODEL_NAME} is ready.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Failed to pull model. Trying default tag...\")\n",
        "    base_model = MODEL_NAME.split(':')[0]\n",
        "    subprocess.run(f\"ollama pull {base_model}\", shell=True, check=True)\n",
        "    print(f\"‚úÖ Model {base_model} is ready.\")\n",
        "\n",
        "# Verify Ollama is running\n",
        "!ollama list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## DUPLICATED CELL\n",
        "# @markdown This cell was duplicated due to an error in a previous edit.\n",
        "# @markdown It contains an older, broken version of the code.\n",
        "# @markdown Please delete this cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 4. (Optional) Create `platinum_prompts.txt` for Large-Scale Runs\n",
        "# @markdown Run this cell **once** to create the `platinum_prompts.txt` file in your Colab environment.\n",
        "# @markdown You can then modify the cell above (Cell 3) to read prompts from this file instead of the small list.\n",
        "\n",
        "import random\n",
        "\n",
        "# --- Components for Prompt Generation ---\n",
        "STYLES = [\n",
        "    \"Modern\", \"Contemporary\", \"Minimalist\", \"Traditional Kerala-style 'Nalukettu'\",\n",
        "    \"Colonial\", \"Industrial\", \"Scandinavian\", \"Bohemian\", \"Farmhouse\", \"Chettinad-style\",\n",
        "    \"Eco-friendly\", \"Brutalist\", \"Art Deco\", \"Mediterranean\"\n",
        "]\n",
        "SIZES_BHK = [\"1BHK\", \"2BHK\", \"3BHK\", \"4BHK\", \"5BHK\", \"6BHK\", \"studio apartment\"]\n",
        "SIZES_SQFT = [\n",
        "    \"800 sqft\", \"1000 sqft\", \"1200 sqft\", \"1500 sqft\", \"1800 sqft\",\n",
        "    \"2000 sqft\", \"2500 sqft\", \"3000 sqft\", \"4000 sqft\", \"5000 sqft\"\n",
        "]\n",
        "FLOORS = [\n",
        "    \"single-story\", \"two-story\", \"G+1\", \"G+2\", \"duplex\", \"triplex\",\n",
        "    \"split-level\", \"penthouse\"\n",
        "]\n",
        "STRUCTURE_TYPES = [\"house\", \"villa\", \"apartment\", \"bungalow\", \"farmhouse\", \"townhouse\", \"cottage\"]\n",
        "PLOT_SIZES = [\n",
        "    \"30x40 feet\", \"30x50 feet\", \"40x60 feet\", \"50x80 feet\", \"60x90 feet\",\n",
        "    \"80x100 feet\", \"100x100 feet\", \"corner\", \"irregular\"\n",
        "]\n",
        "FEATURES = [\n",
        "    \"with an open-plan kitchen and living area\", \"with a swimming pool\", \"with a home theater\",\n",
        "    \"with a large garden\", \"with a central courtyard\", \"with a dedicated home office\",\n",
        "    \"with a private gym\", \"featuring floor-to-ceiling windows\", \"with a rooftop terrace\",\n",
        "    \"with a two-car garage\", \"with servant's quarters\", \"with a library\", \"with a spacious balcony for each bedroom\"\n",
        "]\n",
        "CONSTRAINTS = [\n",
        "    \"and be Vastu-compliant\", \"with a North-facing entrance\", \"with a West-facing plot\",\n",
        "    \"on a tight budget\", \"for a luxury segment\", \"designed for a family of four\",\n",
        "    \"with a focus on natural light and ventilation\", \"for a joint family\", \"as a bachelor pad\",\n",
        "    \"to be wheelchair accessible\"\n",
        "]\n",
        "\n",
        "def generate_prompt():\n",
        "    prompt_parts = []\n",
        "    style = random.choice(STYLES)\n",
        "    num_floors = random.choice(FLOORS)\n",
        "    bhk = random.choice(SIZES_BHK)\n",
        "    structure = random.choice(STRUCTURE_TYPES)\n",
        "    prompt_parts.append(f\"Design a {style}, {num_floors} {bhk} {structure}\")\n",
        "    if random.random() < 0.7:\n",
        "        plot = random.choice(PLOT_SIZES)\n",
        "        prompt_parts.append(f\"for a {plot} plot\")\n",
        "    else:\n",
        "        area = random.choice(SIZES_SQFT)\n",
        "        prompt_parts.append(f\"with a total area of {area}\")\n",
        "    num_features = random.randint(1, 3)\n",
        "    selected_features = random.sample(FEATURES, num_features)\n",
        "    prompt_parts.extend(selected_features)\n",
        "    if random.random() < 0.6:\n",
        "        num_constraints = random.randint(1, 2)\n",
        "        selected_constraints = random.sample(CONSTRAINTS, num_constraints)\n",
        "        prompt_parts.extend(selected_constraints)\n",
        "    return \". \".join(prompt_parts) + \".\"\n",
        "\n",
        "NUM_PROMPTS = 10000\n",
        "OUTPUT_FILE = \"/content/platinum_prompts.txt\"\n",
        "\n",
        "print(f\"Generating {NUM_PROMPTS} prompts and saving to {OUTPUT_FILE}...\")\n",
        "with open(OUTPUT_FILE, 'w') as f:\n",
        "    for i in range(NUM_PROMPTS):\n",
        "        prompt = generate_prompt()\n",
        "        f.write(f\"{prompt}\\\\n\")\n",
        "print(f\"‚úÖ Successfully generated and saved {NUM_PROMPTS} prompts.\")\n",
        "\n",
        "# Display the first 5 prompts to verify\n",
        "!head -n 5 {OUTPUT_FILE}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 5. (Optional) Download Generated Dataset\n",
        "# @markdown Run this cell after the data generation is complete to compress and download the entire output folder.\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Define the source directory in Google Drive and the target zip file path\n",
        "# This should match the DRIVE_OUTPUT_DIR from Cell 3\n",
        "source_dir = \"/content/drive/MyDrive/housebrain_dataset\"\n",
        "zip_filename = \"housebrain_dataset.zip\"\n",
        "zip_filepath = f\"/content/{zip_filename}\"\n",
        "\n",
        "if os.path.exists(source_dir):\n",
        "    # Create the zip archive\n",
        "    print(f\"Compressing '{source_dir}' into '{zip_filepath}'...\")\n",
        "    shutil.make_archive(zip_filepath.replace('.zip', ''), 'zip', source_dir)\n",
        "    print(\"‚úÖ Compression complete.\")\n",
        "\n",
        "    # Provide a download link\n",
        "    print(f\"\\nDownloading '{zip_filename}'...\")\n",
        "    files.download(zip_filepath)\n",
        "else:\n",
        "    print(f\"ERROR: The source directory '{source_dir}' was not found. Please ensure the Data Factory ran correctly.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 1. Setup Environment\n",
        "# @markdown Mount Google Drive and clone the repository using a secure token.\n",
        "from google.colab import drive\n",
        "import os\n",
        "import getpass\n",
        "import subprocess\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted.\")\n",
        "\n",
        "# --- GitHub Setup ---\n",
        "#@markdown Enter your GitHub Personal Access Token (PAT) with repo access.\n",
        "GITHUB_TOKEN = getpass.getpass('Enter your GitHub PAT: ')\n",
        "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/Vinay-O/HouseBrainLLM.git\"\n",
        "REPO_DIR = \"/content/HouseBrainLLM\"\n",
        "\n",
        "# Clone the repository\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(\"Repository already exists. Pulling latest changes...\")\n",
        "    subprocess.run(f\"cd {REPO_DIR} && git pull\", shell=True, check=True)\n",
        "else:\n",
        "    print(\"Cloning repository...\")\n",
        "    subprocess.run(f\"git clone {REPO_URL} {REPO_DIR}\", shell=True, check=True)\n",
        "\n",
        "print(\"‚úÖ Repository is ready.\")\n",
        "\n",
        "# --- Install Dependencies ---\n",
        "#@markdown Install necessary Python packages.\n",
        "!pip install -q pydantic GitPython\n",
        "\n",
        "print(\"‚úÖ Dependencies installed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 2. Configure and Start Ollama Server\n",
        "# @markdown This cell will download and start the Ollama server, then pull the specified model.\n",
        "# @markdown The process will take a few minutes.\n",
        "\n",
        "MODEL_NAME = \"mixtral:instruct\" # @param [\"mixtral:instruct\", \"qwen2:7b\", \"llama3:8b\", \"mistral:7b-instruct\", \"qwen2:72b\"]\n",
        "\n",
        "# Download and start Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama():\n",
        "    try:\n",
        "        subprocess.run(\"ollama serve\", shell=True, check=True, capture_output=True, text=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Ollama server failed: {e.stderr}\")\n",
        "\n",
        "print(\"üöÄ Starting Ollama server in the background...\")\n",
        "ollama_thread = threading.Thread(target=run_ollama)\n",
        "ollama_thread.daemon = True\n",
        "ollama_thread.start()\n",
        "\n",
        "# Wait for the server to be ready\n",
        "print(\"‚è≥ Waiting for Ollama server to initialize...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# Pull the model\n",
        "print(f\"üì¶ Pulling model: {MODEL_NAME}. This may take a while...\")\n",
        "try:\n",
        "    subprocess.run(f\"ollama pull {MODEL_NAME}\", shell=True, check=True, capture_output=True, text=True)\n",
        "    print(f\"‚úÖ Model {MODEL_NAME} is ready.\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"Failed to pull model. Trying default tag...\")\n",
        "    base_model = MODEL_NAME.split(':')[0]\n",
        "    subprocess.run(f\"ollama pull {base_model}\", shell=True, check=True)\n",
        "    print(f\"‚úÖ Model {base_model} is ready.\")\n",
        "\n",
        "# Verify Ollama is running\n",
        "!ollama list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## DUPLICATED CELL\n",
        "# @markdown This cell was duplicated due to an error in a previous edit.\n",
        "# @markdown It contains an older, broken version of the code.\n",
        "# @markdown Please delete this cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 4. (Optional) Create `platinum_prompts.txt` for Large-Scale Runs\n",
        "# @markdown Run this cell **once** to create the `platinum_prompts.txt` file in your Colab environment.\n",
        "# @markdown You can then modify the cell above (Cell 3) to read prompts from this file instead of the small list.\n",
        "\n",
        "import random\n",
        "\n",
        "# --- Components for Prompt Generation ---\n",
        "STYLES = [\n",
        "    \"Modern\", \"Contemporary\", \"Minimalist\", \"Traditional Kerala-style 'Nalukettu'\",\n",
        "    \"Colonial\", \"Industrial\", \"Scandinavian\", \"Bohemian\", \"Farmhouse\", \"Chettinad-style\",\n",
        "    \"Eco-friendly\", \"Brutalist\", \"Art Deco\", \"Mediterranean\"\n",
        "]\n",
        "SIZES_BHK = [\"1BHK\", \"2BHK\", \"3BHK\", \"4BHK\", \"5BHK\", \"6BHK\", \"studio apartment\"]\n",
        "SIZES_SQFT = [\n",
        "    \"800 sqft\", \"1000 sqft\", \"1200 sqft\", \"1500 sqft\", \"1800 sqft\",\n",
        "    \"2000 sqft\", \"2500 sqft\", \"3000 sqft\", \"4000 sqft\", \"5000 sqft\"\n",
        "]\n",
        "FLOORS = [\n",
        "    \"single-story\", \"two-story\", \"G+1\", \"G+2\", \"duplex\", \"triplex\",\n",
        "    \"split-level\", \"penthouse\"\n",
        "]\n",
        "STRUCTURE_TYPES = [\"house\", \"villa\", \"apartment\", \"bungalow\", \"farmhouse\", \"townhouse\", \"cottage\"]\n",
        "PLOT_SIZES = [\n",
        "    \"30x40 feet\", \"30x50 feet\", \"40x60 feet\", \"50x80 feet\", \"60x90 feet\",\n",
        "    \"80x100 feet\", \"100x100 feet\", \"corner\", \"irregular\"\n",
        "]\n",
        "FEATURES = [\n",
        "    \"with an open-plan kitchen and living area\", \"with a swimming pool\", \"with a home theater\",\n",
        "    \"with a large garden\", \"with a central courtyard\", \"with a dedicated home office\",\n",
        "    \"with a private gym\", \"featuring floor-to-ceiling windows\", \"with a rooftop terrace\",\n",
        "    \"with a two-car garage\", \"with servant's quarters\", \"with a library\", \"with a spacious balcony for each bedroom\"\n",
        "]\n",
        "CONSTRAINTS = [\n",
        "    \"and be Vastu-compliant\", \"with a North-facing entrance\", \"with a West-facing plot\",\n",
        "    \"on a tight budget\", \"for a luxury segment\", \"designed for a family of four\",\n",
        "    \"with a focus on natural light and ventilation\", \"for a joint family\", \"as a bachelor pad\",\n",
        "    \"to be wheelchair accessible\"\n",
        "]\n",
        "\n",
        "def generate_prompt():\n",
        "    prompt_parts = []\n",
        "    style = random.choice(STYLES)\n",
        "    num_floors = random.choice(FLOORS)\n",
        "    bhk = random.choice(SIZES_BHK)\n",
        "    structure = random.choice(STRUCTURE_TYPES)\n",
        "    prompt_parts.append(f\"Design a {style}, {num_floors} {bhk} {structure}\")\n",
        "    if random.random() < 0.7:\n",
        "        plot = random.choice(PLOT_SIZES)\n",
        "        prompt_parts.append(f\"for a {plot} plot\")\n",
        "    else:\n",
        "        area = random.choice(SIZES_SQFT)\n",
        "        prompt_parts.append(f\"with a total area of {area}\")\n",
        "    num_features = random.randint(1, 3)\n",
        "    selected_features = random.sample(FEATURES, num_features)\n",
        "    prompt_parts.extend(selected_features)\n",
        "    if random.random() < 0.6:\n",
        "        num_constraints = random.randint(1, 2)\n",
        "        selected_constraints = random.sample(CONSTRAINTS, num_constraints)\n",
        "        prompt_parts.extend(selected_constraints)\n",
        "    return \". \".join(prompt_parts) + \".\"\n",
        "\n",
        "NUM_PROMPTS = 10000\n",
        "OUTPUT_FILE = \"/content/platinum_prompts.txt\"\n",
        "\n",
        "print(f\"Generating {NUM_PROMPTS} prompts and saving to {OUTPUT_FILE}...\")\n",
        "with open(OUTPUT_FILE, 'w') as f:\n",
        "    for i in range(NUM_PROMPTS):\n",
        "        prompt = generate_prompt()\n",
        "        f.write(f\"{prompt}\\\\n\")\n",
        "print(f\"‚úÖ Successfully generated and saved {NUM_PROMPTS} prompts.\")\n",
        "\n",
        "# Display the first 5 prompts to verify\n",
        "!head -n 5 {OUTPUT_FILE}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 5. (Optional) Download Generated Dataset\n",
        "# @markdown Run this cell after the data generation is complete to compress and download the entire output folder.\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Define the source directory in Google Drive and the target zip file path\n",
        "# This should match the DRIVE_OUTPUT_DIR from Cell 3\n",
        "source_dir = \"/content/drive/MyDrive/housebrain_dataset\"\n",
        "zip_filename = \"housebrain_dataset.zip\"\n",
        "zip_filepath = f\"/content/{zip_filename}\"\n",
        "\n",
        "if os.path.exists(source_dir):\n",
        "    # Create the zip archive\n",
        "    print(f\"Compressing '{source_dir}' into '{zip_filepath}'...\")\n",
        "    shutil.make_archive(zip_filepath.replace('.zip', ''), 'zip', source_dir)\n",
        "    print(\"‚úÖ Compression complete.\")\n",
        "\n",
        "    # Provide a download link\n",
        "    print(f\"\\nDownloading '{zip_filename}'...\")\n",
        "    files.download(zip_filepath)\n",
        "else:\n",
        "    print(f\"ERROR: The source directory '{source_dir}' was not found. Please ensure the Data Factory ran correctly.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 4. (One-Time Setup) Generate Master Prompt File\n",
        "# @markdown This cell uses the `generate_prompts.py` script to create your master prompt file in Google Drive.\n",
        "# @markdown **You only need to run this cell once.**\n",
        "# @markdown Once the file is created, Cell 3 will be able to read from it for all future runs.\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Configuration ---\n",
        "#@markdown The desired location in your Google Drive for the master prompt file. This MUST match the path in Cell 3.\n",
        "DRIVE_PROMPT_FILE = \"/content/drive/MyDrive/housebrain_prompts/platinum_prompts.txt\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown The total number of prompts to generate for your master list.\n",
        "NUM_PROMPTS_TO_GENERATE = 30000 #@param {type:\"integer\"}\n",
        "# --- End Configuration ---\n",
        "\n",
        "# --- Execution ---\n",
        "REPO_DIR = \"/content/HouseBrainLLM\"\n",
        "script_path = os.path.join(REPO_DIR, \"scripts/generate_prompts.py\")\n",
        "\n",
        "# Ensure the repository is in the correct directory\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# Ensure the target directory in Drive exists\n",
        "Path(DRIVE_PROMPT_FILE).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Running prompt generation script to create {NUM_PROMPTS_TO_GENERATE} prompts...\")\n",
        "# Use an f-string for safer command construction\n",
        "command = f'python3 \"{script_path}\" --num-prompts {NUM_PROMPTS_TO_GENERATE} --output-file \"{DRIVE_PROMPT_FILE}\"'\n",
        "!{command}\n",
        "\n",
        "print(\"\\n--- Verification ---\")\n",
        "if Path(DRIVE_PROMPT_FILE).exists():\n",
        "    print(f\"‚úÖ Master prompt file successfully created at: {DRIVE_PROMPT_FILE}\")\n",
        "    print(\"First 5 prompts in the file:\")\n",
        "    !head -n 5 \"{DRIVE_PROMPT_FILE}\"\n",
        "else:\n",
        "    print(f\"‚ùå ERROR: Master prompt file was not created. Please check for errors above.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
