{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 1. Setup Environment (Corrected)\n",
        "# @markdown Mount Google Drive, clone/pull the repo, install dependencies, and set the system path.\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "import getpass\n",
        "import subprocess\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"‚úÖ Google Drive mounted.\")\n",
        "\n",
        "# --- GitHub Setup ---\n",
        "#@markdown Enter your GitHub Personal Access Token (PAT) with repo access.\n",
        "GITHUB_TOKEN = getpass.getpass('Enter your GitHub PAT: ')\n",
        "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/Vinay-O/HouseBrainLLM.git\"\n",
        "REPO_DIR = \"/content/HouseBrainLLM\"\n",
        "\n",
        "# Clone or pull the repository\n",
        "if os.path.exists(REPO_DIR):\n",
        "    print(\"Repository already exists. Pulling latest changes...\")\n",
        "    subprocess.run(f\"cd {REPO_DIR} && git pull\", shell=True, check=True)\n",
        "else:\n",
        "    print(\"Cloning repository...\")\n",
        "    subprocess.run(f\"git clone {REPO_URL} {REPO_DIR}\", shell=True, check=True)\n",
        "print(\"‚úÖ Repository is ready.\")\n",
        "\n",
        "# CRITICAL FIX: Add repository to Python's system path\n",
        "if REPO_DIR not in sys.path:\n",
        "    sys.path.append(REPO_DIR)\n",
        "    print(f\"‚úÖ Added {REPO_DIR} to system path.\")\n",
        "\n",
        "# --- Install Dependencies ---\n",
        "requirements_path = os.path.join(REPO_DIR, \"requirements.txt\")\n",
        "print(\"Installing dependencies...\")\n",
        "!pip install -q -r {requirements_path}\n",
        "!pip install -q ollama\n",
        "print(\"‚úÖ Dependencies installed.\")\n",
        "\n",
        "print(\"‚úÖ Environment setup complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 2. Configure and Start Ollama Server (Corrected)\n",
        "# @markdown This cell stops any old server, starts a new one, and pulls the specified model.\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# @markdown Select the model to use for generation.\n",
        "model_name_widget = widgets.Dropdown(\n",
        "    options=[\"deepseek-r1:32b\", \"llama3:70b-instruct\", \"qwen2:72b-instruct\", \"mixtral:instruct\"],\n",
        "    value='deepseek-r1:32b',\n",
        "    description='Model:',\n",
        "    disabled=False,\n",
        ")\n",
        "display(model_name_widget)\n",
        "\n",
        "# Use a button to trigger the setup\n",
        "setup_button = widgets.Button(description=\"Start Server & Pull Model\")\n",
        "display(setup_button)\n",
        "\n",
        "output_area = widgets.Output()\n",
        "display(output_area)\n",
        "\n",
        "def setup_ollama_server(b):\n",
        "    with output_area:\n",
        "        output_area.clear_output()\n",
        "        MODEL_NAME = model_name_widget.value\n",
        "\n",
        "        # CRITICAL FIX: Stop any existing Ollama processes to prevent errors\n",
        "        print(\"üõë Stopping any old Ollama server...\")\n",
        "        subprocess.run(\"pkill -f 'ollama serve'\", shell=True)\n",
        "        time.sleep(3) # Give it a moment to shut down\n",
        "\n",
        "        # Start Ollama serve in a background thread\n",
        "        def run_ollama():\n",
        "            try:\n",
        "                # Using subprocess.run to wait until the command completes or fails.\n",
        "                # The capture_output=True will hold stdout/stderr.\n",
        "                proc = subprocess.run(\"ollama serve\", shell=True, check=True, capture_output=True, text=True)\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(f\"Ollama server failed: {e.stderr}\")\n",
        "\n",
        "        print(\"üöÄ Starting new Ollama server in the background...\")\n",
        "        ollama_thread = threading.Thread(target=run_ollama)\n",
        "        ollama_thread.daemon = True\n",
        "        ollama_thread.start()\n",
        "        print(\"‚è≥ Waiting for Ollama server to initialize (this may take ~20 seconds)...\")\n",
        "        time.sleep(20)\n",
        "\n",
        "        # Pull the model\n",
        "        print(f\"üì¶ Pulling model: {MODEL_NAME}. This may take a while...\")\n",
        "        try:\n",
        "            # Using Popen for real-time output\n",
        "            process = subprocess.Popen(f\"ollama pull {MODEL_NAME}\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "            for line in iter(process.stdout.readline, ''):\n",
        "                print(line, end='')\n",
        "            process.wait(timeout=900)\n",
        "            if process.returncode == 0:\n",
        "                 print(f\"‚úÖ Model {MODEL_NAME} is ready.\")\n",
        "            else:\n",
        "                 print(f\"‚ùå Error pulling model.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error pulling model: {e}\")\n",
        "\n",
        "        # Make the ollama library available globally\n",
        "        global ollama\n",
        "        import ollama\n",
        "        print(\"‚úÖ Ollama server is running and the library is imported.\")\n",
        "        !ollama list\n",
        "\n",
        "setup_button.on_click(setup_ollama_server)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 3. Run the Data Factory (V4.3 - Self-Healing Pipeline)\n",
        "# @markdown This cell runs the main data generation loop. It depends on Cell 1 and Cell 2 being run successfully first.\n",
        "\n",
        "import json\n",
        "import uuid\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "from pydantic import ValidationError\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output, display, HTML\n",
        "\n",
        "# --- Configuration ---\n",
        "DATASET_PATH = \"/content/drive/MyDrive/housebrain_platinum_dataset\"\n",
        "MASTER_PROMPT_LIST_PATH = \"/content/drive/MyDrive/housebrain_prompts/platinum_prompts.txt\"\n",
        "NUM_PROMPTS_TO_GENERATE = 10 # @param {type:\"integer\"}\n",
        "\n",
        "# --- Prerequisite Checks ---\n",
        "SCHEMA_LOADED = False\n",
        "SERVER_RUNNING = False\n",
        "\n",
        "try:\n",
        "    from src.housebrain.schema import HouseOutput, HouseInput, BasicDetails, RoomType\n",
        "    SCHEMA_LOADED = True\n",
        "    print(\"‚úÖ Schema loaded successfully.\")\n",
        "except (ImportError, ModuleNotFoundError) as e:\n",
        "    print(f\"‚ùå Could not import HouseBrain schema: {e}\")\n",
        "    print(\"   Please re-run Cell 1 to ensure the repository is cloned and the path is set.\")\n",
        "\n",
        "# Check if the ollama library and model widget from Cell 2 are available\n",
        "try:\n",
        "    if ollama and model_name_widget:\n",
        "        SERVER_RUNNING = True\n",
        "        MODEL_NAME = model_name_widget.value\n",
        "        client = ollama.Client()\n",
        "        print(f\"‚úÖ Ollama client connected. Using model: {MODEL_NAME}\")\n",
        "except NameError:\n",
        "    print(\"‚ùå Ollama server not ready. Please run Cell 2 successfully before this cell.\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def call_ollama_colab(model, prompt, retries=3, delay=5):\n",
        "    \"\"\"Calls the Ollama server using the client initialised from Cell 2.\"\"\"\n",
        "    if not SERVER_RUNNING: return None\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = client.chat(model=model, messages=[{'role': 'user', 'content': prompt}])\n",
        "            return response['message']['content']\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Attempt {attempt + 1}/{retries} failed. Retrying in {delay}s...\")\n",
        "            print(f\"DETAILS: {e}\")\n",
        "            time.sleep(delay)\n",
        "    print(f\"‚ùå All {retries} attempts to call the Ollama model failed.\")\n",
        "    return None\n",
        "\n",
        "def repair_json(text, target_type):\n",
        "    \"\"\"Aggressively extracts and parses a JSON object or list from a string.\"\"\"\n",
        "    print(\" -> Running Stage 2: Aggressive JSON Repair...\")\n",
        "    if not text: return None\n",
        "    if target_type == 'dict': match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
        "    elif target_type == 'list': match = re.search(r'\\[.*\\]', text, re.DOTALL)\n",
        "    else: return None\n",
        "\n",
        "    if not match:\n",
        "        print(\"    ‚ùå JSON Repair Failed: No JSON object/list found in the output.\")\n",
        "        return None\n",
        "    json_str = match.group(0)\n",
        "    try:\n",
        "        parsed_json = json.loads(json_str)\n",
        "        print(\"    ‚úÖ JSON Repair Successful.\")\n",
        "        return parsed_json\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"    ‚ùå JSON Repair Failed: Could not decode the extracted JSON. Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def heal_and_convert_plan(raw_plan: dict):\n",
        "    \"\"\"Takes a raw dictionary from the LLM and deterministically fixes common schema deviations.\"\"\"\n",
        "    print(\" -> Running Stage 3: Applying Schema Healer...\")\n",
        "    healed_plan = raw_plan.copy()\n",
        "    total_area = 0\n",
        "    if not isinstance(healed_plan.get('levels'), list): healed_plan['levels'] = []\n",
        "    for level in healed_plan.get('levels', []):\n",
        "        if not isinstance(level.get('rooms'), list): level['rooms'] = []\n",
        "        for room in level.get('rooms', []):\n",
        "            raw_type = room.get('name', room.get('room_type', '')).lower().replace('_', ' ')\n",
        "            if 'living' in raw_type: room['room_type'] = 'living_room'\n",
        "            elif 'dining' in raw_type: room['room_type'] = 'dining_room'\n",
        "            elif 'kitchen' in raw_type: room['room_type'] = 'kitchen'\n",
        "            elif 'bed' in raw_type: room['room_type'] = 'bedroom'\n",
        "            elif 'bath' in raw_type: room['room_type'] = 'bathroom'\n",
        "            elif 'balcony' in raw_type: room['room_type'] = 'balcony'\n",
        "            elif 'garage' in raw_type: room['room_type'] = 'garage'\n",
        "            elif 'stor' in raw_type: room['room_type'] = 'storage'\n",
        "            elif 'study' in raw_type or 'office' in raw_type: room['room_type'] = 'study'\n",
        "            elif 'utility' in raw_type: room['room_type'] = 'utility'\n",
        "            elif 'entrance' in raw_type or 'foyer' in raw_type: room['room_type'] = 'entrance'\n",
        "            elif 'hallway' in raw_type or 'corridor' in raw_type: room['room_type'] = 'hallway'\n",
        "            elif 'patio' in raw_type: room['room_type'] = 'patio'\n",
        "            elif 'laundry' in raw_type: room['room_type'] = 'laundry'\n",
        "            bounds = room.get('bounds', {})\n",
        "            if all(k in bounds for k in ['x1', 'y1', 'x2', 'y2']):\n",
        "                bounds['x'] = bounds.get('x1', 0)\n",
        "                bounds['y'] = bounds.get('y1', 0)\n",
        "                bounds['width'] = abs(bounds.get('x2', 0) - bounds.get('x1', 0))\n",
        "                bounds['height'] = abs(bounds.get('y2', 0) - bounds.get('y1', 0))\n",
        "                for k in ['x1', 'y1', 'x2', 'y2']: bounds.pop(k, None)\n",
        "            if all(k in bounds for k in ['width', 'height']):\n",
        "                total_area += bounds['width'] * bounds['height']\n",
        "    healed_plan['total_area'] = total_area\n",
        "    print(\"    ‚úÖ Schema Healer finished.\")\n",
        "    return healed_plan\n",
        "\n",
        "# --- Master Prompt Template (V4.3 - Safe Formatting) ---\n",
        "if SCHEMA_LOADED:\n",
        "    ROOM_TYPES_LIST_STR = ', '.join([f'\"{e.value}\"' for e in RoomType])\n",
        "    PROMPT_HEADER = f\"\"\"You are an expert architectural AI. Your task is to generate a complete, valid, and detailed house plan in JSON format based on a user's request. You MUST adhere strictly to the provided JSON schema. Do NOT add any extra fields or deviate from the specified structure. The output MUST be a single, raw JSON object, without any surrounding text, explanations, or markdown formatting.\n",
        "\n",
        "**JSON Schema:**\n",
        "- `room_type`: Must be one of the following: {ROOM_TYPES_LIST_STR}.\n",
        "- `Rectangle` (for bounds): {{\"x\": float, \"y\": float, \"width\": float, \"height\": float}}.\n",
        "\n",
        "**Constraint Checklist & Rules:**\n",
        "1.  **Output Raw JSON ONLY:** Start with `{{` and end with `}}`.\n",
        "2.  **Strict Schema Adherence:** Every field in the schema MUST be present.\n",
        "3.  **Valid `room_type`:** Use ONLY the provided enum values.\n",
        "4.  **No Overlapping Rooms:** Ensure `bounds` of rooms on the same level do not overlap.\n",
        "5.  **Doors Connect Rooms:** Each `Door` must have a valid `room1` and `room2` ID.\n",
        "6.  **Realistic Dimensions:** Room sizes must be practical.\n",
        "\n",
        "**User Request:**\n",
        "\"\"\"\n",
        "    PROMPT_FOOTER = \"\\n\\n**Your JSON Output:**\"\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if SCHEMA_LOADED and SERVER_RUNNING:\n",
        "    !mkdir -p \"{DATASET_PATH}\"\n",
        "    print(\"\\n--- Starting Data Factory Run (V4.3 - Self-Healing Pipeline) ---\")\n",
        "    try:\n",
        "        with open(MASTER_PROMPT_LIST_PATH, 'r') as f: all_prompts = f.readlines()\n",
        "        print(f\"‚úÖ Found {len(all_prompts)} total prompts.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå ERROR: Master prompt file not found at {MASTER_PROMPT_LIST_PATH}\")\n",
        "        all_prompts = []\n",
        "\n",
        "    if all_prompts:\n",
        "        prompts_to_process = random.sample(all_prompts, min(NUM_PROMPTS_TO_GENERATE, len(all_prompts)))\n",
        "        print(f\"‚úÖ This run will process a random batch of {len(prompts_to_process)} prompts.\")\n",
        "        for i, prompt_text in enumerate(prompts_to_process):\n",
        "            prompt_text = prompt_text.strip()\n",
        "            if not prompt_text: continue\n",
        "            print(f\"\\n================== PROMPT {i+1}/{len(prompts_to_process)} ==================\")\n",
        "            print(prompt_text[:100] + \"...\")\n",
        "            print(\"--------------------------------------------------\")\n",
        "\n",
        "            print(\" -> Running Stage 1: Single-Shot Generation...\")\n",
        "            final_prompt = PROMPT_HEADER + prompt_text + PROMPT_FOOTER\n",
        "            llm_output = call_ollama_colab(MODEL_NAME, final_prompt)\n",
        "\n",
        "            if not llm_output:\n",
        "                print(\"    ‚ùå Stage 1 Failed: No response from model.\")\n",
        "                continue\n",
        "\n",
        "            print(\" -> Running Stage 4: Validation and Save...\")\n",
        "            try:\n",
        "                raw_data = repair_json(llm_output, 'dict')\n",
        "                if not raw_data: raise ValueError(\"JSON repair failed.\")\n",
        "                healed_data = heal_and_convert_plan(raw_data)\n",
        "                plan_id = str(uuid.uuid4())\n",
        "                healed_data['id'] = plan_id\n",
        "                healed_data['input'] = HouseInput(basicDetails=BasicDetails(prompt=prompt_text)).model_dump()\n",
        "                validated_plan = HouseOutput.model_validate(healed_data)\n",
        "                file_path = f\"{DATASET_PATH}/plan_{plan_id.replace('-', '_')}.json\"\n",
        "                with open(file_path, 'w') as f:\n",
        "                    f.write(validated_plan.model_dump_json(indent=2))\n",
        "                print(f\"‚úÖ SUCCESS! Saved validated plan to {file_path}\")\n",
        "            except ValidationError as e:\n",
        "                print(f\"    ‚ùå Stage 4 Failed: Pydantic validation error - {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"    ‚ùå Stage 4 Failed: An unexpected error occurred - {e}\")\n",
        "    print(\"\\nüéâ Data Factory run complete!\")\n",
        "else:\n",
        "    print(\"\\nHALTING: Prerequisites not met. Please run Cell 1 and Cell 2 successfully before running this cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 4. (One-Time Setup) Generate Master Prompt File\n",
        "# @markdown This cell generates a large list of diverse prompts and saves it to your Google Drive.\n",
        "# @markdown You only need to run this once.\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "PROMPT_OUTPUT_DIR = \"/content/drive/MyDrive/housebrain_prompts\"\n",
        "PROMPT_OUTPUT_FILE = os.path.join(PROMPT_OUTPUT_DIR, \"platinum_prompts.txt\")\n",
        "NUM_PROMPTS_TO_GENERATE = 40000 # @param {type:\"integer\"}\n",
        "\n",
        "# --- Prompt Components ---\n",
        "styles = [\"Modern\", \"Traditional\", \"Scandinavian\", \"Industrial\", \"Minimalist\", \"Bohemian\", \"Farmhouse\", \"Art Deco\", \"Brutalist\", \"Colonial\", \"Mediterranean\", \"Japanese Zen\", \"Traditional Kerala-style 'Nalukettu'\"]\n",
        "structures = [\"house\", \"bungalow\", \"villa\", \"apartment\", \"cottage\", \"townhouse\", \"farmhouse\", \"penthouse\"]\n",
        "stories = [\"single-story\", \"two-story\", \"G+1\", \"G+2\", \"split-level\", \"duplex\", \"triplex\"]\n",
        "bhk_options = [\"studio apartment\", \"1BHK\", \"2BHK\", \"3BHK\", \"4BHK\", \"5BHK\", \"6BHK\"]\n",
        "plot_sizes = [\"30x40 feet plot\", \"50x80 feet plot\", \"80x100 feet plot\", \"100x100 feet plot\", \"irregular plot\"]\n",
        "total_areas = [\"1000 sqft\", \"1200 sqft\", \"1800 sqft\", \"3000 sqft\", \"4000 sqft\", \"5000 sqft\"]\n",
        "features = [\"with a library\", \"with a home office\", \"with a private gym\", \"with a large garden\", \"with a rooftop terrace\", \"with a two-car garage\", \"with servant's quarters\", \"with a swimming pool\"]\n",
        "misc = [\"for a joint family\", \"for a nuclear family\", \"with a North-facing entrance\", \"with a West-facing entrance\", \"featuring floor-to-ceiling windows\", \"with an open-plan kitchen\"]\n",
        "\n",
        "# --- Generation Logic ---\n",
        "!mkdir -p \"{PROMPT_OUTPUT_DIR}\"\n",
        "\n",
        "all_options = [styles, structures, stories, bhk_options, plot_sizes, total_areas, features, misc]\n",
        "combinations = list(itertools.product(*all_options))\n",
        "random.shuffle(combinations)\n",
        "\n",
        "prompts_generated = 0\n",
        "with open(PROMPT_OUTPUT_FILE, \"w\") as f:\n",
        "    for combo in combinations:\n",
        "        # Construct a more natural-sounding prompt\n",
        "        prompt = f\"Design a {combo[0]}, {combo[1]} {combo[2]} {combo[3]}. \"\n",
        "        if \"plot\" in combo[4]:\n",
        "            prompt += f\"for a {combo[4]}. \"\n",
        "        else:\n",
        "            prompt += f\"with a total area of {combo[5]}. \"\n",
        "        prompt += f\"{combo[6]}. {combo[7]}.\"\n",
        "        f.write(prompt + \"\\n\")\n",
        "        prompts_generated += 1\n",
        "        if prompts_generated >= NUM_PROMPTS_TO_GENERATE:\n",
        "            break\n",
        "\n",
        "print(f\"‚úÖ Successfully generated and saved {prompts_generated} prompts to {PROMPT_OUTPUT_FILE}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title ## 5. (Optional) Download Generated Dataset\n",
        "# @markdown Zip the entire generated dataset directory and download it to your local machine.\n",
        "from google.colab import files\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# --- Configuration ---\n",
        "# This path should match the DATASET_PATH in Cell 3\n",
        "DATASET_DIR_TO_ZIP = \"/content/drive/MyDrive/housebrain_platinum_dataset\"\n",
        "\n",
        "# --- Zipping Logic ---\n",
        "if os.path.exists(DATASET_DIR_TO_ZIP):\n",
        "    print(f\"Zipping directory: {DATASET_DIR_TO_ZIP}...\")\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    zip_filename = f\"housebrain_dataset_{timestamp}.zip\"\n",
        "    \n",
        "    # The -r flag zips recursively, and -j junks paths to store files at the top level of the zip.\n",
        "    !zip -r -j {zip_filename} \"{DATASET_DIR_TO_ZIP}\"\n",
        "    \n",
        "    print(f\"‚úÖ Created {zip_filename}. Offering for download...\")\n",
        "    files.download(zip_filename)\n",
        "else:\n",
        "    print(f\"‚ùå Directory not found: {DATASET_DIR_TO_ZIP}\")\n",
        "    print(\"   Please run the data factory in Cell 3 to generate some data first.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
