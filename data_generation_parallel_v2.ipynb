{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. Install and Start Ollama (High-Concurrency Mode) ---\n",
        "# This is the most critical step. We must install and configure the Ollama service.\n",
        "# The '%%bash' magic runs the entire cell as a shell script.\n",
        "%%bash\n",
        "# 1. Install Ollama\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# 2. CONFIGURE FOR PARALLELISM: Set the number of parallel requests Ollama will handle.\n",
        "# This is the key to maximizing A100 GPU utilization.\n",
        "export OLLAMA_NUM_PARALLEL=20\n",
        "\n",
        "# 3. Start the server in the background and log its output for debugging\n",
        "ollama serve > ollama_server.log 2>&1 &\n",
        "\n",
        "# 4. Wait 10 seconds to ensure the server has time to fully initialize\n",
        "sleep 10\n",
        "\n",
        "# 5. Verify that the server process is running by checking for the 'ollama' process\n",
        "if pgrep -x \"ollama\" > /dev/null\n",
        "then\n",
        "    echo \"‚úÖ Ollama server is running successfully in high-concurrency mode.\"\n",
        "else\n",
        "    echo \"‚ùå Ollama server failed to start. Please check the log below:\"\n",
        "    cat ollama_server.log\n",
        "fi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2. Install Python Client ---\n",
        "!pip install -q ollama ipywidgets pandas tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3. Mount Drive & Load Prompts ---\n",
        "from google.colab import drive\n",
        "import os\n",
        "import random\n",
        "drive.mount('/content/drive')\n",
        "PROMPT_FILE_PATH = \"/content/drive/MyDrive/housebrain_prompts/platinum_prompts.txt\" \n",
        "def load_prompts_from_file(filepath):\n",
        "    print(f\"\\\\nLoading prompts from {filepath}...\")\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"‚ùå ERROR: Prompt file not found. Please check the path.\")\n",
        "        return []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        prompts = [line.strip() for line in f if line.strip()]\n",
        "    print(f\"‚úÖ Successfully loaded {len(prompts)} prompts.\")\n",
        "    return prompts\n",
        "ALL_PROMPTS = load_prompts_from_file(PROMPT_FILE_PATH)\n",
        "if ALL_PROMPTS:\n",
        "    random.shuffle(ALL_PROMPTS)\n",
        "    print(\"‚úÖ Prompts have been successfully shuffled for this run.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 4. Configure Run & Mount Google Drive ---\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# --- A. Mount Google Drive ---\n",
        "# This is now a critical step. All data will be saved directly to your Drive.\n",
        "print(\"‚ñ∂Ô∏è Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted successfully.\")\n",
        "\n",
        "# --- B. Select Model ---\n",
        "model_options = [\n",
        "    \"gpt-oss:120b\", \"gpt-oss:20b\",\n",
        "    \"magistral:24b\", \"magistral:12b\", \"magistral:8b\",\n",
        "    \"phi4-reasoning:latest\", \"phi4-reasoning:plus\", \"phi4-reasoning:14b\",\n",
        "    \"phi4-reasoning:14b-plus-q4_K_M\", \"phi3:instruct\", \"llama3:instruct\",\n",
        "    \"llama3.1:8b\", \"gemma3:27b\",\"qwen3:14b\",\"qwen2.5:72b\",\"qwen3:32b\",\"qwen2.5:32b\"\n",
        "]\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=model_options, value='phi4-reasoning:latest', description='Select Model:',\n",
        "    disabled=False, style={'description_width': 'initial'}\n",
        ")\n",
        "display(model_dropdown)\n",
        "\n",
        "# --- C. Configure Paths and Counts ---\n",
        "# All output will be saved directly into your Google Drive to prevent data loss.\n",
        "BASE_OUTPUT_DIR = \"/content/drive/MyDrive/HouseBrain/generated_data\" # SAVING TO DRIVE!\n",
        "DATASET_TIER = \"gold_tier\"\n",
        "NUM_PLANS_TO_GENERATE = 3500 # The script will generate *up to* this many new plans.\n",
        "\n",
        "# Create the base directory on Google Drive if it doesn't exist\n",
        "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"‚úîÔ∏è Base output directory is set to: {BASE_OUTPUT_DIR}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 5. Pull & Verify Model ---\n",
        "selected_model = model_dropdown.value\n",
        "print(f\"--- Preparing Model: {selected_model} ---\")\n",
        "print(f\"Attempting to download via Ollama... (This may take several minutes)\")\n",
        "!ollama pull {selected_model}\n",
        "print(f\"\\\\n--- Verifying {selected_model} Installation ---\")\n",
        "!ollama list\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Setup Complete!\")\n",
        "print(\"You are ready to run the data generation in the next cell.\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 6. Run Data Generation (Resilient & Parallelized) ---\n",
        "import ollama\n",
        "import json\n",
        "import hashlib\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import concurrent.futures\n",
        "import os\n",
        "\n",
        "# --- A. Set Parallelism Level ---\n",
        "# We still use a high number of workers to keep the GPU busy.\n",
        "MAX_WORKERS = 20\n",
        "\n",
        "# --- B. Generation Function (Now with Auto-Resume Logic) ---\n",
        "def generate_and_save_raw_plan(args):\n",
        "    \"\"\"\n",
        "    Generates a plan, but FIRST checks if it already exists on Google Drive.\n",
        "    Saves the output directly to Google Drive to prevent data loss.\n",
        "    \"\"\"\n",
        "    task_id, prompt_content, model_name, base_dir, tier = args\n",
        "    prompt_hash = hashlib.sha1(prompt_content.encode()).hexdigest()[:10]\n",
        "    prompt_id = f\"prompt_{task_id:05d}_{prompt_hash}\"\n",
        "\n",
        "    # Construct the final, persistent output path on Google Drive\n",
        "    output_dir_for_prompt = os.path.join(base_dir, tier, model_name, prompt_id)\n",
        "    final_output_path = os.path.join(output_dir_for_prompt, \"raw_output.json\")\n",
        "\n",
        "    # --- RESILIENCE CHECK ---\n",
        "    # If the final output file already exists, we skip this task entirely.\n",
        "    if os.path.exists(final_output_path):\n",
        "        # This is not an error, it's by design. We use tqdm.write for thread-safe logging.\n",
        "        tqdm.write(f\"üü¢ [Task {task_id:05d}] SKIPPING (Already exists): {prompt_id}\")\n",
        "        return (prompt_id, True, \"Skipped\")\n",
        "\n",
        "    # If not skipped, proceed with generation\n",
        "    tqdm.write(f\"‚ö™ [Task {task_id:05d}] STARTING: {prompt_id}\")\n",
        "    os.makedirs(output_dir_for_prompt, exist_ok=True, mode=0o777) # Create dir just-in-time\n",
        "\n",
        "    try:\n",
        "        structured_prompt = f\"\"\"\n",
        "        Please act as an expert architect specializing in Indian residential and commercial design. Your task is to generate a detailed JSON representation of a floor plan based on the following request, keeping local building norms and Vastu principles in mind where appropriate.\n",
        "        **Architectural Request:** \"{prompt_content}\"\n",
        "        **Instructions:** Provide ONLY the JSON output.\n",
        "        **JSON Schema:** {{ \"levels\": [ {{ \"level_id\": \"ground_floor\", \"rooms\": [], \"openings\": [] }} ] }}\n",
        "        Now, begin.\n",
        "        \"\"\"\n",
        "        response = ollama.chat(\n",
        "            model=model_name,\n",
        "            messages=[{'role': 'user', 'content': structured_prompt}],\n",
        "            format='json'\n",
        "        )\n",
        "        raw_output = response['message']['content']\n",
        "\n",
        "        # --- PERSISTENT SAVE ---\n",
        "        # Save the output directly to Google Drive IMMEDIATELY.\n",
        "        with open(final_output_path, 'w') as f:\n",
        "            # The raw output from the model is already a JSON string\n",
        "            f.write(raw_output)\n",
        "\n",
        "        tqdm.write(f\"‚úÖ [Task {task_id:05d}] COMPLETED & SAVED: {prompt_id}\")\n",
        "        return (prompt_id, True, final_output_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred in task {task_id}: {str(e)}\"\n",
        "        error_log_path = os.path.join(output_dir_for_prompt, \"error.log\")\n",
        "        with open(error_log_path, 'w') as f:\n",
        "            f.write(error_message)\n",
        "        tqdm.write(f\"‚ùå [Task {task_id:05d}] FAILED. Log saved to {error_log_path}\")\n",
        "        return (prompt_id, False, error_message)\n",
        "\n",
        "# --- C. Main Generation Loop (Now with Pre-computation) ---\n",
        "if 'ALL_PROMPTS' in locals() and ALL_PROMPTS:\n",
        "    # --- Pre-computation: Find which prompts actually need to be generated ---\n",
        "    print(\"\\\\n--- Checking for existing data in Google Drive to prevent re-work ---\")\n",
        "    \n",
        "    # We will build a list of only the tasks that need to be run.\n",
        "    tasks_to_run = []\n",
        "    total_prompts_to_consider = min(NUM_PLANS_TO_GENERATE, len(ALL_PROMPTS))\n",
        "\n",
        "    for i in range(total_prompts_to_consider):\n",
        "        prompt_content = ALL_PROMPTS[i]\n",
        "        prompt_hash = hashlib.sha1(prompt_content.encode()).hexdigest()[:10]\n",
        "        prompt_id = f\"prompt_{i:05d}_{prompt_hash}\"\n",
        "        output_path = os.path.join(BASE_OUTPUT_DIR, DATASET_TIER, selected_model, prompt_id, \"raw_output.json\")\n",
        "        \n",
        "        if not os.path.exists(output_path):\n",
        "            tasks_to_run.append((i, prompt_content, selected_model, BASE_OUTPUT_DIR, DATASET_TIER))\n",
        "\n",
        "    total_already_generated = total_prompts_to_consider - len(tasks_to_run)\n",
        "    print(f\"Targeting {NUM_PLANS_TO_GENERATE} total plans.\")\n",
        "    print(f\"Found {total_already_generated} plans already completed in previous runs.\")\n",
        "    \n",
        "    if not tasks_to_run:\n",
        "        print(\"\\\\nüéâ All requested plans have already been generated. Nothing to do!\")\n",
        "    else:\n",
        "        print(f\"‚ñ∂Ô∏è Preparing to generate {len(tasks_to_run)} new plans...\")\n",
        "        print(f\"üöÄ Starting PARALLEL data generation using {MAX_WORKERS} workers...\")\n",
        "\n",
        "        successful_generations = 0\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "            # Pass the list of tasks that actually need to be run\n",
        "            results = list(tqdm(executor.map(generate_and_save_raw_plan, tasks_to_run), total=len(tasks_to_run), desc=f\"Generating with {selected_model}\"))\n",
        "        \n",
        "        # Count successes from the results tuple (prompt_id, success_boolean, message)\n",
        "        successful_generations = sum(1 for res in results if res[1] is True and res[2] != \"Skipped\")\n",
        "\n",
        "        print(\"\\\\n\" + \"=\"*50)\n",
        "        print(\"‚úÖ Data Generation Run Complete!\")\n",
        "        print(f\"Successfully generated {successful_generations} NEW raw plan files in this run.\")\n",
        "        final_output_dir = os.path.join(BASE_OUTPUT_DIR, DATASET_TIER, selected_model)\n",
        "        print(f\"All outputs are saved in your Google Drive at: '{final_output_dir}'\")\n",
        "        print(\"=\"*50)\n",
        "else:\n",
        "    print(\"\\\\nüõë HALTED. Please run the prompt loading cell (Cell 3) successfully first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 7. (Optional) Package and Download Results ---\n",
        "import shutil\n",
        "import time\n",
        "import os\n",
        "\n",
        "# The data is already safe in Google Drive. This cell is for creating a convenient zip archive.\n",
        "output_directory_path = os.path.join(BASE_OUTPUT_DIR, DATASET_TIER, selected_model)\n",
        "zip_filename = f\"{DATASET_TIER}_{selected_model}_raw_data_{int(time.time())}\"\n",
        "zip_filepath_in_colab = f\"/content/{zip_filename}\" # We'll create the zip in the local runtime for speed\n",
        "\n",
        "print(f\"Locating generated data in your Google Drive: {output_directory_path}...\")\n",
        "\n",
        "if os.path.isdir(output_directory_path):\n",
        "    print(f\"Compressing into '{zip_filename}.zip' in the Colab runtime...\")\n",
        "    # This might take a while if the dataset is very large\n",
        "    try:\n",
        "        shutil.make_archive(\n",
        "            base_name=zip_filepath_in_colab,\n",
        "            format='zip',\n",
        "            root_dir=output_directory_path\n",
        "        )\n",
        "        print(f\"‚úÖ Success! Your data is compressed and ready for download at {zip_filepath_in_colab}.zip\")\n",
        "        print(\"You can find it in the 'Files' panel on the left.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred during zipping: {e}\")\n",
        "else:\n",
        "    print(f\"‚ùå Error: Could not find the output directory '{output_directory_path}'. Please ensure the generation step ran correctly.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
