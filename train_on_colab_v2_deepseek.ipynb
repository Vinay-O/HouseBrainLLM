{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bc0b23",
   "metadata": {},
   "source": [
    "# HouseBrain V2: Fine-Tuning DeepSeek Coder on Google Colab\n",
    "\n",
    "This notebook provides a complete workflow for fine-tuning the `deepseek-ai/deepseek-coder-6.7b-instruct` model for architectural design. It uses our **\"Gold Standard\" dataset** to teach the model our specific schema and architectural nuances.\n",
    "\n",
    "**GPU Requirement:** An A100 GPU (available on Colab Pro+) is recommended for fine-tuning this model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873d9dc",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "This step clones the project repository from GitHub and installs all the necessary Python packages for fine-tuning, including `transformers`, `peft`, `trl`, and `bitsandbytes` for memory-efficient 4-bit training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Provide your GitHub token\n",
    "# To clone the private repository, you need a GitHub Personal Access Token (PAT)\n",
    "# with repo access. Create one here: https://github.com/settings/tokens\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "# Use a placeholder if you're not running this interactively\n",
    "try:\n",
    "    github_token = getpass('Enter your GitHub token: ')\n",
    "    os.environ['GITHUB_TOKEN'] = github_token\n",
    "except Exception:\n",
    "    print(\"Could not read token, please paste it directly into the next cell\")\n",
    "    os.environ['GITHUB_TOKEN'] = \"your_github_token_here\"\n",
    "\n",
    "# Step 2: Clone the repository using the token\n",
    "# Make sure the repository name is correct\n",
    "!git clone https://{os.environ.get('GITHUB_TOKEN')}@github.com/Vinay-O/HouseBrainLLM.git\n",
    "%cd HouseBrainLLM\n",
    "\n",
    "# Step 3: Install dependencies\n",
    "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b341d",
   "metadata": {},
   "source": [
    "## Step 3: (Optional) Generate New High-Quality Drafts\n",
    "\n",
    "This section is for creating new **Gold** or **Platinum** standard examples. It will set up an Ollama server within the Colab environment, download a powerful base model (`deepseek-r1:8b`), and use it to generate raw drafts based on expert prompts.\n",
    "\n",
    "**Workflow:**\n",
    "1.  Run the cells below to generate the raw `.json` draft files.\n",
    "2.  Download the generated files from the Colab file browser (under `data/training/gold_standard/` or `data/training/platinum_standard/`).\n",
    "3.  Use the AI assistant's \"Analyze and Repair\" process to perfect the drafts locally.\n",
    "4.  Upload the final, corrected `.json` files back to the appropriate directory before proceeding to the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama\n",
    "!if ! command -v ollama &> /dev/null; then curl -fsSL https://ollama.com/install.sh | sh; fi\n",
    "\n",
    "# Start Ollama as a background process\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Set environment variable to bind to all interfaces\n",
    "os.environ['OLLAMA_HOST'] = '0.0.0.0'\n",
    "\n",
    "# Start the server as a raw background process\n",
    "# This is more robust in non-systemd environments like Colab\n",
    "get_ipython().system_raw('ollama serve > ollama.log 2>&1 &')\n",
    "\n",
    "# Wait for Ollama to be ready\n",
    "print(\"⏳ Waiting for Ollama server to start...\")\n",
    "time.sleep(5) # Initial wait\n",
    "for i in range(60): # Wait up to 60 seconds\n",
    "    try:\n",
    "        response = requests.get(\"http://127.0.0.1:11434\")\n",
    "        if response.status_code == 200:\n",
    "            print(\"✅ Ollama server is running!\")\n",
    "            break\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        pass # Keep trying while the server starts up\n",
    "    time.sleep(1)\n",
    "else:\n",
    "    print(\"❌ Ollama server failed to start. Check the logs for errors.\")\n",
    "    !cat ollama.log\n",
    "\n",
    "# Download the model for draft generation\n",
    "!ollama pull deepseek-coder:6.7b-instruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Platinum Standard directory if it doesn't exist\n",
    "!mkdir -p data/training/platinum_standard\n",
    "\n",
    "# --- GENERATE GOLD STANDARD DRAFT #21 ---\n",
    "GOLD_PROMPT = \"Design a luxurious 4BHK G+1 duplex for a 40x60 feet west-facing plot in a gated community in Bangalore. The design must be Vastu-compliant and include a home office on the ground floor, a private family lounge on the first floor, and balconies for every bedroom. The client desires a contemporary architectural style with large windows for ample natural light.\"\n",
    "!python scripts/generate_draft_from_prompt.py --model \"deepseek-coder:6.7b-instruct\" --scenario \"{GOLD_PROMPT}\" --output-file \"data/training/gold_standard/gold_standard_21_draft.json\"\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
    "\n",
    "# --- GENERATE PLATINUM STANDARD DRAFT #01 ---\n",
    "PLATINUM_PROMPT = \"Design a one-of-a-kind, 'biophilic' 3BHK luxury retreat on a 50x80 feet plot overlooking the backwaters of Kerala. The design must seamlessly integrate indoor and outdoor spaces, featuring a central open-to-sky courtyard with a water body, extensive use of natural materials like laterite stone and teak wood, and a cantilevered infinity pool on the first floor. Prioritize sustainability with rainwater harvesting and solar panel provisions. The architectural style should be a modern interpretation of traditional Kerala design.\"\n",
    "!python scripts/generate_draft_from_prompt.py --model \"deepseek-coder:6.7b-instruct\" --scenario \"{PLAT_PROMPT}\" --output-file \"data/training/platinum_standard/platinum_standard_01_draft.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca665d",
   "metadata": {},
   "source": [
    "## Step 4: Prepare the Gold Standard Data\n",
    "\n",
    "This step runs our preparation script. It will process the 20 raw Gold Standard JSON files (plus any new ones you've generated and perfected) and create a new `gold_standard_finetune_ready` directory containing the data in the simple `{\"prompt\": \"...\", \"output\": \"...\"}` format required by the training script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/prepare_gold_standard_data.py\n",
    "!echo \"\\n✅ Data preparation complete. Verifying the new directory:\"\n",
    "!ls -l data/training/gold_standard_finetune_ready | wc -l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92814c9a",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Data for DeepSeek Fine-Tuning\n",
    "\n",
    "This step is crucial. The `deepseek-coder` model requires a specific prompt format for instruction fine-tuning. We will load the data prepared in the previous step and reformat it into the required structure, then save it to a new directory for the trainer to use.\n",
    "\n",
    "**DeepSeek Prompt Template:**\n",
    "```\n",
    "You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company. For inquiries regarding creation tool compatibility, provide comprehensive guidance and support.\n",
    "### Instruction:\n",
    "{user_prompt}\n",
    "### Response:\n",
    "{model_response}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Define the DeepSeek prompt template\n",
    "DEEPSEEK_TEMPLATE = \"\"\"You are an expert Indian architect AI, utilizing the DeepSeek Coder model. Your task is to generate a complete, valid, and architecturally sound house design in JSON format.\n",
    "### Instruction:\n",
    "{prompt}\n",
    "### Response:\n",
    "{completion}\"\"\"\n",
    "\n",
    "# Load the dataset prepared by the previous script\n",
    "source_dir = \"data/training/gold_standard_finetune_ready\"\n",
    "dataset = load_dataset(\"json\", data_files=[str(f) for f in Path(source_dir).glob(\"*.json\")])['train']\n",
    "\n",
    "def format_for_deepseek(entry):\n",
    "    \"\"\"Applies the DeepSeek prompt format to a dataset entry.\"\"\"\n",
    "    formatted_text = DEEPSEEK_TEMPLATE.format(\n",
    "        prompt=entry['prompt'],\n",
    "        completion=json.dumps(json.loads(entry['output']), indent=2) # Ensure completion is a formatted string\n",
    "    )\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# Apply the formatting\n",
    "formatted_dataset = dataset.map(format_for_deepseek)\n",
    "\n",
    "# Save the newly formatted dataset\n",
    "output_dir = Path(\"data/training/gold_standard_finetune_deepseek_ready\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as a single JSONL file, which is efficient for the trainer\n",
    "formatted_dataset.to_json(output_dir / \"data.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "print(f\"✅ Successfully formatted and saved dataset for DeepSeek at {output_dir}\")\n",
    "print(\"Example of formatted data:\")\n",
    "print(formatted_dataset[0]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084dc267",
   "metadata": {},
   "source": [
    "## Step 6: Authenticate with Hugging Face\n",
    "\n",
    "To download gated models like Llama 3, you need to be authenticated with Hugging Face. \n",
    "\n",
    "1.  Create a Hugging Face account if you don't have one.\n",
    "2.  Generate an Access Token with \"read\" permissions here: https://huggingface.co/settings/tokens\n",
    "3.  Run the cell below and paste your token when prompted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e0563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "# Prompt for Hugging Face token and login\n",
    "try:\n",
    "    hf_token = getpass('Enter your Hugging Face token: ')\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "except Exception:\n",
    "    print(\"Could not read token, please paste it directly into the next cell\")\n",
    "    os.environ['HF_TOKEN'] = \"your_hf_token_here\"\n",
    "\n",
    "!huggingface-cli login --token $HF_TOKEN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbc72b",
   "metadata": {},
   "source": [
    "## Step 7: Run the Fine-Tuning Script\n",
    "\n",
    "This is the core of the process. We execute the `run_finetuning.py` script, which will:\n",
    "\n",
    "1.  **Load** our prepared Gold Standard examples.\n",
    "2.  **Download** the base `meta-llama/Llama-3-8B-Instruct` model from Hugging Face.\n",
    "3.  **Configure** 4-bit quantization and LoRA for efficient training.\n",
    "4.  **Fine-tune** the model on our data.\n",
    "5.  **Save** the final, specialized `housebrain-llama3-8b-v0.1` model to the `models/` directory.\n",
    "\n",
    "We will use a high number of epochs (e.g., 200) because our dataset is very high-quality but small. This is necessary to ensure the model learns the schema thoroughly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ac353",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/run_finetuning.py \\\n",
    "    --dataset-path \"data/training/gold_standard_finetune_deepseek_ready\" \\\n",
    "    --base-model \"deepseek-ai/deepseek-coder-6.7b-instruct\" \\\n",
    "    --output-path \"models/housebrain-deepseek-coder-6.7b-v0.1\" \\\n",
    "    --epochs 200 \\\n",
    "    --batch-size 2 \\\n",
    "    --learning-rate 2e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ed78e",
   "metadata": {},
   "source": [
    "## Step 8: Next Steps - Using Your Fine-Tuned Model\n",
    "\n",
    "Once training is complete, the new model is saved in the `models/housebrain-deepseek-coder-6.7b-v0.1` directory. \n",
    "\n",
    "You can now use this specialized model in your `generate_validated_silver_data.py` script (by changing the model ID) to generate a large, high-quality dataset of thousands of examples. This is the path to a truly production-ready system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e119bb7",
   "metadata": {},
   "source": [
    "## Step 9 (Optional): A/B Test with an Alternative Model (Llama 3)\n",
    "\n",
    "Now that you have a fine-tuned DeepSeek Coder model, you can run an experiment to compare it against another powerful base model like Llama 3. You can use the original `train_on_colab.ipynb` notebook to fine-tune Llama 3 on the same Gold Standard dataset.\n",
    "\n",
    "Once both are trained, you will have two expert models: `housebrain-deepseek-coder-6.7b-v0.1` and `housebrain-llama3-8b-v0.1`. You can then evaluate them head-to-head on a new set of prompts to see which one produces superior architectural designs. This data-driven approach guarantees we select the best possible foundation for our production system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e7519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python scripts/run_finetuning.py \\\n",
    "#     --dataset-path \"data/training/gold_standard_finetune_ready\" \\\n",
    "#     --base-model \"Qwen/Qwen2-7B-Instruct\" \\\n",
    "#     --output-path \"models/housebrain-qwen2-7b-v0.1\" \\\n",
    "#     --epochs 200 \\\n",
    "#     --batch-size 2 \\\n",
    "#     --learning-rate 2e-5\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
