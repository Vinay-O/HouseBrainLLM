{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bc0b23",
   "metadata": {},
   "source": [
    "# HouseBrain V2: Fine-Tuning DeepSeek-R1-Distill-Llama-8B on Colab\n",
    "\n",
    "This notebook provides a complete workflow for fine-tuning the `deepseek-ai/DeepSeek-R1-Distill-Llama-8B` model for architectural design. It uses our **\"Gold Standard\" dataset** to teach the model our specific schema and architectural nuances.\n",
    "\n",
    "**GPU Requirement:** An A100 GPU (available on Colab Pro+) is recommended for fine-tuning this model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873d9dc",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "This step clones the project repository from GitHub and installs all the necessary Python packages for fine-tuning, including `transformers`, `peft`, `trl`, and `bitsandbytes` for memory-efficient 4-bit training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Provide your GitHub token\n",
    "# To clone the private repository, you need a GitHub Personal Access Token (PAT)\n",
    "# with repo access. Create one here: https://github.com/settings/tokens\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "# Use a placeholder if you're not running this interactively\n",
    "try:\n",
    "    github_token = getpass('Enter your GitHub token: ')\n",
    "    os.environ['GITHUB_TOKEN'] = github_token\n",
    "except Exception:\n",
    "    print(\"Could not read token, please paste it directly into the next cell\")\n",
    "    os.environ['GITHUB_TOKEN'] = \"your_github_token_here\"\n",
    "\n",
    "# Step 2: Clone the repository using the token\n",
    "# Make sure the repository name is correct\n",
    "!git clone https://{os.environ.get('GITHUB_TOKEN')}@github.com/Vinay-O/HouseBrainLLM.git\n",
    "%cd HouseBrainLLM\n",
    "\n",
    "# Step 3: Install dependencies\n",
    "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6cf83",
   "metadata": {},
   "source": [
    "## Step 2: Authenticate with Hugging Face\n",
    "\n",
    "To download powerful models from Hugging Face, you need to be authenticated. \n",
    "\n",
    "1.  Create a Hugging Face account if you don't have one.\n",
    "2.  Generate an Access Token with \"read\" permissions here: https://huggingface.co/settings/tokens\n",
    "3.  Run the cell below and paste your token when prompted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d05705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "# Prompt for Hugging Face token and login\n",
    "try:\n",
    "    hf_token = getpass('Enter your Hugging Face token: ')\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "except Exception:\n",
    "    print(\"Could not read token, please paste it directly into the next cell\")\n",
    "    os.environ['HF_TOKEN'] = \"your_hf_token_here\"\n",
    "\n",
    "!huggingface-cli login --token $HF_TOKEN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b341d",
   "metadata": {},
   "source": [
    "## Step 3: (Optional) Generate New Drafts with Ollama\n",
    "\n",
    "This section is for creating new **Gold** or **Platinum** standard examples. It will set up an Ollama server within the Colab environment, download a powerful base model (`deepseek-r1:8b`), and use it to generate raw drafts based on expert prompts.\n",
    "\n",
    "**Workflow:**\n",
    "1.  Run the cells below to generate the raw `.json` draft files.\n",
    "2.  Download the generated files from the Colab file browser (under `data/training/gold_standard/` or `data/training/platinum_standard/`).\n",
    "3.  Use the AI assistant's \"Analyze and Repair\" process to perfect the drafts locally.\n",
    "4.  Upload the final, corrected `.json` files back to the appropriate directory before proceeding to the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama\n",
    "!if ! command -v ollama &> /dev/null; then curl -fsSL https://ollama.com/install.sh | sh; fi\n",
    "\n",
    "# Start Ollama as a background process\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Set environment variable to bind to all interfaces\n",
    "os.environ['OLLAMA_HOST'] = '0.0.0.0'\n",
    "\n",
    "# Start the server as a raw background process\n",
    "# This is more robust in non-systemd environments like Colab\n",
    "get_ipython().system_raw('ollama serve > ollama.log 2>&1 &')\n",
    "\n",
    "# Wait for Ollama to be ready\n",
    "print(\"⏳ Waiting for Ollama server to start...\")\n",
    "time.sleep(5) # Initial wait\n",
    "for i in range(60): # Wait up to 60 seconds\n",
    "    try:\n",
    "        response = requests.get(\"http://127.0.0.1:11434\")\n",
    "        if response.status_code == 200:\n",
    "            print(\"✅ Ollama server is running!\")\n",
    "            break\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        pass # Keep trying while the server starts up\n",
    "    time.sleep(1)\n",
    "else:\n",
    "    print(\"❌ Ollama server failed to start. Check the logs for errors.\")\n",
    "    !cat ollama.log\n",
    "\n",
    "# Download the model for draft generation\n",
    "# Note: Ollama may not have this exact model, we pull a close equivalent for generation\n",
    "!ollama pull deepseek-r1:8b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HEALTH CHECK ---\n",
    "# First, let's run a very simple prompt to confirm the model is loaded and responding.\n",
    "# This should be very fast.\n",
    "HEALTH_CHECK_PROMPT = \"Generate a valid JSON object for a single wall with id 'test-wall', level_id 'ground_floor', and a simple rectangular polygon.\"\n",
    "!python scripts/generate_draft_from_prompt.py --model \"deepseek-r1:8b\" --scenario \"{HEALTH_CHECK_PROMPT}\" --output-file \"data/training/health_check_output.json\"\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"✅ Health check prompt sent. Checking for output...\")\n",
    "!cat data/training/health_check_output.json\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"If you see a valid JSON object above, the model is working. You can now proceed to the next cell to generate the full drafts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05012dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEBUGGING: VIEW RAW MODEL OUTPUT ---\n",
    "# The cell above may show a \"No such file or directory\" error if the model's\n",
    "# response was not pure JSON. This is expected behavior.\n",
    "# The script saves the full, raw response to a .raw_error.txt file.\n",
    "# Let's print the content of that file to see what the model *actually* said.\n",
    "\n",
    "!cat data/training/health_check_output.json.raw_error.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Platinum Standard directory if it doesn't exist\n",
    "!mkdir -p data/training/platinum_standard\n",
    "\n",
    "# --- GENERATE GOLD STANDARD DRAFT #21 ---\n",
    "GOLD_PROMPT = \"Design a luxurious 4BHK G+1 duplex for a 40x60 feet west-facing plot in a gated community in Bangalore. The design must be Vastu-compliant and include a home office on the ground floor, a private family lounge on the first floor, and balconies for every bedroom. The client desires a contemporary architectural style with large windows for ample natural light.\"\n",
    "!python scripts/generate_draft_from_prompt.py --model \"deepseek-r1:8b\" --scenario \"{GOLD_PROMPT}\" --output-file \"data/training/gold_standard/gold_standard_21_draft.json\"\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
    "\n",
    "# --- GENERATE PLATINUM STANDARD DRAFT #01 ---\n",
    "PLATINUM_PROMPT = \"Design a one-of-a-kind, 'biophilic' 3BHK luxury retreat on a 50x80 feet plot overlooking the backwaters of Kerala. The design must seamlessly integrate indoor and outdoor spaces, featuring a central open-to-sky courtyard with a water body, extensive use of natural materials like laterite stone and teak wood, and a cantilevered infinity pool on the first floor. Prioritize sustainability with rainwater harvesting and solar panel provisions. The architectural style should be a modern interpretation of traditional Kerala design.\"\n",
    "!python scripts/generate_draft_from_prompt.py --model \"deepseek-r1:8b\" --scenario \"{PLATINUM_PROMPT}\" --output-file \"data/training/platinum_standard/platinum_standard_01_draft.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# STEP 4.1: (NEW) Sanitize Gold Standard Data\n",
    "# ------------------------------------------------------------------\n",
    "# This step fixes a common data inconsistency issue where some JSON files\n",
    "# might use `null` for list fields (like `doors`: null) while others use\n",
    "# an empty list (`doors`: []). This mismatch can cause the `datasets`\n",
    "# library to fail during loading. This script scans all gold standard\n",
    "# files and enforces `[]` for consistency.\n",
    "\n",
    "!python scripts/sanitize_gold_data.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca665d",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Base Training Data\n",
    "\n",
    "This step runs our preparation script. It will process the 20 raw Gold Standard JSON files (plus any new ones you've generated and perfected) and create a new `gold_standard_finetune_ready` directory containing the data in the simple `{\"prompt\": \"...\", \"output\": \"...\"}` format required by the training script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/prepare_gold_standard_data.py\n",
    "!echo \"\\n✅ Data preparation complete. Verifying the new directory:\"\n",
    "!ls -l data/training/gold_standard_finetune_ready | wc -l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92814c9a",
   "metadata": {},
   "source": [
    "## Step 5: Format Data for Fine-Tuning\n",
    "\n",
    "This step is crucial. Since `DeepSeek-R1-Distill-Llama-8B` is based on Llama, it requires a Llama-3-style prompt format for instruction fine-tuning. We will load the data prepared in the previous step and reformat it into the required structure, then save it to a new directory for the trainer to use.\n",
    "\n",
    "**Llama 3 Prompt Template:**\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{completion}<|eot_id|>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Define the Llama 3 prompt template\n",
    "LLAMA3_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{completion}<|eot_id|>\"\"\"\n",
    "\n",
    "# Load the dataset prepared by the previous script\n",
    "source_dir = \"data/training/gold_standard_finetune_ready\"\n",
    "dataset = load_dataset(\"json\", data_files=[str(f) for f in Path(source_dir).glob(\"*.json\")])['train']\n",
    "\n",
    "def format_for_llama3(entry):\n",
    "    \"\"\"Applies the Llama 3 prompt format to a dataset entry.\"\"\"\n",
    "    formatted_text = LLAMA3_TEMPLATE.format(\n",
    "        prompt=entry['prompt'],\n",
    "        completion=json.dumps(json.loads(entry['output']), indent=2) # Ensure completion is a formatted string\n",
    "    )\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# Apply the formatting\n",
    "formatted_dataset = dataset.map(format_for_llama3)\n",
    "\n",
    "# Save the newly formatted dataset\n",
    "output_dir = Path(\"data/training/gold_standard_finetune_llama3_ready\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as a single JSONL file, which is efficient for the trainer\n",
    "formatted_dataset.to_json(output_dir / \"data.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "print(f\"✅ Successfully formatted and saved dataset for Llama-style fine-tuning at {output_dir}\")\n",
    "print(\"Example of formatted data:\")\n",
    "print(formatted_dataset[0]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084dc267",
   "metadata": {},
   "source": [
    "# This step was moved to the top of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e0563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell's logic was moved to Step 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbc72b",
   "metadata": {},
   "source": [
    "## Step 6: Run the Fine-Tuning Script\n",
    "\n",
    "This is the core of the process. We execute the `run_finetuning.py` script, which will:\n",
    "\n",
    "1.  **Load** our prepared Gold Standard examples.\n",
    "2.  **Download** the base `meta-llama/Llama-3-8B-Instruct` model from Hugging Face.\n",
    "3.  **Configure** 4-bit quantization and LoRA for efficient training.\n",
    "4.  **Fine-tune** the model on our data.\n",
    "5.  **Save** the final, specialized `housebrain-llama3-8b-v0.1` model to the `models/` directory.\n",
    "\n",
    "We will use a high number of epochs (e.g., 200) because our dataset is very high-quality but small. This is necessary to ensure the model learns the schema thoroughly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ac353",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/run_finetuning.py \\\n",
    "    --dataset-path \"data/training/gold_standard_finetune_llama3_ready\" \\\n",
    "    --base-model \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\" \\\n",
    "    --output-path \"models/housebrain-deepseek-r1-distill-llama-8b-v0.1\" \\\n",
    "    --epochs 200 \\\n",
    "    --batch-size 2 \\\n",
    "    --learning-rate 2e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ed78e",
   "metadata": {},
   "source": [
    "## Step 7: Next Steps\n",
    "\n",
    "Once training is complete, the new model is saved in the `models/housebrain-deepseek-r1-distill-llama-8b-v0.1` directory. \n",
    "\n",
    "You can now use this specialized model in your `generate_validated_silver_data.py` script (by changing the model ID) to generate a large, high-quality dataset of thousands of examples. This is the path to a truly production-ready system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e119bb7",
   "metadata": {},
   "source": [
    "## Step 8 (Optional): A/B Test with an Alternative Model\n",
    "\n",
    "Now that you have a fine-tuned DeepSeek-Llama model, you can run an experiment to compare it against the original, non-distilled Llama 3 model. You can use the `train_on_colab.ipynb` notebook to fine-tune Llama 3 on the same Gold Standard dataset.\n",
    "\n",
    "Once both are trained, you will have two expert models: `housebrain-deepseek-r1-distill-llama-8b-v0.1` and `housebrain-llama3-8b-v0.1`. You can then evaluate them head-to-head on a new set of prompts to see which one produces superior architectural designs. This data-driven approach guarantees we select the best possible foundation for our production system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e7519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python scripts/run_finetuning.py \\\n",
    "#     --dataset-path \"data/training/gold_standard_finetune_ready\" \\\n",
    "#     --base-model \"Qwen/Qwen2-7B-Instruct\" \\\n",
    "#     --output-path \"models/housebrain-qwen2-7b-v0.1\" \\\n",
    "#     --epochs 200 \\\n",
    "#     --batch-size 2 \\\n",
    "#     --learning-rate 2e-5\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
