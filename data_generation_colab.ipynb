{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain: Automated Data Factory ðŸ­\n",
        "\n",
        "This notebook is a dedicated environment for generating a large, high-quality dataset of architectural plans. It implements a hybrid-model `Generate -> Analyze -> Repair` pipeline.\n",
        "\n",
        "**Workflow:**\n",
        "1.  **Setup:** Mounts Google Drive, clones the repository, and installs Ollama.\n",
        "2.  **Model Provisioning:** Pulls two models: a fast model for initial generation (`llama3:8b`) and a powerful model for repairs (`llama3:70b`).\n",
        "3.  **Prompt Loading:** Reads a list of design prompts from a text file.\n",
        "4.  **Automated Curation:** For each prompt, it runs the orchestration script which:\n",
        "    a.  **Generates** a draft using the fast `llama3:8b`.\n",
        "    b.  **Analyzes** the draft for errors.\n",
        "    c.  **Repairs** the draft using the powerful `llama3:70b` if needed.\n",
        "5.  **Output:** Saves the final, validated \"Gold Standard\" JSON files directly to your Google Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive to persist our dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "# IMPORTANT: Replace <YOUR_GITHUB_PAT> with your GitHub Personal Access Token and YOUR_USERNAME with your GitHub username\n",
        "GITHUB_PAT = \"<YOUR_GITHUB_PAT>\"\n",
        "REPOSITORY_URL = f\"https://{GITHUB_PAT}@github.com/YOUR_USERNAME/housebrain_v1_1.git\"\n",
        "\n",
        "!git clone {REPOSITORY_URL}\n",
        "%cd housebrain_v1_1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install and start Ollama in the background\n",
        "!echo \"Installing Ollama...\"\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import asyncio\n",
        "\n",
        "# Run Ollama serve in the background\n",
        "command = \"ollama serve\"\n",
        "process = subprocess.Popen(command.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "print(\"Ollama server started in the background.\")\n",
        "\n",
        "!sleep 5 # Give the server a moment to start\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Health Check: Verify Ollama Server is Running\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This command lists the models that Ollama is currently serving.\n",
        "# Before you pull any models, this will likely be empty.\n",
        "# After the server starts, it should return a success message.\n",
        "!ollama list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Provision LLM Models\n",
        "This step will take a while, especially for the 70B model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pull the fast model for generation\n",
        "!echo \"Pulling Llama 3 8B model (for generation)...\"\n",
        "!ollama pull llama3:8b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 2.1. Verify 8B Model Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Running 'ollama list' again.\n",
        "# You should now see 'llama3:8b' in the list of available models.\n",
        "!ollama list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pull the powerful model for repair\n",
        "# This will download ~19GB.\n",
        "!echo \"Pulling Qwen3 30B model (for repair)...\"\n",
        "!ollama pull qwen3:30b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### 2.2. Verify 30B Model Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Running 'ollama list' a final time.\n",
        "# You should now see both 'llama3:8b' and 'qwen3:30b' in the list.\n",
        "!ollama list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Prompt List\n",
        "\n",
        "Create a file named `prompts.txt` in the root of your repository. Each line in this file should be a unique prompt for a house plan. The more varied and detailed the prompts, the better the dataset will be.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample prompts.txt file. \n",
        "# YOU SHOULD REPLACE THIS WITH YOUR OWN LARGER FILE.\n",
        "prompts = [\n",
        "    \"A modern, single-story 3BHK house for a 50x80 feet plot. It must feature an open-plan kitchen and living area, a dedicated home office, and be Vastu-compliant with a North-facing entrance.\",\n",
        "    \"A luxurious two-story 5BHK villa for a 100x100 feet plot, west-facing, with a swimming pool, home theater, and a large garden.\",\n",
        "    \"A compact, budget-friendly 2BHK apartment design for a family of four, with a total area of 1200 sqft.\",\n",
        "    \"A traditional Kerala-style 'Nalukettu' house with a central courtyard for a 60x90 feet south-facing plot.\",\n",
        "    \"Design a G+2 building on a 30x60 feet plot. The ground floor should be for parking. The first and second floors should be identical 2BHK units.\",\n",
        "]\n",
        "\n",
        "with open('prompts.txt', 'w') as f:\n",
        "    for prompt in prompts:\n",
        "        f.write(prompt + '\\\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run the Automated Curation Factory\n",
        "\n",
        "This final step will loop through your `prompts.txt` file and run the full pipeline for each one. Validated files will be saved to your Google Drive.\n",
        "\n",
        "This process can run for a very long time. Ensure your Colab session does not time out.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "prompts_file = 'prompts.txt'\n",
        "# This path should point to a folder in your Google Drive\n",
        "output_directory = '/content/drive/MyDrive/housebrain_automated_dataset'\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "with open(prompts_file, 'r') as f:\n",
        "    prompts = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "    print(f\"\"\"\n",
        "    =================================================\n",
        "    Processing prompt {i+1}/{len(prompts)}\n",
        "    PROMPT: {prompt[:100]}...\n",
        "    =================================================\n",
        "    \"\"\")\n",
        "    \n",
        "    command = [\n",
        "        'python',\n",
        "        'scripts/automated_curation.py',\n",
        "        '--prompt', prompt,\n",
        "        '--output-dir', output_directory,\n",
        "        '--model', 'llama3:8b',\n",
        "        '--repair-model', 'qwen3:30b',\n",
        "        '--max-retries', '3'\n",
        "    ]\n",
        "    \n",
        "    # Using subprocess.run to see live output in Colab\n",
        "    with subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, universal_newlines=True) as p:\n",
        "        for line in p.stdout:\n",
        "            print(line, end='')\n",
        "\n",
        "print(\"\\\\n\\\\nðŸŽ‰ Data Factory run complete! Check your Google Drive for the generated files.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
