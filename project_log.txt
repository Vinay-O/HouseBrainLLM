Project Log: HouseBrain AI Architect
===================================

This document serves as a persistent brain for the HouseBrain AI Architect project, tracking our goals, progress, decisions, and next steps.

---

### Project Goal

To create a specialized, fine-tuned AI model named the "HouseBrain Architect." This model will be an expert at a single task: taking a user's natural language prompt (e.g., "a modern 3BHK house...") and generating a complete, valid, and high-quality architectural plan in a single step, conforming to our specific `HouseOutput` JSON schema.

---

### Current Status

**Date:** 2024-08-25

We have successfully validated our entire data generation pipeline using the powerful `mixtral:instruct` model in Google Colab, resulting in a high-quality 5-prompt "Gold Standard" dataset. This proves the "Architect's Assembly Line" strategy is effective.

**The immediate next step is to generate a large-scale "platinum" dataset of 10,000 prompts using our new prompt generation script.**

---

### Project History & Key Decisions

1.  **Initial Problem:** Attempts to run the generation pipeline on the local machine failed.
2.  **Debugging Journey:**
    *   Initially, we encountered a `file not found` error. We diagnosed this as a working directory issue and fixed it by changing the directory before executing the script.
    *   Next, we hit a `connection refused` error. We identified that the local Ollama server was not running.
    *   After starting Ollama, the script began to run but then `timed out`. We diagnosed that the `mixtral:instruct` model (26GB) was too large to run efficiently on the local machine.
3.  **Strategic Pivot: Pipeline Validation:**
    *   **Decision:** To separate "code validation" from "quality generation," we decided to test the pipeline with a smaller, faster model.
    *   **Action:** We modified `test_pipeline_mixtral.sh` to use `llama3:instruct` (4.7GB).
    *   **Result:** The pipeline ran successfully from end to end, but as expected, the final output failed schema validation. This was a **successful test**, proving the code works and that a small model is insufficient for quality.
4.  **Moving to the Cloud for Quality:**
    *   **Decision:** The best way to run the large, high-quality `mixtral:instruct` model is in a GPU-powered environment. We chose Google Colab as the most efficient tool for this.
    *   **Action:** We configured the `data_generation_colab.ipynb` notebook to use the `mixtral:instruct` model.
    *   **Outcome:** The notebook is now ready to generate our "platinum" dataset. Any data produced by this notebook is considered our highest quality tier, as it must pass strict validation before being saved.
6.  **Successful Dataset Generation (Mixtral):**
    *   **Date:** 2024-08-25
    *   **Action:** The user executed the `data_generation_colab.ipynb` notebook, processing 5 prompts with the `mixtral:instruct` model.
    *   **Outcome:** The pipeline ran successfully for all 5 prompts, producing a small but high-quality, schema-compliant dataset. This is a major milestone, validating that the "Architect's Assembly Line" can produce "Gold Standard" data.
7.  **Scaling Up for Platinum Dataset:**
    *   **Date:** 2024-08-25
    *   **Action:** Created `scripts/generate_prompts.py` to programmatically generate 10,000+ diverse architectural prompts.
    *   **Next Step:** Use these prompts to run the Colab pipeline at scale and generate the full platinum training dataset.
5.  **Syncing to GitHub:**
    *   **Date:** 2024-08-25
    *   **Action:** Created the `project_log.txt` file to track progress. Added an entry to `.gitignore` to ignore the `output/` directory. Pushed all updated scripts and the new log file to the `main` branch on GitHub.
    *   **Outcome:** The latest project version is now available on GitHub, ready to be pulled into Google Colab for data generation.

---

### The Long-Term Plan

Once we have generated a sufficient dataset (e.g., 10,000+ examples), our plan is as follows:

1.  **Fine-Tuning:** We will use our platinum dataset to fine-tune a new, specialized model. This will train a base model to become an expert specifically at generating `HouseBrain` plans.
2.  **Evaluation:** We will use a portion of our generated data (a "gold" or "test" set) to rigorously evaluate the performance of our new fine-tuned model.
3.  **Integration:** The final, fine-tuned "HouseBrain Architect" model will be integrated into a simplified, single-step production pipeline, making it highly efficient.
