{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79344bd0",
   "metadata": {},
   "source": [
    "# HouseBrain V3: Data Curation & Training with Llama 3\n",
    "\n",
    "This notebook provides a complete workflow for generating, curating, and fine-tuning with the `llama3` model for architectural design. It uses our **\"Gold Standard\" dataset** to teach the model our specific schema and architectural nuances.\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Generate & Curate Data (Steps 1-3.5):** Use the interactive testing cells to generate new drafts with `llama3` and manually repair them to perfection.\n",
    "2.  **Fine-Tune (Steps 4-7):** Once you have a high-quality, curated dataset, use the second half of the notebook to fine-tune a specialist model.\n",
    "\n",
    "**GPU Requirement:** An A100 or H100 GPU (available on Colab Pro+) is recommended for fine-tuning. A T4 is sufficient for data generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bec9525",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7bc0b23",
   "metadata": {},
   "source": [
    "# HouseBrain V3: Data Curation & Training with Llama 3\n",
    "\n",
    "This notebook provides a complete workflow for generating, curating, and fine-tuning with the `llama3` model for architectural design. It uses our **\"Gold Standard\" dataset** to teach the model our specific schema and architectural nuances.\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Generate & Curate Data (Steps 1-3.5):** Use the interactive testing cells to generate new drafts with `llama3` and manually repair them to perfection.\n",
    "2.  **Fine-Tune (Steps 4-6):** Once you have a high-quality, curated dataset, use the second half of the notebook to fine-tune a specialist model.\n",
    "\n",
    "**GPU Requirement:** An A100 or H100 GPU (available on Colab Pro+) is recommended for fine-tuning. A T4 is sufficient for data generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873d9dc",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "This step clones the project repository from GitHub and installs all the necessary Python packages for fine-tuning, including `transformers`, `peft`, `trl`, and `bitsandbytes` for memory-efficient 4-bit training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749c0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Provide your GitHub token\n",
    "# To clone the private repository, you need a GitHub Personal Access Token (PAT)\n",
    "# with repo access. Create one here: https://github.com/settings/tokens\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "# Use a placeholder if you're not running this interactively\n",
    "try:\n",
    "    github_token = getpass('Enter your GitHub token: ')\n",
    "    os.environ['GITHUB_TOKEN'] = github_token\n",
    "except Exception:\n",
    "    print(\"Could not read token, please paste it directly into the next cell\")\n",
    "    os.environ['GITHUB_TOKEN'] = \"your_github_token_here\"\n",
    "\n",
    "# Step 2: Clone the repository using the token\n",
    "# Make sure the repository name is correct\n",
    "!git clone https://{os.environ.get('GITHUB_TOKEN')}@github.com/Vinay-O/HouseBrainLLM.git\n",
    "%cd HouseBrainLLM\n",
    "\n",
    "# Step 3: Install dependencies\n",
    "!pip install -q -U transformers datasets accelerate peft trl bitsandbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6cf83",
   "metadata": {},
   "source": [
    "## Step 2: Authenticate with Hugging Face\n",
    "\n",
    "To download powerful models from Hugging Face, you need to be authenticated. \n",
    "\n",
    "1.  Create a Hugging Face account if you don't have one.\n",
    "2.  Generate an Access Token with \"read\" permissions here: https://huggingface.co/settings/tokens\n",
    "3.  Run the cell below and paste your token when prompted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d05705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "# Prompt for Hugging Face token and login\n",
    "try:\n",
    "    hf_token = getpass('Enter your Hugging Face token: ')\n",
    "    os.environ['HF_TOKEN'] = hf_token\n",
    "except Exception:\n",
    "    print(\"Could not read token, please paste it directly into the next cell\")\n",
    "    os.environ['HF_TOKEN'] = \"your_hf_token_here\"\n",
    "\n",
    "!huggingface-cli login --token $HF_TOKEN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b341d",
   "metadata": {},
   "source": [
    "## Step 3: Generate New Drafts with Ollama & Llama 3\n",
    "\n",
    "This section is for creating new **Gold** or **Platinum** standard examples. It will set up an Ollama server within the Colab environment, download a powerful base model (`llama3`), and use it to generate raw drafts based on expert prompts.\n",
    "\n",
    "**Workflow:**\n",
    "1.  Run the cells below to set up the server and download the model.\n",
    "2.  Use the **Interactive Curation** section (Step 3.5) to generate, analyze, and repair individual drafts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama\n",
    "!if ! command -v ollama &> /dev/null; then curl -fsSL https://ollama.com/install.sh | sh; fi\n",
    "\n",
    "# Start Ollama as a background process\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Set environment variable to bind to all interfaces\n",
    "os.environ['OLLAMA_HOST'] = '0.0.0.0'\n",
    "\n",
    "# Start the server as a raw background process\n",
    "# This is more robust in non-systemd environments like Colab\n",
    "get_ipython().system_raw('ollama serve > ollama.log 2>&1 &')\n",
    "\n",
    "# Wait for Ollama to be ready\n",
    "print(\"⏳ Waiting for Ollama server to start...\")\n",
    "time.sleep(5) # Initial wait\n",
    "for i in range(60): # Wait up to 60 seconds\n",
    "    try:\n",
    "        response = requests.get(\"http://127.0.0.1:11434\")\n",
    "        if response.status_code == 200:\n",
    "            print(\"✅ Ollama server is running!\")\n",
    "            break\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        pass # Keep trying while the server starts up\n",
    "    time.sleep(1)\n",
    "else:\n",
    "    print(\"❌ Ollama server failed to start. Check the logs for errors.\")\n",
    "    !cat ollama.log\n",
    "\n",
    "# Download the model for draft generation\n",
    "!ollama pull llama3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HEALTH CHECK ---\n",
    "# First, let's run a very simple prompt to confirm the model is loaded and responding.\n",
    "# This should be very fast.\n",
    "HEALTH_CHECK_PROMPT = \"Generate a valid JSON object containing a single key 'status' with the value 'ok'.\"\n",
    "!python scripts/generate_draft_from_prompt.py --model \"llama3\" --scenario \"{HEALTH_CHECK_PROMPT}\" --output-file \"data/training/health_check_output.json\"\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"✅ Health check prompt sent. Checking for output...\")\n",
    "!cat data/training/health_check_output.json\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"If you see a valid JSON object above, the model is working. You can now proceed to the next cell to generate the full drafts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05012dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEBUGGING: VIEW RAW MODEL OUTPUT ---\n",
    "# The cell above may show a \"No such file or directory\" error if the model's\n",
    "# response was not pure JSON. This is expected behavior.\n",
    "# The script saves the full, raw response to a .raw_error.txt file.\n",
    "# Let's print the content of that file to see what the model *actually* said.\n",
    "\n",
    "!cat data/training/health_check_output.json.raw_error.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa38692b",
   "metadata": {},
   "source": [
    "### Step 3.5: Interactive Testing & Curation (Generate -> Analyze -> Repair)\n",
    "\n",
    "This is the most important part of building a high-quality dataset. Use this section to test a specific prompt, see the model's raw output, and then manually repair it to create a \"Gold Standard\" file.\n",
    "\n",
    "**Your Workflow:**\n",
    "1.  **Modify the `TEST_SCENARIO`** in the cell below to the prompt you want to test.\n",
    "2.  **Run the cell.** It will generate a draft and save it to `data/training/curation_test_draft.json`.\n",
    "3.  **Inspect the raw output** printed below the cell. It will likely contain errors or be incomplete.\n",
    "4.  **Copy the JSON** part of the raw output.\n",
    "5.  **Paste it into the text cell** at the very bottom of this notebook.\n",
    "6.  **Manually edit and correct the JSON** until it is a perfect, schema-compliant `HouseOutput` object.\n",
    "7.  **Save the corrected file** to `data/training/gold_standard/` with a descriptive name.\n",
    "\n",
    "Repeat this process to build up your Gold Standard dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02919527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define Your Test Prompt ---\n",
    "TEST_SCENARIO = \"A modern, single-story 3BHK house for a 50x80 feet plot. It must feature an open-plan kitchen and living area, a dedicated home office, and be Vastu-compliant with a North-facing entrance.\"\n",
    "\n",
    "\n",
    "# --- 2. Generate the Draft ---\n",
    "# We use the same schema-aware prompt template from the next step\n",
    "# Note: The schema is defined in the *next* cell. Run that cell first.\n",
    "final_test_prompt = NEW_PROMPT_TEMPLATE.format(scenario=TEST_SCENARIO)\n",
    "with open(\"test_prompt.txt\", \"w\") as f:\n",
    "    f.write(final_test_prompt)\n",
    "\n",
    "!python scripts/generate_draft_from_prompt.py --model \"llama3\" --prompt-file \"test_prompt.txt\" --output-file \"data/training/curation_test_draft.json\"\n",
    "\n",
    "\n",
    "# --- 3. View the Raw Output for Curation ---\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"RAW MODEL OUTPUT (COPY THE JSON FROM HERE TO REPAIR IT):\")\n",
    "print(\"=\"*80)\n",
    "!cat data/training/curation_test_draft.json.raw_error.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519e366",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# STEP 4.1: (NEW) Sanitize Gold Standard Data\n",
    "# ------------------------------------------------------------------\n",
    "# This step fixes a common data inconsistency issue where some JSON files\n",
    "# might use `null` for list fields (like `doors`: null) while others use\n",
    "# an empty list (`doors`: []). This mismatch can cause the `datasets`\n",
    "# library to fail during loading. This script scans all gold standard\n",
    "# files and enforces `[]` for consistency.\n",
    "\n",
    "!python scripts/sanitize_gold_data.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca665d",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Base Training Data\n",
    "\n",
    "This step runs our preparation script. It will process the 20 raw Gold Standard JSON files (plus any new ones you've generated and perfected) and create a new `gold_standard_finetune_ready` directory containing the data in the simple `{\"prompt\": \"...\", \"output\": \"...\"}` format required by the training script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/prepare_gold_standard_data.py\n",
    "!echo \"\\n✅ Data preparation complete. Verifying the new directory:\"\n",
    "!ls -l data/training/gold_standard_finetune_ready | wc -l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92814c9a",
   "metadata": {},
   "source": [
    "## Step 5: Format Data for Fine-Tuning\n",
    "\n",
    "This step is crucial. The `meta-llama/Meta-Llama-3-8B-Instruct` model requires a specific chat template for instruction fine-tuning. We will load the data prepared in the previous step and reformat it into the required structure, then save it to a new directory for the trainer to use.\n",
    "\n",
    "**Llama 3 Prompt Template:**\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{completion}<|eot_id|>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Define the Llama 3 prompt template\n",
    "LLAMA3_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{completion}<|eot_id|>\"\"\"\n",
    "\n",
    "# Load the dataset prepared by the previous script\n",
    "source_dir = \"data/training/gold_standard_finetune_ready\"\n",
    "dataset = load_dataset(\"json\", data_files=[str(f) for f in Path(source_dir).glob(\"*.json\")])['train']\n",
    "\n",
    "def format_for_llama3(entry):\n",
    "    \"\"\"Applies the Llama 3 prompt format to a dataset entry.\"\"\"\n",
    "    formatted_text = LLAMA3_TEMPLATE.format(\n",
    "        prompt=entry['prompt'],\n",
    "        completion=json.dumps(json.loads(entry['output']), indent=2) # Ensure completion is a formatted string\n",
    "    )\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# Apply the formatting\n",
    "formatted_dataset = dataset.map(format_for_llama3)\n",
    "\n",
    "# Save the newly formatted dataset\n",
    "output_dir = Path(\"data/training/gold_standard_finetune_llama3_ready\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as a single JSONL file, which is efficient for the trainer\n",
    "formatted_dataset.to_json(output_dir / \"data.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "print(f\"✅ Successfully formatted and saved dataset for Llama-style fine-tuning at {output_dir}\")\n",
    "print(\"Example of formatted data:\")\n",
    "print(formatted_dataset[0]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbc72b",
   "metadata": {},
   "source": [
    "## Step 6: Run the Fine-Tuning Script\n",
    "\n",
    "This is the core of the process. We execute the `run_finetuning.py` script, which will:\n",
    "\n",
    "1.  **Load** our prepared Gold Standard examples.\n",
    "2.  **Download** the base `meta-llama/Meta-Llama-3-8B-Instruct` model from Hugging Face.\n",
    "3.  **Configure** 4-bit quantization and LoRA for efficient training.\n",
    "4.  **Fine-tune** the model on our data.\n",
    "5.  **Save** the final, specialized `housebrain-llama3-8b-v1.0` model to the `models/` directory.\n",
    "\n",
    "We will use a high number of epochs (e.g., 200) because our dataset is very high-quality but small. This is necessary to ensure the model learns the schema thoroughly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ac353",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/run_finetuning.py \\\n",
    "    --dataset-path \"data/training/gold_standard_finetune_llama3_ready\" \\\n",
    "    --base-model \"meta-llama/Meta-Llama-3-8B-Instruct\" \\\n",
    "    --output-path \"models/housebrain-llama3-8b-v1.0\" \\\n",
    "    --epochs 200 \\\n",
    "    --batch-size 2 \\\n",
    "    --learning-rate 2e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ed78e",
   "metadata": {},
   "source": [
    "## Step 7: Next Steps\n",
    "\n",
    "Once training is complete, the new model is saved in the `models/housebrain-llama3-8b-v1.0` directory. \n",
    "\n",
    "You can now use this specialized model in the \"Interactive Testing & Curation\" section (by changing the model ID) to generate a large, high-quality \"Silver Standard\" dataset. This is the path to a truly production-ready system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e119bb7",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382263e0",
   "metadata": {},
   "source": [
    "### PASTE AND REPAIR YOUR JSON HERE\n",
    "\n",
    "_Double-click this cell to edit. Paste the raw JSON output from the interactive test above and manually correct it until it is a perfect `HouseOutput` object. Once it's perfect, save it as a new file in the `data/training/gold_standard` directory._\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"paste_your_raw_json_here\": \"delete this line and paste the model's output\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e7519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python scripts/run_finetuning.py \\\n",
    "#     --dataset-path \"data/training/gold_standard_finetune_ready\" \\\n",
    "#     --base-model \"Qwen/Qwen2-7B-Instruct\" \\\n",
    "#     --output-path \"models/housebrain-qwen2-7b-v0.1\" \\\n",
    "#     --epochs 200 \\\n",
    "#     --batch-size 2 \\\n",
    "#     --learning-rate 2e-5\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
