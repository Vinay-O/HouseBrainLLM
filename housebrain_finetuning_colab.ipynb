{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain Architect - Fine-Tuning on Colab\n",
        "\n",
        "This notebook fine-tunes the `Qwen/Qwen2.5-3B-Instruct` model on our curated house plan dataset.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Set up the GPU:** Go to `Runtime` > `Change runtime type` and select `T4 GPU` or another available GPU.\n",
        "2.  **Run each cell in order:** Execute the cells one by one from top to bottom.\n",
        "3.  **Hugging Face Token (Optional but Recommended):** The second code cell will ask for a Hugging Face token. While not always required for public models, it's good practice. You can create a token in your Hugging Face account settings and add it as a \"Secret\" in Colab (click the ðŸ”‘ icon on the left). Name the secret `HF_TOKEN`.\n",
        "4.  **Download the result:** After the final cell runs, a file named `housebrain_v1_adapters.zip` will appear in the file browser on the left. Right-click it and select `Download` to save your trained model adapters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. Clone the Repository ---\n",
        "# This will download your project files, including the necessary datasets.\n",
        "\n",
        "!git clone https://github.com/Vinay-O/HouseBrainLLM.git\n",
        "%cd HouseBrainLLM\n",
        "\n",
        "print(\"âœ… Repository cloned and directory changed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2. Install Dependencies ---\n",
        "# This multi-step process ensures that all libraries, especially torch, bitsandbytes,\n",
        "# and triton, are installed and compiled correctly for the Colab A100 GPU environment.\n",
        "\n",
        "# Step A: Install the main application libraries\n",
        "!pip install -U \"transformers==4.41.2\" \"peft==0.10.0\" \"accelerate==0.30.1\" \"datasets==2.19.1\"\n",
        "\n",
        "# Step B: Force a re-install of torch and a compatible triton from the PyTorch CUDA 12.1 index\n",
        "# This is the critical step for A100 GPUs.\n",
        "!pip install -U --force-reinstall \"torch==2.3.1\" \"triton==2.3.1\" --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Step C: Install the correct version of bitsandbytes\n",
        "!pip install -U \"bitsandbytes==0.43.1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3. Imports & Login ---\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig\n",
        "import os\n",
        "\n",
        "# Optional: Login to Hugging Face\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "  hf_token = userdata.get('HF_TOKEN')\n",
        "  login(token=hf_token)\n",
        "  print(\"âœ… Successfully logged into Hugging Face.\")\n",
        "except Exception as e:\n",
        "  print(\"Proceeding without Hugging Face login. Add a secret named HF_TOKEN if you encounter download issues.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 4. Load and Prepare Datasets ---\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "# The tokenizer is loaded first to be used in the data preparation.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define the correct file paths inside the cloned repository\n",
        "train_file_path = \"/content/HouseBrainLLM/finetune_dataset_train.jsonl\"\n",
        "val_file_path = \"/content/HouseBrainLLM/finetune_dataset_val.jsonl\"\n",
        "\n",
        "# Load from the cloned repository\n",
        "train_dataset = load_dataset(\"json\", data_files=train_file_path, split=\"train\")\n",
        "val_dataset = load_dataset(\"json\", data_files=val_file_path, split=\"train\")\n",
        "\n",
        "# This function formats the prompts and tokenizes them\n",
        "def formatting_and_tokenizing_func(examples):\n",
        "    texts = []\n",
        "    for msg_list in examples[\"messages\"]:\n",
        "        prompt = \"\"\n",
        "        response = \"\"\n",
        "        for msg in msg_list:\n",
        "            if msg[\"role\"] == \"user\":\n",
        "                prompt = msg[\"content\"]\n",
        "            elif msg[\"role\"] == \"assistant\":\n",
        "                response = msg[\"content\"]\n",
        "        \n",
        "        if prompt and response:\n",
        "            # Use the model's chat template for correct formatting\n",
        "            formatted_chat = tokenizer.apply_chat_template(\n",
        "                [{\"role\": \"system\", \"content\": \"You are a world-class AI architect. Generate a detailed and accurate JSON representation of a house floor plan based on the user's request.\"},\n",
        "                 {\"role\": \"user\", \"content\": prompt},\n",
        "                 {\"role\": \"assistant\", \"content\": response}],\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=False\n",
        "            )\n",
        "            texts.append(formatted_chat)\n",
        "    \n",
        "    # Tokenize the formatted text\n",
        "    return tokenizer(texts, truncation=True, max_length=1024, padding=False)\n",
        "\n",
        "# Apply the function to the datasets\n",
        "train_dataset = train_dataset.map(formatting_and_tokenizing_func, batched=True, remove_columns=train_dataset.column_names)\n",
        "val_dataset = val_dataset.map(formatting_and_tokenizing_func, batched=True, remove_columns=val_dataset.column_names)\n",
        "\n",
        "print(\"âœ… Datasets loaded and prepared.\")\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 5. Configure and Load Model ---\n",
        "\n",
        "# Configure 4-bit quantization to save memory\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the base model with our quantization config\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Configure LoRA for efficient fine-tuning\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(\"âœ… Model configured with 4-bit quantization and LoRA.\")\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 6. Start Fine-Tuning ---\n",
        "\n",
        "output_dir = \"housebrain_v1_adapters\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    num_train_epochs=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True, # Use fp16 for mixed-precision training\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"âœ… Fine-tuning complete.\")\n",
        "trainer.save_model(output_dir)\n",
        "print(f\"ðŸ’¾ Final model adapters saved to '{output_dir}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 7. Package and Download ---\n",
        "import shutil\n",
        "\n",
        "# Zip the adapters folder\n",
        "archive_path = shutil.make_archive(\"housebrain_v1_adapters\", 'zip', \"housebrain_v1_adapters\")\n",
        "\n",
        "print(f\"âœ… Model adapters saved and zipped at: {archive_path}\")\n",
        "print(\"You can now download 'housebrain_v1_adapters.zip' from the Files panel on the left.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
