{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. Install and Start Ollama ---\n",
        "%%bash\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "ollama serve > ollama_server.log 2>&1 &\n",
        "sleep 10\n",
        "if pgrep -x \"ollama\" > /dev/null; then echo \"‚úÖ Ollama server is running.\"; else echo \"‚ùå Ollama server failed to start.\"; cat ollama_server.log; fi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2. Install Python Client ---\n",
        "!pip install -q ollama ipywidgets pandas tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 3. Mount Drive & Load Prompts ---\n",
        "from google.colab import drive\n",
        "import os\n",
        "import random\n",
        "drive.mount('/content/drive')\n",
        "PROMPT_FILE_PATH = \"/content/drive/MyDrive/housebrain_prompts/platinum_prompts.txt\" \n",
        "def load_prompts_from_file(filepath):\n",
        "    print(f\"\\\\nLoading prompts from {filepath}...\")\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"‚ùå ERROR: Prompt file not found. Please check the path.\")\n",
        "        return []\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        prompts = [line.strip() for line in f if line.strip()]\n",
        "    print(f\"‚úÖ Successfully loaded {len(prompts)} prompts.\")\n",
        "    return prompts\n",
        "ALL_PROMPTS = load_prompts_from_file(PROMPT_FILE_PATH)\n",
        "if ALL_PROMPTS:\n",
        "    random.shuffle(ALL_PROMPTS)\n",
        "    print(\"‚úÖ Prompts have been successfully shuffled for this run.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 4. Configure Run ---\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import os\n",
        "model_options = [\n",
        "    \"phi4-reasoning:latest\", \"phi4-reasoning:plus\", \"phi4-reasoning:14b\",\n",
        "    \"phi4-reasoning:14b-plus-q4_K_M\", \"phi3:instruct\", \"llama3:instruct\"\n",
        "]\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=model_options, value='phi4-reasoning:latest', description='Select Model:',\n",
        "    disabled=False, style={'description_width': 'initial'}\n",
        ")\n",
        "display(model_dropdown)\n",
        "DATASET_TIER = \"gold_tier\"\n",
        "NUM_PLANS_TO_GENERATE = 15000 \n",
        "BASE_OUTPUT_DIR = \"raw_generated_data\"\n",
        "TIER_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, DATASET_TIER)\n",
        "os.makedirs(TIER_OUTPUT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 5. Pull & Verify Model ---\n",
        "selected_model = model_dropdown.value\n",
        "print(f\"--- Preparing Model: {selected_model} ---\")\n",
        "print(f\"Attempting to download via Ollama... (This may take several minutes)\")\n",
        "!ollama pull {selected_model}\n",
        "print(f\"\\\\n--- Verifying {selected_model} Installation ---\")\n",
        "!ollama list\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Setup Complete!\")\n",
        "print(\"You are ready to run the data generation in the next cell.\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 6. Run Data Generation (Parallelized for Max GPU Utilization) ---\n",
        "import ollama\n",
        "import json\n",
        "import hashlib\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "import concurrent.futures\n",
        "\n",
        "# --- A. Set Parallelism Level ---\n",
        "# The number of parallel requests to send to the Ollama server.\n",
        "# For an A100 with 40GB VRAM, a value between 32 and 96 is a good range.\n",
        "# We will start with an aggressive value of 64.\n",
        "# MONITOR your \"GPU RAM\" in the Colab resource panel. If it gets too close to the limit,\n",
        "# stop this cell, lower the number, and run it again.\n",
        "MAX_WORKERS = 64\n",
        "\n",
        "# --- B. Generation Function (Modified for Parallelism & Logging) ---\n",
        "def generate_and_save_raw_plan(task_tuple):\n",
        "    \"\"\"\n",
        "    Generates a plan and saves the raw output. Includes logging to visualize workers.\n",
        "    \"\"\"\n",
        "    index, prompt, model_name, tier_dir = task_tuple\n",
        "    \n",
        "    # Use tqdm.write for thread-safe logging that won't corrupt the progress bar.\n",
        "    tqdm.write(f\"[Task {index:05d}] STARTING...\")\n",
        "\n",
        "    prompt_hash = hashlib.md5(prompt.encode()).hexdigest()[:10]\n",
        "    timestamp = int(time.time())\n",
        "    unique_id = f\"prompt_{index:05d}_{prompt_hash}_{timestamp}\"\n",
        "    run_output_dir = os.path.join(tier_dir, model_name, unique_id)\n",
        "    os.makedirs(run_output_dir, exist_ok=True)\n",
        "    output_filename = os.path.join(run_output_dir, \"raw_output.json\")\n",
        "    \n",
        "    try:\n",
        "        structured_prompt = f\"\"\"\n",
        "        Please act as an expert architect specializing in Indian residential and commercial design. Your task is to generate a detailed JSON representation of a floor plan based on the following request, keeping local building norms and Vastu principles in mind where appropriate.\n",
        "        **Architectural Request:** \"{prompt}\"\n",
        "        **Instructions:** Provide ONLY the JSON output.\n",
        "        **JSON Schema:** {{ \"levels\": [ {{ \"level_id\": \"ground_floor\", \"rooms\": [], \"openings\": [] }} ] }}\n",
        "        Now, begin.\n",
        "        \"\"\"\n",
        "        response = ollama.chat(\n",
        "            model=model_name,\n",
        "            messages=[{'role': 'user', 'content': structured_prompt}],\n",
        "            format='json'\n",
        "        )\n",
        "        raw_output = response['message']['content']\n",
        "        with open(output_filename, 'w') as f:\n",
        "            f.write(raw_output)\n",
        "        \n",
        "        tqdm.write(f\"[Task {index:05d}] COMPLETED SUCCESSFULLY.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred: {str(e)}\"\n",
        "        with open(os.path.join(run_output_dir, \"error.log\"), 'w') as f:\n",
        "            f.write(error_message)\n",
        "        \n",
        "        tqdm.write(f\"[Task {index:05d}] FAILED. See error.log for details.\")\n",
        "        return False\n",
        "\n",
        "# --- C. Main Generation Loop (Parallelized) ---\n",
        "if 'ALL_PROMPTS' in locals() and ALL_PROMPTS:\n",
        "    print(f\"\\\\nüöÄ Starting PARALLEL data generation for {NUM_PLANS_TO_GENERATE} samples using {MAX_WORKERS} workers...\")\n",
        "    \n",
        "    tasks = []\n",
        "    for i in range(NUM_PLANS_TO_GENERATE):\n",
        "        current_prompt = ALL_PROMPTS[i % len(ALL_PROMPTS)]\n",
        "        tasks.append((i, current_prompt, selected_model, TIER_OUTPUT_DIR))\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        results = list(tqdm(executor.map(generate_and_save_raw_plan, tasks), total=len(tasks), desc=f\"Generating with {selected_model}\"))\n",
        "\n",
        "    successful_generations = sum(1 for res in results if res is True)\n",
        "\n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ Gold Tier Data Generation Complete!\")\n",
        "    print(f\"Successfully generated {successful_generations} / {NUM_PLANS_TO_GENERATE} raw plan files.\")\n",
        "    print(f\"All outputs are saved in: '{TIER_OUTPUT_DIR}/{selected_model}'\")\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(\"\\\\nüõë HALTED. Please run the prompt loading cell (Cell 3) successfully first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 7. Package and Download Results ---\n",
        "import shutil\n",
        "import time\n",
        "output_directory_path = os.path.join(BASE_OUTPUT_DIR, DATASET_TIER, selected_model)\n",
        "zip_filename = f\"{DATASET_TIER}_{selected_model}_raw_data_{int(time.time())}\"\n",
        "zip_filepath = f\"/content/{zip_filename}\"\n",
        "print(f\"Locating generated data in: {output_directory_path}...\")\n",
        "if os.path.isdir(output_directory_path):\n",
        "    print(f\"Compressing into '{zip_filename}.zip'...\")\n",
        "    shutil.make_archive(zip_filepath, 'zip', output_directory_path)\n",
        "    print(f\"‚úÖ Success! Your data is compressed and ready at {zip_filepath}.zip\")\n",
        "else:\n",
        "    print(f\"‚ùå Error: Could not find the output directory '{output_directory_path}'.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
