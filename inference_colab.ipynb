{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HouseBrain V1: Junior Architect - Inference Notebook\n",
        "\n",
        "This notebook is designed to run inference with the fine-tuned HouseBrain V1 model. \n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Upload Adapters:** Make sure you have uploaded the `housebrain_v1_adapters.zip` file from the fine-tuning notebook and unzipped it. The `housebrain_v1_adapters` directory should be present in the root of your Colab environment.\n",
        "2.  **Run All Cells:** Execute the cells in order to install dependencies, load the model with the adapters, and run a test inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Install Dependencies\n",
        "# We need to install the core libraries for running the model.\n",
        "# Triton is a dependency for bitsandbytes on some Colab GPUs.\n",
        "\n",
        "print(\"üöÄ Installing required libraries...\")\n",
        "!pip install -q transformers==4.43.3 bitsandbytes==0.43.1 accelerate==0.32.1 torch==2.2.1 peft==0.12.0 triton\n",
        "print(\"‚úÖ Installation complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Imports and Setup\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import os\n",
        "\n",
        "print(\"‚úÖ Libraries imported.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Load the Fine-Tuned Model\n",
        "# This is where we load the original base model and then apply our trained adapters on top.\n",
        "import os\n",
        "# Define the models\n",
        "base_model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "adapter_path = \"/content/housebrain_v1_adapters\" # Path to your local adapters\n",
        "\n",
        "print(f\"üîç Checking for adapter directory at: {adapter_path}\")\n",
        "if not os.path.isdir(adapter_path):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Adapter directory not found at '{adapter_path}'. \"\n",
        "        f\"Please make sure you have uploaded and unzipped your adapters.\"\n",
        "    )\n",
        "print(\"‚úÖ Adapter directory found.\")\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "print(f\"‚¨áÔ∏è Loading base model: {base_model_name}\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"‚úÖ Base model loaded.\")\n",
        "\n",
        "print(f\"üîß Fusing adapters from: {adapter_path}\")\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "print(\"‚úÖ Adapters loaded.\")\n",
        "\n",
        "print(\"üîÑ Merging model and adapters...\")\n",
        "# Important: Merge the adapters into the base model for faster inference\n",
        "model = model.merge_and_unload()\n",
        "print(\"‚úÖ Model merged and ready for inference!\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Run Inference\n",
        "# Now we can test our specialized model with a new prompt.\n",
        "\n",
        "# The prompt should follow the same \"messages\" format we trained on\n",
        "prompt_text = \"Design a modern, single-story 2BHK house for a 30x40 feet plot with a total area of 1200 sqft.\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that generates house plans in JSON format.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt_text}\n",
        "]\n",
        "\n",
        "# Apply the chat template and tokenize\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "print(\"ü§ñ Generating house plan...\")\n",
        "\n",
        "# Generate the output\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=2048, # Increased token limit for potentially complex plans\n",
        "    do_sample=True,     # Use sampling for more creative/varied outputs\n",
        "    top_p=0.9,          # Use nucleus sampling\n",
        "    temperature=0.6     # A bit of creativity, but not too much\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"---‚ú® Generated Plan ‚ú®---\")\n",
        "print(response)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
